[{"categories":["原理分析"],"content":"Kube-apiserver 不止 OOM，还可能 Panic。 现象 Kube-apiserver 在处理外部请求时发生不可恢复的报错，直接 Fatal 退出运行。看日志调用堆栈，会发现 concurrent map iteration and map write 的字样。这是 golang 检测到有对 map 的并发读写后返回的一个 fatal 报错的内容，无法通过 recover 捕捉并恢复。 影响版本 目前线上使用的主流版本，都有这个问题，最新的 master（截止写这篇的时间） 分支依然有这个问题。虽然几乎所有版本中都存在这个问题，但由于其触发条件有一定的要求，也和参数配置有关，所以大家可能遇到的次数不多，至少相比 OOM 问题遇到的次数会少一些。 触发原因 这个问题的触发条件是 api 请求超时，同时在执行后续 filter 时也用到了 response header。 Kube-apiserver 在启动时通过 DefaultBuildHandlerChain 注册了特别多的 filter/handler，其中就有一个专门用来控制请求超时的 timeoutHandler。kube-apiserver 并没有为单独的 handler 或者对应的 Goroutine 设置一个 WriteTimeout 的超时，而是单纯的通过 timeoutHandler 实现全部 NonLongRunning 类型请求的超时控制。 func DefaultBuildHandlerChain(apiHandler http.Handler, c *Config) http.Handler { handler := apiHandler handler = filterlatency.TrackCompleted(handler) handler = genericapifilters.WithAuthorization(handler, c.Authorization.Authorizer, c.Serializer) handler = filterlatency.TrackStarted(handler, c.TracerProvider, \"authorization\") if c.FlowControl != nil { workEstimatorCfg := flowcontrolrequest.DefaultWorkEstimatorConfig() requestWorkEstimator := flowcontrolrequest.NewWorkEstimator( c.StorageObjectCountTracker.Get, c.FlowControl.GetInterestedWatchCount, workEstimatorCfg, c.FlowControl.GetMaxSeats) handler = filterlatency.TrackCompleted(handler) handler = genericfilters.WithPriorityAndFairness(handler, c.LongRunningFunc, c.FlowControl, requestWorkEstimator, c.RequestTimeout/4) handler = filterlatency.TrackStarted(handler, c.TracerProvider, \"priorityandfairness\") } else { handler = genericfilters.WithMaxInFlightLimit(handler, c.MaxRequestsInFlight, c.MaxMutatingRequestsInFlight, c.LongRunningFunc) } handler = filterlatency.TrackCompleted(handler) handler = genericapifilters.WithImpersonation(handler, c.Authorization.Authorizer, c.Serializer) handler = filterlatency.TrackStarted(handler, c.TracerProvider, \"impersonation\") handler = filterlatency.TrackCompleted(handler) handler = genericapifilters.WithAudit(handler, c.AuditBackend, c.AuditPolicyRuleEvaluator, c.LongRunningFunc) handler = filterlatency.TrackStarted(handler, c.TracerProvider, \"audit\") failedHandler := genericapifilters.Unauthorized(c.Serializer) failedHandler = genericapifilters.WithFailedAuthenticationAudit(failedHandler, c.AuditBackend, c.AuditPolicyRuleEvaluator) failedHandler = filterlatency.TrackCompleted(failedHandler) handler = filterlatency.TrackCompleted(handler) handler = genericapifilters.WithAuthentication(handler, c.Authentication.Authenticator, failedHandler, c.Authentication.APIAudiences, c.Authentication.RequestHeaderConfig) handler = filterlatency.TrackStarted(handler, c.TracerProvider, \"authentication\") handler = genericfilters.WithCORS(handler, c.CorsAllowedOriginList, nil, nil, nil, \"true\") // WithTimeoutForNonLongRunningRequests will call the rest of the request handling in a go-routine with the // context with deadline. The go-routine can keep running, while the timeout logic will return a timeout to the client. handler = genericfilters.WithTimeoutForNonLongRunningRequests(handler, c.LongRunningFunc) handler = genericapifilters.WithRequestDeadline(handler, c.AuditBackend, c.AuditPolicyRuleEvaluator, c.LongRunningFunc, c.Serializer, c.RequestTimeout) handler = genericfilters.WithWaitGroup(handler, c.LongRunningFunc, c.NonLongRunningRequestWaitGroup) if c.ShutdownWatchTerminationGracePeriod \u003e 0 { handler = genericfilters.WithWatchTerminationDuringShutdown(handler, c.lifecycleSignals, c.WatchRequestWaitGroup) } if c.SecureServing != nil \u0026\u0026 !c.SecureServing.DisableHTTP2 \u0026\u0026 c.GoawayChance \u003e 0 { handler = genericfilters.WithProbabilisticGoaway(handler, c.GoawayChance) } handler = genericapifilters.WithWarningRecorder(handler) handler = genericapifilters.WithCacheControl(handler) handler = genericfilters.WithHSTS(handler, c.HSTSDirectives) if c.ShutdownSendRetryAfter { handler = genericfilters.WithRetryAfter(handler, c.lifecycleSignals.NotAcceptingNewRequest.Signaled()) } handler = genericfilters.WithHTTPLogging(handler) if utilfeature.DefaultF","date":"2024-01-30","objectID":"https://www.likakuli.com/posts/k8sapiserverpanic/:0:0","tags":["kubernetes"],"title":"kube-apiserver 又 Panic 了 - handler","uri":"https://www.likakuli.com/posts/k8sapiserverpanic/"},{"categories":["原理分析"],"content":"最新状态 问题出现的根本原因在于上面提到的 t.handler.ServeHTTP 与 timeoutHandler 并行执行，可能出现对 Response Header 的并发读写或者并发写。所以从根源上解决此问题的办法是避免 handler 之间的并行执行，也就是 timeout 要同时停止 t.handler.ServeHTTP 的执行，这就依赖 golang 本身的实现了。在 golang v1.20 中实现了对 per handler 的超时控制，详情可以参考 golang/go#54136，k8s 侧的实现参考 set per handler read/write deadline，此 PR 于 2022 年 11 月份提交，至今尚未合并到 master 分支上。 也就是说问题尚未彻底解决，在使用过程中仍然可能会遇到 header race condition 导致的 panic 问题，但已经提到的一些很具体的 case 已经得到了解决。 ","date":"2024-01-30","objectID":"https://www.likakuli.com/posts/k8sapiserverpanic/:1:0","tags":["kubernetes"],"title":"kube-apiserver 又 Panic 了 - handler","uri":"https://www.likakuli.com/posts/k8sapiserverpanic/"},{"categories":["原理分析"],"content":"历史修复 Fix race condition in logging when request times out 修复了请求超时后 respLogger handler 打印日志时从 request header 中读取 UserAgent 的操作与 t.handler.ServeHTTP 执行后其他 handler 操作 request header 的 race condition。但是这个修改并不彻底，respLogger 里面还存在其他使用 request header 的地方，比如获取 verb 时最后会调用 CleanVerb 函数，他也会访问 request header。 Fix header mutation race in timeout filter 在构建 baseTimeoutWriter 时对传入的 ResponseWriter 的 header 进行了复制保存为 handlerHeaders，然后在其实现的 ResponseWriter 接口的 Header() 中返回 handlerHeaders，同时在 Write() 方法中也是遍历了 handlerHeaders 写到最终的 ResponseHeader 里面。这个 PR 可以避免注册在 timeoutHandler 之前的 handler 与 timeoutWriter 的 timeout() 方法对 ResponseWriter header 的并发写操作导致的问题。 Copy request in timeout handler 在 timeoutHandler 里面 Clone 了外部传进来的 request 对象，把新的对象传递给 t.handler.ServeHTTP 方法，来彻底解决 timeoutHandler 之后注册的 handler 中使用到了 request header 而导致的 race condition 问题，从最开始的代码片段中可以看到这个操作。 apiserver: warning.AddWarning should not panic when request times out 通过调整 WithWarningRecorder 的顺序到 WithTimeoutForNonLongRunningRequests 前面，解决了 ResponseWriter header 并发写导致 Panic 的一个特例。放在前面后 recorder 里面使用的 ResponseWriter 就是 baseTimeoutWriter 了，其对 WriteHeader() 方法的调用加了锁，就可以避免这个问题。recorder 在注册 handler 时只是被写到了 context 里面，真正被使用的地方很多，其中就有 WithAuthentication 返回的 handler。 总结 因为 request 或者 response header 的并发读写导致的 panic 问题至今尚未完全解决，如果遇到的话可以在报错堆栈里面搜一下看是否存在 Header 和 handler 关键字，如果有，那么基本就八九不离十可以确定问题了。 理论上目前因 request header 导致的问题已经被彻底修复，一般情况下服务端也很少去修改 request header。等 set per handler read/write deadline 这个 PR 合入主干后，理论上因为 response header 导致的问题也可以被彻底修复。 kube-apiserver Panic 的一个影响是上面的所有请求可能与其他实例建立连接，如果量比较大，而 k8s 服务端本身又没有做好限流保护的话，很可能会导致其他实例出现 OOM，导致 k8s 控制面雪崩。 ","date":"2024-01-30","objectID":"https://www.likakuli.com/posts/k8sapiserverpanic/:2:0","tags":["kubernetes"],"title":"kube-apiserver 又 Panic 了 - handler","uri":"https://www.likakuli.com/posts/k8sapiserverpanic/"},{"categories":["原理分析"],"content":" 从一个看似简单的功能说明 k8s 确实越来越复杂了。 约定 k8s list 返回结果中的元素集合是按照字母顺序从 a 到 z 升序排列的。 原因 这个约定存在的原因是为了保持开启 WatchCache 功能前后 list 请求返回结果的一致性。在关闭 WatchCache 功能的情况下，请求直接透传给 Etcd，此时返回的结果就是字母升序排列的。 实现 ","date":"2024-01-21","objectID":"https://www.likakuli.com/posts/k8slist/:0:0","tags":["kubernetes"],"title":"k8s 越来越复杂了吗？","uri":"https://www.likakuli.com/posts/k8slist/"},{"categories":["原理分析"],"content":"Etcd 在关闭 WatchCache 时 list 结果有序，这个能力是 Etcd 提供的。kube-apiserver 的实现中调用 Etcd API 执行 Range 操作的时候并没有显示指定 SortOrder 和 SortTarget，但最终却返回了按照 key 排序的数据。如果直接使用 Etcdctl 去获取指定 key collection 的话，不需要显示指定顺序，返回的结果也是有序的。 这就涉及到 Etcd Range 的实现，在不显示设置排序顺序和排序对象的时候，默认返回 key 按照字母升序排序后的结果，相关的代码如下 // 最终排序位置 func (ti *treeIndex) visit(key, end []byte, f func(ki *keyIndex) bool) { keyi, endi := \u0026keyIndex{key: key}, \u0026keyIndex{key: end} ti.RLock() defer ti.RUnlock() ti.tree.AscendGreaterOrEqual(keyi, func(item btree.Item) bool { if len(endi.key) \u003e 0 \u0026\u0026 !item.Less(endi) { return false } if !f(item.(*keyIndex)) { return false } return true }) } 如果是沿着调用链路一步步看的话，在前一步（默认线性一致性读，需要等待 appliedIndex 赶上 confirmedIndex）返回数据时的代码中也有相关注释 func (a *applierV3backend) Range(ctx context.Context, txn mvcc.TxnRead, r *pb.RangeRequest) (*pb.RangeResponse, error) { ... sortOrder := r.SortOrder if r.SortTarget != pb.RangeRequest_KEY \u0026\u0026 sortOrder == pb.RangeRequest_NONE { // Since current mvcc.Range implementation returns results // sorted by keys in lexiographically ascending order, // sort ASCEND by default only when target is not 'KEY' sortOrder = pb.RangeRequest_ASCEND } ... } 注释写的很明显了，默认情况下 mvcc.Range 就是返回按字母升序排序后的结果。 ","date":"2024-01-21","objectID":"https://www.likakuli.com/posts/k8slist/:1:0","tags":["kubernetes"],"title":"k8s 越来越复杂了吗？","uri":"https://www.likakuli.com/posts/k8slist/"},{"categories":["原理分析"],"content":"WatchCache 如果在开启 WatchCache 的情况下调用 list 可能会发现结果并不是有序的，这是因为直到 v1.27 才开始加上有序的功能，换句话说就是从有了 WatchCache 开始直到 v1.27，list 都不是严格有序的，也就是没有遵循之前的约定。如果你的客户端依赖 list 结果的顺序，那么在使用 v1.27 之前的版本的时候可能就会遇到问题了，如果没有遇到问题，说明客户端对顺序没有依赖，而这也是大部分的场景，也正如此，这个问题才不容易被发现。 List 返回的是 WatchCache store 中的数据，而 store 中的数据是无序的。在 v1.27 Reuse generic GetList test for watchcache and fix inconsistency issues for both etcd3 and watchcache 开始在从 store 返回最终数据时进行了主动排序的操作，原理很简单，就是实现了 golang sort.Interface 接口，然后调用了 sort.Sort 方法对 slice 进行排序。 看似人畜无害的一个行为，引入了一个新的问题，sort.Sort 的执行是在加锁的情况下执行的，而在 Reflector 处理每个从 Etcd 返回的 event 的时候也会进行加锁操作，因此 list 操作就会对 event 的处理产生影响，导致一些不必要的处理延迟。而这个问题最终在 v1.30（尚未发布） Don’t sort in the critical section 才得到修复，通过 defer 把排序操作放在了加锁前面。 即使在没有上述问题的情况下，针对每次请求都额外多一次排序操作是否会对 API 延迟或 kube-apiserver 内存有一些影响呢？或者影响有多大呢？这就和其使用的排序算法有关了，上面提到使用 sort.Sort 进行排序，不同版本性能不一样，最新的实现是 pdqsort 算法，是字节贡献给社区的，其性能是之前的 2~60 倍，无需额外内存。所以对最终的内存没有影响，对延迟的影响取决于数据量的大小，但相比于对数据进行序列化，网络传输等的耗时，此处排序的耗时显得微乎其微。如果要较真的话，这里还可以再修改一下，改成使用 slices.Sort 的方式，使用泛型 + pdqsort，耗时会更低。 那有没有更好的办法来实现返回有序的效果呢，能想到的一种方案是在处理 event 将资源对象保存到 WatchCache store 的时候就保持 store 有序，这样可以避免每次 list 时的实时排序操作。但收益如何需要实现之后对比评估，理论上收益并不大。 ","date":"2024-01-21","objectID":"https://www.likakuli.com/posts/k8slist/:2:0","tags":["kubernetes"],"title":"k8s 越来越复杂了吗？","uri":"https://www.likakuli.com/posts/k8slist/"},{"categories":["原理分析"],"content":"WatchList WatchCache 是将 store 中的数据排序后一次性的返回给客户端，WatchList 为了优化 kube-apiserver 内存消耗，改用流的方式实现 list 的效果，参考 从 ListWatch 到 WatchList，那么他返回的结果也应该遵循规范做到按字母升序排列。 在最新的 v1.29.0 实现中 WatchList 还是 alpha 状态，尚未做到严格有序，原因是 WatchList 的实现是用 watchCacheInterval 的数据（WatchCache store 或者 cyclic buffer 中的数据） + cacheWatcher input chan 中的数据作为 list 的结果，由于 cacheWatcher input chan 中的数据是按 resourceversion 排序的，而且必须是按 RV 排序，就会导致最终的数据无法严格字母升序。 社区也是在对此功能进行开发，目前存在两个尚未合并的 PR，Ensure that initial events are sorted for WatchList 和 storage/cacher: ensure the cache is at the Most Recent ResourceVersion when streaming was requested。前者是对前半部分 watchCacheInterval 的数据进行排序，后者是用来保证 watchCacheInterval 中的数据已经是 list 所需的全量数据，也就是说不再需要从 cacheWatcher input chan 拼接数据了。 理论上通过上面两个改动是可以实现 WatchList 下的 list 有序的，但同样会引入新的问题，即当前的实现是服务端收到 Watch 请求后会立马开始往客户端发送数据，而上述改动则需要服务端等待 watchCacheInterval 是全量数据，排序后才开始往客户端发送数据，这就引入了一个数据处理的延迟。也就是说处理方式变成了从当前的有一条数据就发一条的实现，变成了要先等到服务端是全量数据后再开始一条一条的发送给客户端。 那么这个延迟会有多大呢，这个值跟 Etcd 的配置有关，当前的实现是依赖 ProgressNotify 的，通过 --experimental-watch-progress-notify-interval 控制 ProgressNotify event 发送的周期，默认是 10m，也就是说默认情况下会存在最大 10m + 1.25s 的延迟，这显然是不可接受的。可以通过缩小此参数的值来缩短延迟，一般情况下设置为 5s，也就会有最大 5s + 1.25s 的延迟。这个延迟其实也很难让人接受，其中的 1.25s 是来自 bookmarktimer 的 1s~1.25s 的定时周期导致的。社区计划是利用 ConsistentRead 机制中已经使用到的 RequestProgressNotify 机制，由 kube-apiserver 周期性（100ms）主动请求 Etcd 发送 ProgessNotify 的原理把这个周期缩短到 100ms（1.25是否存在待定），此功能已经在 ConsistentRead FeatureGate 开启后的 List 请求中使用到了，尚未在 WatchList 中支持。 这个延迟和 ConsistentRead/WatchList + ProgressNotify 机制有关，ConsistentRead/WatchList 都是先请求 Etcd 获取当前最大的 RV，等待 Cache 数据追上 RV 之后才开始后续流程的，问题就在这个最大 RV，因为 Etcd 的 RV 是全局的，由于节点一直在上报 lease 状态，就会导致 Etcd 中的最新 RV 一直在增加，但是资源对象的 RV 很可能是一直不变的。例如获取 default namespace 下的所有 pods，即使在此 ns 下没有任何 pod 的情况下使用 ConsistentRead 或者 WatchList 时也能感觉到明显的延迟，延迟的大小和上面提到的 Etcd 参数有关，如果没有显示指定的话给人的感觉就是一直无法返回数据直到超时或者取消请求，避险问题，感兴趣的话可以用最新版本（v1.29.0）测试下。 结束语 一个看似简单的 list 有序，竟也会牵扯到这么多的内容，而且还有很多尚未实现的功能。k8s 为了一个基本不会用到的 list 有序，需要做这么多的工作，复杂度的提升可想而知，谁知道里面还会有多少这种实际几乎不会用到的功能呢？！ ","date":"2024-01-21","objectID":"https://www.likakuli.com/posts/k8slist/:3:0","tags":["kubernetes"],"title":"k8s 越来越复杂了吗？","uri":"https://www.likakuli.com/posts/k8slist/"},{"categories":["原理介绍"],"content":"从一个有趣的问题引出很多人都在关注的 Kubernetes LTS 的问题。 有趣的问题 2019 年，一个名为 apiserver LoopbackClient Server cert expired after 1 year 的 issue 中提到了一个有趣的问题，如果一个 kube-apiserver 已经一年没有重启过了，那么这个 kube-apiserver 就无法再正常工作了。 issue 作者给出了自己的定位的原因：kube-apiserver 没有更新自签的 LoopbackClient 证书相关内容。从下面代码中可以看到证书过期时间被设置为了 1 年。 // create self-signed cert+key with the fake server.LoopbackClientServerNameOverride and // let the server return it when the loopback client connects. certPem, keyPem, err := certutil.GenerateSelfSignedCertKey(server.LoopbackClientServerNameOverride, nil, nil) if err != nil { return fmt.Errorf(\"failed to generate self-signed certificate for loopback connection: %v\", err) } certProvider, err := dynamiccertificates.NewStaticSNICertKeyContent(\"self-signed loopback\", certPem, keyPem, server.LoopbackClientServerNameOverride) if err != nil { return fmt.Errorf(\"failed to generate self-signed certificate for loopback connection: %v\", err) } --- // GenerateSelfSignedCertKeyWithFixtures creates a self-signed certificate and key for the given host. // Host may be an IP or a DNS name. You may also specify additional subject alt names (either ip or dns names) // for the certificate. // // If fixtureDirectory is non-empty, it is a directory path which can contain pre-generated certs. The format is: // \u003chost\u003e_\u003cip\u003e-\u003cip\u003e_\u003calternateDNS\u003e-\u003calternateDNS\u003e.crt // \u003chost\u003e_\u003cip\u003e-\u003cip\u003e_\u003calternateDNS\u003e-\u003calternateDNS\u003e.key // Certs/keys not existing in that directory are created. func GenerateSelfSignedCertKeyWithFixtures(host string, alternateIPs []net.IP, alternateDNS []string, fixtureDirectory string) ([]byte, []byte, error) { validFrom := time.Now().Add(-time.Hour) // valid an hour earlier to avoid flakes due to clock skew maxAge := time.Hour * 24 * 365 // one year self-signed certs 注：LoopbackClient 是在 kube-apiserver 中用来访问自身时使用的，例如 kube-apiserver 在启动时需要获取 Service，EndPoint 等信息（AA 中用到了），就用到了这个 LoopbackClient。 同时也给出了用来检查自己集群相关证书过期时间的方法，可以通过重启 kube-apiserver 来临时解决这个问题。 # replace {Master_IP} with your master IP and 6443 with your apiserver port curl --resolve apiserver-loopback-client:6443:{Master_IP} -k -v https://apiserver-loopback-client:6443/healthz root@kind-control-plane:/# curl --resolve apiserver-loopback-client:6443:172.17.0.2 -k -v https://apiserver-loopback-client:6443/healthz * Added apiserver-loopback-client:6443:172.17.0.2 to DNS cache * Hostname apiserver-loopback-client was found in DNS cache * Trying 172.17.0.2:6443... * TCP_NODELAY set * Connected to apiserver-loopback-client (172.17.0.2) port 6443 (#0) * ALPN, offering h2 * ALPN, offering http/1.1 * successfully set certificate verify locations: * CAfile: none CApath: /etc/ssl/certs * TLSv1.3 (OUT), TLS handshake, Client hello (1): * TLSv1.3 (IN), TLS handshake, Server hello (2): * TLSv1.2 (IN), TLS handshake, Certificate (11): * TLSv1.2 (IN), TLS handshake, Server key exchange (12): * TLSv1.2 (IN), TLS handshake, Request CERT (13): * TLSv1.2 (IN), TLS handshake, Server finished (14): * TLSv1.2 (OUT), TLS handshake, Certificate (11): * TLSv1.2 (OUT), TLS handshake, Client key exchange (16): * TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1): * TLSv1.2 (OUT), TLS handshake, Finished (20): * TLSv1.2 (IN), TLS handshake, Finished (20): * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 * ALPN, server accepted to use h2 * Server certificate: * subject: CN=apiserver-loopback-client@1577103676 * start date: Dec 23 11:21:16 2019 GMT * expire date: Dec 22 11:21:16 2020 GMT * issuer: CN=apiserver-loopback-client-ca@1577103676 * SSL certificate verify result: self signed certificate in certificate chain (19), continuing anyway. * Using HTTP2, server supports multi-use * Connection state changed (HTTP/2 confirmed) * Copying HTTP/2 data in stream buffer to connection buffer after upgrade: len=0 * Using Stream ID: 1 (easy handle 0x55d565d9c1d0) \u003e GET /healthz HTTP/2 \u003e Host: apiserver-loopback-client:6443 \u003e User-Agent: curl/7.65.3 \u003e Accept: */* \u003e * Connection state changed (MAX_CONCURRENT_STREAMS == 250","date":"2024-01-13","objectID":"https://www.likakuli.com/posts/k8slts/:0:0","tags":["kubernetes"],"title":"kubernetes 究竟有没有 LTS？","uri":"https://www.likakuli.com/posts/k8slts/"},{"categories":["使用说明"],"content":"大家在对 2023 年诸多互联网公司故障的总结中多次提到了控制 “爆炸半径”，几乎都在说缩小集群规模，那除了缩小集群规模外还有没有其他办法呢？如果一出问题就通过缩小规模去解决，多少会显得有点不够专业（草台班子）。k8s 已经经历了九年半的发展，众多的终端用户在以什么样的方式使用 k8s，即便社区高手如云，也很难把所有使用场景都考虑到并且处理好，但也不至于差到连我们这群\"草台班子\"都能想到的一些最基本的问题（比如控制爆炸半径）都想不到。比起把集群搞大出问题的人，反而是在出问题后只会喊控制集群规模的那些 k8s 相关的云原生专家们，那些 k8s 集群管理员们，更像是草台班子。（并没有说 k8s 等于云原生的意思，但只要做的事情和 k8s 沾点边就号称云原生，这是事实） k8s 已经提供了一些用来保障自身及运行于其中的服务的 SLO 的能力，本篇围绕相关参数进行介绍。部分参数可能已经从启动参数中挪到了对应组件的 Configuration 资源中或者只存在于 Configuration 资源中，有需要请参考官方文档进行设置，基于 v1.29。 Common --http2-max-streams-per-connection 服务器为客户端提供的 HTTP/2 连接中最大流数的限制，零表示使用 GoLang 的默认值（250）。这个参数涉及到一个安全漏洞和连接数，详情参考连接数也会影响 kube-apiserver 内存。并非只有 kube-apiserver 暴露了这个参数，其他组件对外暴露 API 的同样也暴露了这个参数。 kube-apiserver 早期为 MaxConcurrentStreams 设置了默认值 250，并且暴露了参数可以在外部修改，而在 v1.29 的发布中，将其默认值修改为了 100，同时 backport 回了从 v1.25 及之后的所有的版本，这个修改和 golang 的安全漏洞 CVE-2023-44487 and CVE-2023-39325 有关。生产环境一般情况下保持默认值即可。 --kube-api-burst default 400 --kube-api-qps default 200 访问 kube-apiserver 时的客户端限流设置，尤其注意 kube-scheduler，这两个值会影响 binding 操作，进而影响到调度吞吐，生产环境建议按需设置，尤其是 kube-scheduler。 kube-apiserver ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:0:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Rate Limit --enable-priority-and-fairness default true 如果为 true 且启用了 APIPriorityAndFairness 特性门控， 则使用增强的处理程序替换 max-in-flight 处理程序， 以便根据优先级和公平性完成排队和调度，生产环境强烈建议开启并按需做好对应的配置。 --max-mutating-requests-inflight default 200 如果 –enable-priority-and-fairness 为 true，那么此值和 –max-requests-inflight 的和将确定服务器的总并发限制（必须是正数）。 否则，该值限制同时运行的变更类型的请求的个数上限。0 表示无限制。 --max-requests-inflight default 400 如果 –enable-priority-and-fairness 为 true，那么此值和 –max-mutating-requests-inflight 的和将确定服务器的总并发限制（必须是正数）。 否则，该值限制进行中非变更类型请求的最大个数，零表示无限制。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:1:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Reliability --etcd-servers-overrides etcd 服务器针对每个资源的重载设置，以逗号分隔。 单个替代格式：组/资源#服务器（group/resource#servers）， 其中服务器是 URL，以分号分隔。 注意，此选项仅适用于编译进此服务器二进制文件的资源。 笔者曾经修复了一个相关的 bug 并随着 v1.25 发布，该 bug 会导致即使通过此参数配置了 event 使用单独的 etcd 存储，kube-controller-manager 中的 gc controller 也需要在启动时创建对应的 event informer，这回带来两个问题：一个 kube-controller-manager 内存使用过高，二是在 event etcd 出现问题时，如果 kube-controller-manager 发生了重启，则即使存其他资源的 etcd 正常，kube-controller-manager 也无法正常启动工作，因为 event informer 会同步失败。详情可以参考 feat: ignore all event resource for gc。生产环境建议大规模集群中通过此参数把 event 拆到单独的 etcd 中存储。 --goaway-chance 为防止 HTTP/2 客户端卡在单个 API 服务器上，随机关闭某连接（GOAWAY）。其原理是随机给 HTTP/2 客户端发送 GOAWAY 帧，客户端收到后如果有新的请求，则会创建新的连接，而不会继续复用之前的连接，客户端的其他运行中请求不会受到影响。新连接被负载均衡后可能会与其他 API 服务器开始通信。 此参数设置将被发送 GOAWAY 指令的请求的比例。 只有一个 API 服务器或不使用负载均衡器的集群不应启用此特性。 最小值为 0（关闭），最大值为 0.02（1/50 请求）。 --min-request-timeout default 1800 表示处理程序在请求超时前，必须保持连接处于打开状态的最小秒数。 当前只对监听（Watch）请求的处理程序有效。如果是通过 informer 发起的 Watch 请求，默认 5m ~ 10m 的超时时间。 --request-timeout default 1m 表示处理程序在超时之前必须保持打开请求的持续时间。 这是请求的默认请求超时，但对于 Watch 请求，会被 --min-request-timeout 标志覆盖。生产环境建议按需设置，尤其在单个资源类型数据量较大时，如果在 timeout 时间内没有执行完 List 请求，会触发 informer 的 Relist 操作，严重影响 kube-apiserver 内存，甚至 OOM。 --watch-cache default true 在 API 服务器中启用 watch cache。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:2:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Other --anonymous-auth default true 启用针对 API 服务器的安全端口的匿名请求。 未被其他身份认证方法拒绝的请求被当做匿名请求。 匿名请求的用户名为 system:anonymous， 用户组名为 system:unauthenticated，生产环境建议关闭。 --default-not-ready-toleration-seconds default 300 --default-unreachable-toleration-seconds default 300 对污点 NotReady:NoExecute 和 Unreachable:NoExecute 的容忍时长（以秒计）。 开启 DefaultTolerationSeconds feature-gate 的情况下，上述两个容忍度之一会被按需添加到尚未具有此容忍度的每个 pod 中。在节点出现上述异常时会影响其上的 Pod 的运行情况，具体在下面会解释。 kube-controller-manager ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:3:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Pod Deletion --unhealthy-zone-threshold default 0.55 仅当给定区域中处于非就绪状态的节点（最少 3 个）的占比高于此值时， 才将该区域视为不健康。 --large-cluster-size-threshold default 50 node-lifecycle-controller 在执行 Pod 驱逐操作逻辑时， 基于此标志所设置的节点个数阈值来判断所在集群是否为大规模集群。 当集群规模小于等于此规模时， --secondary-node-eviction-rate 会被隐式重设为 0。 --node-eviction-rate default 0.1 当某区域健康时，在节点故障的情况下每秒删除 Pods 的节点数。请参阅 --unhealthy-zone-threshold 以了解“健康”的判定标准。 这里的区域（zone）在集群并不跨多个区域时指的是整个集群。0.1 代表每 10s 驱逐一台节点，在 Taint Based Eviction 模式下，这个频率是用来控制为 Node 设置 Taint 的，并不是控制最终驱逐 Pod 的。 --node-monitor-grace-period default 40s 在将一个 Node 标记为不健康之前允许其无响应的时长上限。 必须比 kubelet 的 nodeStatusUpdateFrequency 大 N 倍； 这里 N 指的是 kubelet 发送节点状态的重试次数。 --node-monitor-period default 5s node-lifecycle-controller 对节点状态进行同步的周期。 --node-startup-grace-period default 1m 在节点启动期间，节点可以处于无响应状态； 但超出此标志所设置的时长仍然无响应则该节点被标记为不健康。 --secondary-node-eviction-rate default 0.01 当一个区域不健康造成节点失效时，每秒钟从此标志所给的节点上删除 Pod 的节点个数。 参见 --unhealthy-zone-threshold 以了解“健康与否”的判定标准。 在只有一个区域的集群中，区域指的是整个集群。如果集群规模小于 --large-cluster-size-threshold 所设置的节点个数时， 此值被隐式地重设为 0。 以上参数均与 Node 状态异常时如何处理其上运行中的 Pod 有关，需要格外注意，合理设置这些参数可以有效的控制故障域的大小或者说控制所谓的爆炸半径。上面提到的 zone 可以通过为 Node 添加如下 Label 来设置： failure-domain.beta.kubernetes.io/region (deprecated) Note: Starting in v1.17, this label is deprecated in favor of topology.kubernetes.io/region. failure-domain.beta.kubernetes.io/zone (deprecated) Note: Starting in v1.17, this label is deprecated in favor of topology.kubernetes.io/zone. --terminated-pod-gc-threshold default 12500 在已终止 Pod 垃圾收集器删除已终止 Pod 之前，可以保留的已终止 Pod 的个数上限。 若此值小于等于 0，则相当于禁止垃圾回收已终止的 Pod。主要影响 Job 创建出来的 Pod，或者不按照官网文档指导非得给非 Job 的 Pod 设置 activeDeadlineSeconds 的 Pod。影响集群中的 Pod 数量，可能会存在大量 Completed 的 Pod，如果客户端使用不当，有可能给 kube-apiserver 内存造成压力。生产环境建议调小这个值。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:4:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Performance –concurrent-*-syncs 这是一类参数的统称，代表对应资源的 controller 并行执行时的 worker 数量，按需调整即可，影响 workload 的处理速度。 Kube-scheduler 除了 Common 提到的参数外，这个组件本身没有什么需要特别关注的参数。正常情况下即使他挂了，也不影响存量业务的运行，一个极端情况需要注意，那就是在本身业务或者平台故障恢复时如果调度器不能用的话，会影响故障恢复速度。但这其实也说明业务或者平台自身架构设计就有问题。 Kubelet ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:5:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Eviction --eviction-hard default imagefs.available\u003c15%,memory.available\u003c100Mi,nodefs.available\u003c10% 触发 Pod 驱逐操作的一组硬性门限（例如：memory.available\u003c1Gi （内存可用值小于 1G）设置。在 Linux 节点上，默认值还包括 nodefs.inodesFree\u003c5%。 --eviction-minimum-reclaim 当某资源压力过大时，kubelet 将执行 Pod 驱逐操作。 此参数设置软性驱逐操作需要回收的资源的最小数量（例如：imagefs.available=2Gi）。 --eviction-pressure-transition-period default 5m kubelet 在驱逐压力状况解除之前的最长等待时间。 --eviction-max-pod-grace-period 响应满足软性驱逐阈值（Soft Eviction Threshold）而终止 Pod 时使用的最长宽限期（以秒为单位）。 如果设置为负数，则遵循 Pod 的指定值。 --eviction-soft 设置一组驱逐阈值（例如：memory.available\u003c1.5Gi）。 如果在相应的宽限期内达到该阈值，则会触发 Pod 驱逐操作。 --eviction-soft-grace-period 设置一组驱逐宽限期（例如，memory.available=1m30s），对应于触发软性 Pod 驱逐操作之前软性驱逐阈值所需持续的时间长短。 生产环境如果集群管理员对集群缺乏足够的把控力，建议关闭驱逐，显示设置 –eviction-hard= 即可关闭，关闭之后驱逐相关的其他参数就不需要关注了。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:6:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Reserve --kube-reserved kubernetes 系统预留的资源配置，以一组 \u003c资源名称\u003e=\u003c资源数量\u003e 格式表示。 （例如：cpu=200m,memory=500Mi,ephemeral-storage=1Gi,pid='100'）。 当前支持 cpu、memory 和用于根文件系统的 ephemeral-storage。 --qos-reserved 【警告：Alpha 特性】设置在指定的 QoS 级别预留的 Pod 资源请求，以一组 \"资源名称=百分比\" 的形式进行设置，例如 memory=50%。 当前仅支持内存（memory）。要求启用 QOSReserved 特性门控。 --reserved-cpus 用逗号分隔的一组 CPU 或 CPU 范围列表，给出为系统和 Kubernetes 保留使用的 CPU。 此列表所给出的设置优先于通过 --system-reserved 和 --kube-reskube-reserved 所保留的 CPU 个数配置。 --reserved-memory 以逗号分隔的 NUMA 节点内存预留列表。（例如 --reserved-memory 0:memory=1Gi,hugepages-1M=2Gi --reserved-memory 1:memory=2Gi）。 每种内存类型的总和应该等于--kube-reserved、--system-reserved和--eviction-threshold。 --system-reserved 系统预留的资源配置，以一组 资源名称=资源数量 的格式表示， （例如：cpu=200m,memory=500Mi,ephemeral-storage=1Gi,pid='100'）。 目前仅支持 cpu 和 memory 的设置。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:7:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Image --serialize-image-pulls default true 逐一拉取镜像。建议 不要 在 docker 守护进程版本低于 1.9 或启用了 Aufs 存储后端的节点上 更改默认值。 --registry-burst default 10 设置突发性镜像拉取的个数上限，仅在 --registry-qps 大于 0 时使用。 --registry-qps default 5 如此值大于 0，可用来限制镜像仓库的 QPS 上限。设置为 0，表示不受限制。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:8:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"Other --max-open-files default 1000000 kubelet 进程可以打开的最大文件数量。 --node-status-update-frequency default 10s 指定 kubelet 向主控节点汇报节点状态的时间间隔。注意：更改此常量时请务必谨慎， 它必须与节点控制器中的 nodeMonitorGracePeriod 一起使用。 --pod-max-pids default -1 设置每个 Pod 中的最大进程数目。如果为 -1，则 kubelet 使用节点可分配的 PID 容量作为默认值。 configMapAndSecretChangeDetectionStrategy default Watch configMapAndSecretChangeDetectionStrategy 是 ConfigMap 和 Secret Manager 的运行模式。合法值包括： Get：kubelet 从 API 服务器直接取回必要的对象； Cache：kubelet 使用 TTL 缓存来管理来自 API 服务器的对象； Watch：kubelet 使用 watch 操作来观察所关心的对象的变更； 生产环境中如果存在大量使用 ConfigMap 或者 Secret 作为卷挂载到 Pod 中的场景时，Watch 策略会导致 kube-apiserver 中对应资源 Get 请求的 QPS 非常高，这里笔者也已经通过如下 PR feat: minimize unnecessary API requests to the API server for the configmap/secret get API 修复了这个问题并且在 v1.29 中发布，有相同使用场景而运行的 k8s 版本比较低的话，建议把此 PR pick 回来，然后调整为 Cache 策略，可以有效的降低 QPS。相关原理可以参考如下三篇： High QPS for ConfigMap Get Requests - 1 High QPS for ConfigMap Get Requests - 2 High QPS for ConfigMap Get Requests - 3 Other 其他组件比如 kube-proxy 很多公司不使用或者用其他组件代替了，CNI，CSI，Device Plugin 等实现也众多，具体组件具体去看吧。 爆炸半径 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:9:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"禁止删除 可以通过 validating webhook 来设置一些规则，默认禁用一些大范围删除请求，例如删除 CRD，删除 Namespace，删除 Node，批量删除 Pod 等高危操作。这里推荐使用 kinitiras，一款通用可编程的 admission webhook 策略引擎来处理，对 k8s 代码没有任何侵入，想要实现什么功能，拦截什么请求，只需要添加一个对应的策略（cr 文件）即可； ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:10:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"限制性删除 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:11:0","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"服务端限流 kube-apiserver 开启 apf 限流，按需设置。 ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:11:1","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["使用说明"],"content":"客户端限流 node 异常引起的 pod deletion，比如遇到网络问题或者其他原因导致 kubelet 异常或者无法访问 kube-apiserver 的话，如果超过一定时间，则 kube-controller-manager 就会为 node 设置对应的 taint，然后开始删除上面的容器（设置 deletionTimestamp），虽然存量 Pod 可能无法立刻被删除，但等到网络问题或者其他问题被解决，kubelet 恢复后，就会开始执行 pod 删除的操作。可以通过为 node 划分 region，zone，eviction rate 等参数进行详细的设置来控制影响范围，起到客户端限流的效果。 资源紧张时的驱逐，考虑关掉这个能力。computePodActions hash 校验导致的重启，最初这个能力正常是供 Deployment 变更时用的，比如只修改 image，现在也会用于 In-Place Update 的场景。但由于其校验逻辑是直接使用整个 container 计算 hash，在遇到集群 “原地升级/降级” 的场景时，即使没有其他任何主动变更操作，如果 container 在新旧版本的 kube-apiserver 中有字段的增减，也会导致前后 hash 不一致，从而容器重启，而这种重启虽然是单机上执行的，但波及范围是整个集群。 In-Place Update 自 v1.27 发布为 alpha 状态至今仍然是 alpha 状态，笔者也曾给社区反馈过 hash 校验导致的存量容器重启问题：In place update trigger container restart when upgrade k8s cluster Old pod created by old version k8s will restart when to resize the pod spec after upgrade k8s cluster version and enable the specified feature gate pod which have do some resize operations will restart when disable the in place update feature gate 最终这个问题被当做其成为 beta 状态的一个 PRR，可以看到即使强如社区，在遇到这种问题的时候也没有把相关场景都考虑到（确实考虑到了 hash 会变的场景，做了一定的处理，但没有考虑全），做到不影响存量容器，这是非常非常危险的事情。 要解决这个问题，只能修改 kubelet 代码，要么添加更全面的 hash 校验场景，要么在判断 hash 不一致之后额外访问全局 api，实现类似限流的能力，在获得许可后再进行实际的容器重启的动作。各有利弊，前者很有可能随着版本升级出现新的 case，需要每次升级时都关注这块逻辑，后者则是引入了外部依赖，需要考虑外部依赖异常场景。但即使实现了后者，还是需要在每次升级时关注这个问题，所以可以的话，还是尽量在根上解决问题，而不是靠限流。 最后 虽然怎么也飞不出，草台班子的世界，但还是可以在草台班子里面提高自己的演技。以上内容来自笔者的经验和一些狭隘的认知，如有问题或者更好的方案，欢迎一起交流~ ","date":"2023-12-28","objectID":"https://www.likakuli.com/posts/k8sparams/:11:2","tags":["kubernetes"],"title":"k8s: 到底谁才是草台班子？","uri":"https://www.likakuli.com/posts/k8sparams/"},{"categories":["源码分析"],"content":" Watch 请求像极了打工人的一生，从加入社会（cacher.watchers）开始就不停地被 Push，拼命的干，干的慢了就有可能会被干掉，干的快的奖励干到寿终正寝，就像某些专家建议的一样。 前面已经通过一个系列来介绍 kube-apiserver cache 相关内容，本篇旨在通过分析 Watch 请求的生命周期把前面涉及到的关键内容串联起来，系统的描述在每个阶段都涉及到哪些关键内容，方便更直观的了解之前每一篇在整个过程中的位置和作用。 基于 v1.29 分支，针对 HTTP 协议分析（忽略 WS）。 怎么来滴 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:0:0","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"客户端 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:1:0","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"发起请求 Watch 请求本质还是 List 请求，集群中的大部分 watch 请求来自于集群内的 agent 或者 DaemonSet 类的数据面组件，比如 kubelet，kube-proxy 等，当然也可能会存在一些非 Go 语言的组件，但无论什么语言，什么组件，最终都是调用了 kube-apiserver 对应的 API 来实现的 watch。以常用的 client-go informer 为例，看如下调用 c.client.Get(). Resource(\"pods\"). VersionedParams(\u0026opts, scheme.ParameterCodec). Timeout(timeout). Watch(ctx) VersionedParams 方法处理 opts（ListOptions）里面的参数并保存到 request.params 中，在最后调用 Watch 时会调用 URL 方法，利用 request.params 生成 url 的 query，最后返回完整的 url，其中 Watch: true 就是 ListOptions 的参数，最终会拼到生成的 url query 中，类似 /api/v1/pods?watch=true 的形式。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:1:1","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"数据流 特指使用 client-go informer 时的数据流 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:1:2","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"服务端 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:2:0","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"路由 当客户端最终调用了 API 进行 watch，kube-apiserver 收到请求后会启动一个 goroutine 调用相应的 handler 来处理这个请求。kube-apiserver 启动时会进行路由注册，最终通过 APIInstaller 的 registerResourceHandlers 注册路由及对应的 handler，如下 func (a *APIInstaller) registerResourceHandlers(path string, storage rest.Storage, ws *restful.WebService) (*metav1.APIResource, *storageversion.ResourceInfo, error) { ... } 三个入参，分来是路径，后端存储，路由，每一种资源都有自己对应的 Storage，但所有的 Storage 又都包含了相同的匿名嵌套结构 *genericregistry.Store，而此结构实现了 Watch 接口，所以所有的 Storage 也就都实现了 Watch 接口。handler 注册如下 case \"LIST\": // List all resources of a kind. doc := \"list objects of kind \" + kind if isSubresource { doc = \"list \" + subresource + \" of objects of kind \" + kind } handler := metrics.InstrumentRouteFunc(action.Verb, group, version, resource, subresource, requestScope, metrics.APIServerComponent, deprecated, removedRelease, restfulListResource(lister, watcher, reqScope, false, a.minRequestTimeout)) handler = utilwarning.AddWarningsHandler(handler, warnings) route := ws.GET(action.Path).To(handler) 如果看代码的话，还能看到其他地方有 watch handler 的注册，但会标注 deprecated in 1.11，这些 case 分支对应的是在 uri path 中添加 watch/ 的那些请求，类似 /api/v1/pods/watch 的路径，而客户端是通过 query 传递的参数，所以最终还是命中 case List。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:2:1","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"Handler 业务逻辑在 restfulListResource 中，经过一层层的函数调用，最终实例化一个对应的 WatchServer 对象，并通过 HandleHTTP 来处理 // HandleHTTP serves a series of encoded events via HTTP with Transfer-Encoding: chunked. // or over a websocket connection. func (s *WatchServer) HandleHTTP(w http.ResponseWriter, req *http.Request) { flusher, ok := w.(http.Flusher) if !ok { err := fmt.Errorf(\"unable to start watch - can't get http.Flusher: %#v\", w) utilruntime.HandleError(err) s.Scope.err(errors.NewInternalError(err), w, req) return } framer := s.Framer.NewFrameWriter(w) if framer == nil { // programmer error err := fmt.Errorf(\"no stream framing support is available for media type %q\", s.MediaType) utilruntime.HandleError(err) s.Scope.err(errors.NewBadRequest(err.Error()), w, req) return } // ensure the connection times out timeoutCh, cleanup := s.TimeoutFactory.TimeoutCh() defer cleanup() // begin the stream w.Header().Set(\"Content-Type\", s.MediaType) w.Header().Set(\"Transfer-Encoding\", \"chunked\") w.WriteHeader(http.StatusOK) flusher.Flush() kind := s.Scope.Kind watchEncoder := newWatchEncoder(req.Context(), kind, s.EmbeddedEncoder, s.Encoder, framer) ch := s.Watching.ResultChan() done := req.Context().Done() for { select { case \u003c-s.ServerShuttingDownCh: // the server has signaled that it is shutting down (not accepting // any new request), all active watch request(s) should return // immediately here. The WithWatchTerminationDuringShutdown server // filter will ensure that the response to the client is rate // limited in order to avoid any thundering herd issue when the // client(s) try to reestablish the WATCH on the other // available apiserver instance(s). return case \u003c-done: return case \u003c-timeoutCh: return case event, ok := \u003c-ch: if !ok { // End of results. return } metrics.WatchEvents.WithContext(req.Context()).WithLabelValues(kind.Group, kind.Version, kind.Kind).Inc() isWatchListLatencyRecordingRequired := shouldRecordWatchListLatency(event) if err := watchEncoder.Encode(event); err != nil { utilruntime.HandleError(err) // client disconnect. return } if len(ch) == 0 { flusher.Flush() } if isWatchListLatencyRecordingRequired { metrics.RecordWatchListLatency(req.Context(), s.Scope.Resource, s.metricsScope) } } } } 可以看到 watch 是通过为 HTTP 设置 Transfer-Encoding: chunked 实现的，s.Watching 就是上面提到的 Storage.(Watcher).Watch 的返回结果，Storage.(Watcher) 就是 *genericregistry.Store，Store 结构体本身也有个叫 Storage 的属性，经典的 store，storage 模式，store 是服务的封装，storage 的存储的封装。最终往 response 里面写的内容是从 ch := s.Watching.ResultChan() 获取的，然后对 Event 以及 Event 中包含的资源对象进行序列化后写到最后的 response 中。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:2:2","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"数据流 Store.Storage 要么是 cacher，要么是 Etcd，由 kube-apiserver 的启动参数 --watch-cache 控制，默认为 true，对应 Store.Storage 是 cacher，这里我们只关注 cacher 的场景，最终返回的 cacheWatcher 现了 k8s.io/apimachinery/pkg/watch/watch.go 文件中定义的 Interface 接口（这个接口类型名字有点抽象）。 // Watch implements storage.Interface. func (c *cacher) Watch(ctx context.Context, key string, opts storage.ListOptions) (watch.Interface, error) { ... triggerValue, triggerSupported := \"\", false if c.indexedTrigger != nil { for _, field := range pred.IndexFields { if field == c.indexedTrigger.indexName { if value, ok := pred.Field.RequiresExactMatch(field); ok { triggerValue, triggerSupported = value, true } } } } // It boils down to a tradeoff between: // - having it as small as possible to reduce memory usage // - having it large enough to ensure that watchers that need to process // a bunch of changes have enough buffer to avoid from blocking other // watchers on our watcher having a processing hiccup chanSize := c.watchCache.suggestedWatchChannelSize(c.indexedTrigger != nil, triggerSupported) // Determine a function that computes the bookmarkAfterResourceVersion bookmarkAfterResourceVersionFn, err := c.getBookmarkAfterResourceVersionLockedFunc(ctx, requestedWatchRV, opts) if err != nil { return newErrWatcher(err), nil } // Determine a function that computes the watchRV we should start from startWatchResourceVersionFn, err := c.getStartResourceVersionForWatchLockedFunc(ctx, requestedWatchRV, opts) if err != nil { return newErrWatcher(err), nil } // Determine watch timeout('0' means deadline is not set, ignore checking) deadline, _ := ctx.Deadline() identifier := fmt.Sprintf(\"key: %q, labels: %q, fields: %q\", key, pred.Label, pred.Field) // Create a watcher here to reduce memory allocations under lock, // given that memory allocation may trigger GC and block the thread. // Also note that emptyFunc is a placeholder, until we will be able // to compute watcher.forget function (which has to happen under lock). watcher := newCacheWatcher( chanSize, filterWithAttrsFunction(key, pred), emptyFunc, c.versioner, deadline, pred.AllowWatchBookmarks, c.groupResource, identifier, ) // note that c.waitUntilWatchCacheFreshAndForceAllEvents must be called without // the c.watchCache.RLock held otherwise we are at risk of a deadlock // mainly because c.watchCache.processEvent method won't be able to make progress // // moreover even though the c.waitUntilWatchCacheFreshAndForceAllEvents acquires a lock // it is safe to release the lock after the method finishes because we don't require // any atomicity between the call to the method and further calls that actually get the events. forceAllEvents, err := c.waitUntilWatchCacheFreshAndForceAllEvents(ctx, requestedWatchRV, opts) if err != nil { return newErrWatcher(err), nil } // We explicitly use thread unsafe version and do locking ourself to ensure that // no new events will be processed in the meantime. The watchCache will be unlocked // on return from this function. // Note that we cannot do it under cacher lock, to avoid a deadlock, since the // underlying watchCache is calling processEvent under its lock. c.watchCache.RLock() defer c.watchCache.RUnlock() startWatchRV := startWatchResourceVersionFn() var cacheInterval *watchCacheInterval if forceAllEvents { cacheInterval, err = c.watchCache.getIntervalFromStoreLocked() } else { cacheInterval, err = c.watchCache.getAllEventsSinceLocked(startWatchRV) } if err != nil { // To match the uncached watch implementation, once we have passed authn/authz/admission, // and successfully parsed a resource version, other errors must fail with a watch event of type ERROR, // rather than a directly returned error. return newErrWatcher(err), nil } addedWatcher := false func() { c.Lock() defer c.Unlock() if generation, ok := c.ready.checkAndReadGeneration(); generation != readyGeneration || !ok { // We went unready or are already on a different generation. // Avoid registering and starting the watch as it will have to be // terminated","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:2:3","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"小结 至此，已经介绍了从客户端发起请求，到服务端通过已经注册的路由和对应 handler 去创建新的 cacheWatcher，并消费 cacheWatcher 的 ResultChan 把最终的 event 序列化后写到 response 中的整个过程。 干到干不动 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:2:4","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"三驾马车 type cacheWatcher struct { input chan *watchCacheEvent result chan watch.Event done chan struct{} } cacheWatcher 有三个 chan，其中 input 用来接收 cacher 发过来的 event，result 用来保存最终要返回的 event，上面提到的 processInterval 会消费 input 中的数据，发送到 result 中，done 用来控制是否还要往 result 中发送数据。前两个 chan 用来存放 event，是带缓冲的，缓冲的大小与资源类型以及 watchCache 的滑动窗口大小有关， func (w *watchCache) suggestedWatchChannelSize(indexExists, triggerUsed bool) int { // To estimate the channel size we use a heuristic that a channel // should roughly be able to keep one second of history. // We don't have an exact data, but given we store updates from // the last \u003ceventFreshDuration\u003e, we approach it by dividing the // capacity by the length of the history window. chanSize := int(math.Ceil(float64(w.currentCapacity()) / eventFreshDuration.Seconds())) // Finally we adjust the size to avoid ending with too low or // to large values. if chanSize \u003c minWatchChanSize { chanSize = minWatchChanSize } var maxChanSize int switch { case indexExists \u0026\u0026 triggerUsed: maxChanSize = maxWatchChanSizeWithIndexAndTrigger case indexExists \u0026\u0026 !triggerUsed: maxChanSize = maxWatchChanSizeWithIndexWithoutTrigger case !indexExists: maxChanSize = maxWatchChanSizeWithoutIndex } if chanSize \u003e maxChanSize { chanSize = maxChanSize } return chanSize } watchCache 滑动窗口大小从 v1.19 开始支持动态调整，初始值 100，最大 100 * 1024，chanSize 根据当前滑动窗口大小和 eventFreshDuration（从最初的 5m 调整到了最新的 75s）计算一个值，然后再根据传入的两个参数得到最大值，最终的 chanSize 介于最大值和最小值之间。传入的两个参数的计算逻辑如下 triggerValue, triggerSupported := \"\", false if c.indexedTrigger != nil { for _, field := range pred.IndexFields { if field == c.indexedTrigger.indexName { if value, ok := pred.Field.RequiresExactMatch(field); ok { triggerValue, triggerSupported = value, true } } } } // It boils down to a tradeoff between: // - having it as small as possible to reduce memory usage // - having it large enough to ensure that watchers that need to process // a bunch of changes have enough buffer to avoid from blocking other // watchers on our watcher having a processing hiccup chanSize := c.watchCache.suggestedWatchChannelSize(c.indexedTrigger != nil, triggerSupported) 这里是专门为 Pod 做的优化，c.indexedTrigger 只有当资源类型是 Pod 时才有值，indexName 为 spec.nodeName，indexerFunc 返回 pod.spec.nodeName 的值，其作用会在下文分析。最终 chanSize 的结果如下 对于携带了 fieldSelector=spec.nodeName=nodeA 的 pod watch 请求（多来自 kubelet），其最终的 chanSize 固定为 10； 对于没有携带上述 fieldSelector 的 pod watch 请求，其最终的 chanSize 介于 10 ~ 1000； 对于其他非 pod 的 watch 请求，其最终的 chanSize 介于 10 ~ 100 之间； chanSize 会作为最终创建的 cacheWatcher 的 input 和 result chan 的缓冲区大小。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:3:0","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"辛苦劳碌 c）Push 数据到 input chan：cacheWatcher input 和 result chan 接受资源的 add，update，delete event，同时也接受自 v1.15 开始支持的 bookmark event； a）把 initEvents 发送到 result chan：对于 watch 请求，如果 RV 不设置或者设置为 0，则从 watchCache store 返回 initEvents，否则只要其值不小于 watchCache 滑动窗口最老的 event 的 RV，则从 watchCache 滑动窗口返回 initEvents； b）消费 input chan 并发送到 result chan：initEvents 发送完之后开始消费 input chan 中的数据； 由于 cacheWatcher input chan 数据来自 cacher，所以下面的分析中也会对 cacher 相关逻辑进行分析，会标注功能所属，如前置过滤 - cacher 就代表 cacher 里面的功能。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:4:0","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"前置过滤 - cacher // startDispatching chooses watchers potentially interested in a given event // a marks dispatching as true. func (c *Cacher) startDispatching(event *watchCacheEvent) { // It is safe to call triggerValuesThreadUnsafe here, because at this // point only this thread can access this event (we create a separate // watchCacheEvent for every dispatch). triggerValues, supported := c.triggerValuesThreadUnsafe(event) c.Lock() defer c.Unlock() c.dispatching = true // We are reusing the slice to avoid memory reallocations in every // dispatchEvent() call. That may prevent Go GC from freeing items // from previous phases that are sitting behind the current length // of the slice, but there is only a limited number of those and the // gain from avoiding memory allocations is much bigger. c.watchersBuffer = c.watchersBuffer[:0] if event.Type == watch.Bookmark { c.startDispatchingBookmarkEventsLocked() // return here to reduce following code indentation and diff return } // iterate over watchers for each applicable namespace/name tuple namespace := event.ObjFields[\"metadata.namespace\"] name := event.ObjFields[\"metadata.name\"] if len(namespace) \u003e 0 { if len(name) \u003e 0 { // namespaced watchers scoped by name for _, watcher := range c.watchers.allWatchers[namespacedName{namespace: namespace, name: name}] { c.watchersBuffer = append(c.watchersBuffer, watcher) } } // namespaced watchers not scoped by name for _, watcher := range c.watchers.allWatchers[namespacedName{namespace: namespace}] { c.watchersBuffer = append(c.watchersBuffer, watcher) } } if len(name) \u003e 0 { // cluster-wide watchers scoped by name for _, watcher := range c.watchers.allWatchers[namespacedName{name: name}] { c.watchersBuffer = append(c.watchersBuffer, watcher) } } // cluster-wide watchers unscoped by name for _, watcher := range c.watchers.allWatchers[namespacedName{}] { c.watchersBuffer = append(c.watchersBuffer, watcher) } if supported { // Iterate over watchers interested in the given values of the trigger. for _, triggerValue := range triggerValues { for _, watcher := range c.watchers.valueWatchers[triggerValue] { c.watchersBuffer = append(c.watchersBuffer, watcher) } } } else { // supported equal to false generally means that trigger function // is not defined (or not aware of any indexes). In this case, // watchers filters should generally also don't generate any // trigger values, but can cause problems in case of some // misconfiguration. Thus we paranoidly leave this branch. // Iterate over watchers interested in exact values for all values. for _, watchers := range c.watchers.valueWatchers { for _, watcher := range watchers { c.watchersBuffer = append(c.watchersBuffer, watcher) } } } } c）是 cacher 的行为，发送之前会先调用 startDispatching 方法根据 cacher.watchers 和 event type 准备好要接收对应 event 的所有 cacheWatcher 放到 c.watchersBuffer 中。上一小节提到的 indexedTrigger 的作用就是减少特定情况下的需要遍历的 cacheWatcher 的数量，提升 cacher.incoming chan 的消费速度，因为他也是带缓冲的 chan，chan size 也是 100，而且也是在不停的接受来自 Etcd 的 event； for _, watcher := range c.watchersBuffer { if !watcher.nonblockingAdd(event) { c.blockedWatchers = append(c.blockedWatchers, watcher) } } 相当于提前根据 event 过滤掉了不关注此 event 的 cacheWatcher，这样上面的 for 循环需要遍历的次数就会降低，尤其是对带了 spec.nodeName 的来自 kubelet 的 watch 请求，一个非 bookmark event 里面的资源对象如果是已经调度成功的 pod 的话，他只需要发送到指定的一个 cacheWatcher 即可，而如果没有这个功能的话，他就得把这个 event 发送给所有 cacheWatcher，然后等到 b）消费 input 数据时再根据 filter 过滤掉不匹配的 event。 对于一个 5000 个 node 的集群来说，c.watchersBuffer 数量会从 5000 变成 1，同时数据也只往一个 cacheWatcher 的 input chan 发送，无论内存大小还是 cpu 消耗都是一个巨大的提升，如果没有这个功能的话，随着集群规模的提升，在 kube-apiserver 中会出现特别多 Fast watcher, slow processing 字样的日志（前提日志等级需要至少开到 3），这代表处理从 Etcd 读到的 event 太慢了，watchChan 的 incomingEventChan 或者 resultChan 缓冲区（都是 100）满了，导致客户端感知到的 event 出现延迟，同时由于从 Etcd 接受到的 event 无法及时处理都堆积在内存中，内存也会开始增加。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:4:1","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"控制 bookmark 频率 - cacher bookmark event 是由 bookmark timer 定时发送的，来自 Etcd 的 progressNotify 所转换的 bookmark 并不会直接发送给所有的 cacheWatcher，只是更新 lastProcessedResourceVersion，代码如下 func (c *cacher) dispatchEvents() { // Jitter to help level out any aggregate load. bookmarkTimer := c.clock.NewTimer(wait.Jitter(time.Second, 0.25)) defer bookmarkTimer.Stop() lastProcessedResourceVersion := uint64(0) for { select { case event, ok := \u003c-c.incoming: if !ok { return } // Don't dispatch bookmarks coming from the storage layer. // They can be very frequent (even to the level of subseconds) // to allow efficient watch resumption on kube-apiserver restarts, // and propagating them down may overload the whole system. // // TODO: If at some point we decide the performance and scalability // footprint is acceptable, this is the place to hook them in. // However, we then need to check if this was called as a result // of a bookmark event or regular Add/Update/Delete operation by // checking if resourceVersion here has changed. if event.Type != watch.Bookmark { c.dispatchEvent(\u0026event) } lastProcessedResourceVersion = event.ResourceVersion metrics.EventsCounter.WithLabelValues(c.groupResource.String()).Inc() case \u003c-bookmarkTimer.C(): bookmarkTimer.Reset(wait.Jitter(time.Second, 0.25)) ... bookmarkEvent := \u0026watchCacheEvent{ Type: watch.Bookmark, Object: c.newFunc(), ResourceVersion: lastProcessedResourceVersion, } ... c.dispatchEvent(bookmarkEvent) case \u003c-c.stopCh: return } } } 注释写的很明确，不会直接把来自底层存储（Etcd）的 bookmark event 直接发给 cacheWatcher，代码里面也可以看到在消费 c.incoming 的时候有 event.Type 的判断，如果是 bookmark 的话，仅仅更新了 lastProcessedResourceVersion 属性。最终发给 cacheWatcher 的 bookmark 是通过 bookmarkTimer 定时触发的，每 1s ~ 1.25s 触发一次发送，每 1m 发一次 bookmark，这个地方很复杂，笔者觉得实现有问题，尤其是在开启 WatchList 的时候，还在和社区讨论，可以参考 return a bookmark event with bookmarkafterresourceversion immediately to reduce WatchList time cost，如果确认存在此问题的话，那么在 WatchList beta 之前这个问题是需要被解决的。涉及到的逻辑在上面展示的 startDispatching 方法中 if event.Type == watch.Bookmark { c.startDispatchingBookmarkEventsLocked() // return here to reduce following code indentation and diff return } 针对 bookmark event，会调用 startDispatchingBookmarkEventsLocked 筛选出来需要接收 bookmark event 的 cacheWatcher。 func (c *cacher) startDispatchingBookmarkEventsLocked() { // Pop already expired watchers. However, explicitly ignore stopped ones, // as we don't delete watcher from bookmarkWatchers when it is stopped. for _, watchers := range c.bookmarkWatchers.popExpiredWatchersThreadUnsafe() { for _, watcher := range watchers { // c.Lock() is held here. // watcher.stopThreadUnsafe() is protected by c.Lock() if watcher.stopped { continue } c.watchersBuffer = append(c.watchersBuffer, watcher) c.expiredBookmarkWatchers = append(c.expiredBookmarkWatchers, watcher) } } } startDispatchingBookmarkEventsLocked 调用 c.bookmarkWatchers 的 popExpiredWatchersThreadUnsafe 找到所有需要发送 bookmark event 的 cacheWatcher，放到 c.watchsBuffer 中，就和普通的 event 一样。同时还要加入到 c.expiredBookmarkWatchers 中，在 finishDispatching 时重新把这些已经过期但没有 stop 的 cacheWatcher 重新加入到 c.bookmarkWatchers 中。 c.bookmarkWatchers 的类型是 watcherBookmarkTimeBuckets，专门用来存储那些 query 中携带了 allowWatchBookmarks=true 的 watch 请求对应的 cacheWatcher。从有了 bookmark 开始，watcherBookmarkTimeBuckets 和其 addWatcher 方法进行过几次改动，梳理如下： v1.15 引入 bookmark 功能，watcherBookmarkTimeBuckets.addWatcher 会根据 cacheWatcher 的超时时间 - 2s 计算一个发送 bookmark event 的时间，然后根据这个时间戳，放到 watcherBookmarkTimeBuckets 里面。然后每 1s ~ 1.25s 通过 popExpiredWatchersThreadUnsafe 去获取需要发送 bookmark event 的 cacheWatcher，然后将生成的 bookmark event 发送给这些 cacheWatcher。此阶段在消费 c.incoming chan 时并没有判断 event type，因为此时并没有 progressNotify 的功能，所以 c.incoming chan 里面也就没有 bookmark event。这个阶段 bookmark event 发送时机是 watch 请求过期的前两秒，因为定期发送周期为 1s ~ 1.25s 所以在因为超时断开连接之前是可以发送一次 bookmark event 回去的； V1.16 通过 Pop expired watchers in case there is no update to cache 修复了 v1.15 中 bookmark 功能引入的一个 kube–apiserver 内存泄露的问题，在 cacher incoming chan 中没有 event 时，lastProcessedResourceVersion 一直是 0，在 bookmark timer 处理逻辑如下 if lastProcessedResourceVersion == 0 { continue } 就会造成客户端每","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:4:2","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":"及时消费 input chan 在 cacheWatcher 创建之后就开始写数据，而消费则需要等待 initEvent 发送完之后才开始，所以就有可能在 c）往 input 发数据时，input chan 缓冲区已经满了的情况。为什么我们几乎没有发觉这个问题呢，那是因为 client-go 使用 ListWatch 的方式会先通过 List 获得最新的 RV，然后带着这个 RV 去 Watch，所以 initEvent 的数量会比较少。但这个机制的问题在开启 WatchList 时就会凸显出来，当要返回的数据量较大时，因为都是通过 Watch 实现的，input chan 在被消费之前就被写满的概率就会大大提升，如果阻塞时间较长（超过 100ms）就会被强制关闭，有一定的副作用，会在下文深入分析。在 WatchList 成为 beta 之前，这个问题是必须要解决的。 怎么没滴 Watch 请求有如下几种结束条件： 异常退出：客户端或者服务端 crash； 超时：客户端在发起 watch 请求时会设置 timeoutseconds 参数，同时服务端也有个默认值，优先使用客户端设置的值 client-go informer 发起的 watch 请求，默认 5m ~ 10m 超时； kube-apiserver 通过 --min-request-timeout 控制，默认为 1800，即 30m ~ 60m 超时； cacher 为 cacheWatcher 分发 event 阻塞太长时间； 前两种好理解，下面主要讲第三种，涉及到的逻辑较复杂，通过控制 chan 实现。 for _, watcher := range c.watchersBuffer { if !watcher.nonblockingAdd(event) { c.blockedWatchers = append(c.blockedWatchers, watcher) } } if len(c.blockedWatchers) \u003e 0 { // dispatchEvent is called very often, so arrange // to reuse timers instead of constantly allocating. startTime := time.Now() timeout := c.dispatchTimeoutBudget.takeAvailable() c.timer.Reset(timeout) // Send event to all blocked watchers. As long as timer is running, // `add` will wait for the watcher to unblock. After timeout, // `add` will not wait, but immediately close a still blocked watcher. // Hence, every watcher gets the chance to unblock itself while timer // is running, not only the first ones in the list. timer := c.timer for _, watcher := range c.blockedWatchers { if !watcher.add(event, timer) { // fired, clean the timer by set it to nil. timer = nil } } // Stop the timer if it is not fired if timer != nil \u0026\u0026 !timer.Stop() { // Consume triggered (but not yet received) timer event // so that future reuse does not get a spurious timeout. \u003c-timer.C } c.dispatchTimeoutBudget.returnUnused(timeout - time.Since(startTime)) } 依赖 dispatchTimeoutBudget 实现对第一次发送失败（input chan 满了）的 cacheWatcher 重新发送的能力。dispatchTimeoutBudget 类似内核中的 cpu burst 的控制逻辑，第一次分配 100ms 的等待时长，如果用完了，那么剩下所有的 input chan 仍然满着的 cacheWatcher 都会被关闭，如果在 100ms 内都发送成功了，那么会把还没有消耗的时间返回回去供下次使用，也就是说下次就会有大于 100ms 的等待时长可用。 // we are graceful = false, when: // // (a) The bookmarkAfterResourceVersionReceived hasn't been received, // we can safely terminate the watcher. Because the client is waiting // for this specific bookmark, and we even haven't received one. // (b) We have seen the bookmarkAfterResourceVersion, and it was sent already to the client. // We can simply terminate the watcher. // we are graceful = true, when: // // (a) We have seen a bookmark, but it hasn't been sent to the client yet. // That means we should drain the input buffer which contains // the bookmarkAfterResourceVersion we want. We do that to make progress // as clients can re-establish a new watch with the given RV and receive // further notifications. graceful := func() bool { c.stateMutex.Lock() defer c.stateMutex.Unlock() return c.state == cacheWatcherBookmarkReceived }() klog.V(1).Infof(\"Forcing %v watcher close due to unresponsiveness: %v. len(c.input) = %v, len(c.result) = %v, graceful = %v\", c.groupResource.String(), c.identifier, len(c.input), len(c.result), graceful) c.forget(graceful) 通过 c.forget 实现关闭的效果，分两种情况： 如果开启了 WatchList 且 cacheWatcher 已经收到了来自 cacher 的特殊 bookmark 且还没有发送给 result chan 的话，则进行优雅关闭，只是关闭了 input chan 不再接收 cacher 的数据，并不会关闭 done，也即是还可以执行消费 input 并发送到 result 的过程； 其他情况直接关闭 input 和 done，既不往 input 发送数据，也不再消费 input 往 result chan 发数据； 从各种 watchers slice 中删除对应 cacheWatcher，最后执行 processInterval 方法的 goroutine 结束运行，并关闭 result chan。WatchServer 在 HandleHTTP 方法中判断 result chan 关闭，消费完里面的数据后整个 handler 执行完毕。至此，watch 请求走完了他的一生。 ","date":"2023-12-23","objectID":"https://www.likakuli.com/posts/watchlife/:4:3","tags":["kubernetes"],"title":"一条 Watch 请求的一生","uri":"https://www.likakuli.com/posts/watchlife/"},{"categories":["源码分析"],"content":" 之前从资源对象的获取方式、序列化、深拷贝的角度分别分析了对 kube-apiserver 内存使用量的影响以及社区是如何进行优化的，这一篇围绕网络连接展开分析其对 kube-apiserver 内存的影响。涉及到如下内容： 连接数是如何影响 kube-apiserver 内存的 client-go 是如何与 kube-apiserver 建立连接的 kube-apiserver 是如何与 etcd 建立连接的 etcd 相关服务端实现和问题 http2 的 io 多路复用的原理 golang 在 http2 实现上的一些缺陷和安全漏洞 背景 之前从资源对象的获取方式、序列化、深拷贝的角度分别分析了对 kube-apiserver 内存使用量的影响以及社区是如何进行优化的，这一篇围绕网络连接展开分析其对 kube-apiserver 内存的影响，以及这中间涉及到的其他相关问题。 分析 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:0:0","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"内存消耗 按照常理理解的话，进程负责了网络连接的管理，理论上连接数多了之后是对进程内存有影响的，但一般情况下在做内存优化的时候并不会上来就从连接数的角度去分析。在 kube-apiserver 中真实的出现了因为网络连接数造成的不可忽视的内存开销。 相关问题是在 2022 年提出来的，在名为 etcd3 client logger uses significant memory with many CRDs 的 issue 中提到了一个现象，在一个通过 kind 启动的包含大约 1900 个 CRD 的集群中（注意是 CRD 不是 CR），issue 的作者发现 kube-apiserver 的内存使用量高达到 8G，而 etcd3 client’s logger 贡献了大约 1.5G，很夸张的一个比例。随后其又做了一些测试并贴出来了 pprof 的结果，如下 可以明显的看到 zapcore.newCounters 占用了 800+M 内存，这还只是一个用来复现问题的 demo，实际情况下的内存消耗要比这个值多。zap 是 etcd3 client 使用的日志包。 经过其分析，logger 消耗内存多和 kube-apiserver 与 etcd3 的连接数有关，过程如下 All of /apis is handled by a *crdHandler When a request comes in it gets an instance of the crd (from an informer) Gets serving info from a map of CRDs - if there is none it… Calls customresource.NewStorage which creates a genericregistry.Store Calls its CompleteWithOptions method That calls its “Decorator”, which is a genericregistry.StorageWithCacher That calls generic.NewRawStorage which just wraps factory.Create factory.Create calls newETCD3Storage which calls newETCD3Client 最后的结果就是 kube-apiserver 会为每个 CRD 都和 etcd 建立一个连接，而 kube-apiserver 在设置 clientv3.Config 时并没有显示的指定 Logger，而 etcd clientv3 内部逻辑会判断 Logger 是否为空，是的话会实例化一个 Logger 对象出来，最终 kube-apiserver 和 etcd 有多少连接存在，就会有多少 Logger 对象存在。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:1:0","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"Logger 问题修复 issue 作者提了 Share a single etcd3 client logger across all clients 来修复上述问题，改动比较简单，在 init 函数里创建一个 logger 对象，并将其设置给 clientv3.Config.Logger，结果就会是全局只使用一个 logger 对象，避免上述问题的出现。在 v1.25 中发布，并于 8 月份 backport 回了 v1.22 ~ v1.24 版本中，如果运行的 k8s 发行版的发型时间在此之前，且集群内使用了大量的 CRD 的话，就需要去注意下这个问题了。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:2:0","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"Kube-apiserver 与 Etcd 的连接 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:3:0","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"One Client Per Etcd 至此，logger 对象只有一个了，但 kube-apiserver 与 etcd 的连接数仍然存在多个，与资源类型的数量正相关，所以事情到这里并没有结束，大佬们又开始讨论是否可以继续优化 kube-apiserver 与 etcd 的连接数，而这个问题早在 2021 年就已经开始讨论了：https://kubernetes.slack.com/archives/C0EG7JC6T/p1618439560231400 Yeah that sounds like what it currently does. What it should do is make one client per database (recall that you can put different collections in different etcd databases). 针对这个问题，上面 issue 作者又提了 API server creates one etcd client per CRD 来跟进，同时还提了对应的修复代码 Only create one etcd client per transport，通过添加一个 transport 级别的 etcd3 client Cache 来解决此问题，并且给出了他的测试结果，内存减少了 35%，但这个 PR 最终没有合入主干中 I would expect us to be able to move the client construction earlier in the server startup so a single instance can be shared by multiple registries. 多个 reviewer 表示希望通过单例的方式实现，而不是添加 transport 级别的 cache，此 PR 作者并没有继续按上述思路去修改，随后关闭了这个 PR，由另外一个作者通过新的 PR Create and reuse a single etcd client 来解决此问题，新的 PR 按照上面的思路，通过 transport 级别的单例实现了每个 etcd 集群一个 Client 实例（注意这里不是连接，而是 Client 实例，原因后文会涉及到），但新的 PR 至今依然尚未合入主干，也就是说至今仍然是每种资源类型一个 Etcd Client 实例。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:3:1","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"又现惊天大 bug？ 在已经关闭的老 PR 中 Jordan 也提出了他的一些疑问 As an aside, I do wonder if this will change the performance characteristics of large servers with lots of requests going to parallel resources at the same time, which previously got their own connections, and now would multiplex over a single (?) connection. 从之前每种资源一个连接变成每个 etcd集群一个 Client 实例，依赖 io 多路复用，在大规模集群的场景下会不会存在性能问题？然后大佬们又就此进行了讨论 I will be a little surprised if gRPC’s multiplexing (over single HTTP/2 connection) is worse than the kernel/network fabric’s multiplexing (over multiple connections). 因为 etcd clientv3 本身已经是通过 gRPC 方式访问 etcd 了，应该不会有问题，但出于谨慎，还是进行了专门的测试，不测不要紧，一测就测出了问题：Etcd watch stream starvation under high read response load when sharing same connection and TLS is enabled。 When etcd client is generating high read response load, it can result in watch response stream in the same connection being starved. For example a client with an open watch and running 10 concurrent Range requests each returning around 10MB. The watch might get starved and not get any response for tens of seconds. Problem does not occur when TLS is not enabled nor when watch is created on separate client/connection. This affects also K8s (any version) as Kubernetes has one client per resource. Problem will trigger when single K8s resource has a lot of data and there are 10+ concurrent LIST requests for the same resource send to apiserver. For example 10k pods. This can cause serious correctness issues in K8s, like controllers not doing any job as they depend on watch to get updates. For example scheduler not scheduling any pods. We tested and confirmed that all v3.4+ versions are affected. Issue affects any watch response type: Event Periodic watch progress notification Manual watch progress notification 在 TLS 模式下访问 Etcd 时，如果请求的数据量较大，可能会导致同一个连接的 watch stream 无法正常返回数据。尽管目前仍然是针对每种资源类型存在一个 Client 实例，但在针对同一个资源进行并行的 LIST 请求且资源量较大时，这个问题依然存在，且会影响 k8s 的所有版本。从某种程度上说，幸亏针对每个 Etcd 集群使用一个 Client 的功能还没有合入主干，否则这个问题出现的概率会被放大，大佬们对技术的谨慎态度值得我们学习。 经过大佬们的分析，产生此问题的原因在 Etcd 的实现，以及 golang 中对 HTTP/2 处理。 Problem occurs only when TLS is enabled. This is because grpc handler is served through http server, which doesn’t occur otherwise. http server is affected by net/http: Stream starvation in http2 priority write scheduler golang/go#58804 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:3:2","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"Golang 相关问题 问题是在 golang 1.19 中引入 PriorityWriteScheduler 开始的，在 golang/net@120fc90 修复，对应的 tag 为 v0.11.0。 When the children of a node have varying weights, walkReadyInOrder sorts the children of a node by the ratio of weight to bytes sent. This prevents starvation–a low-priority stream will get a smaller fraction of the available resources, but it’ll still get a fraction. However, when the children all have the same weight (the common case), walkReadyInOrder just picks the first available stream without consideration for how many bytes have been sent, permitting starvation. PriorityWriteScheduler 可以支持基于流的优先级的写的能力，但是当没有优先级的差异时，就会退化为 LIFO 的模式，新创建的流会优先得到处理，而已经创建一段时间并且发送了很多数据的流会排在靠后的位置等待处理。 社区提到了几种解决方案，比如使用 random write scheduler 或者 round robin write scheduler 替换 priority write scheduler，最终采取了 round robin write scheduler，一是因为 random write scheduler 虽然可以解决上述问题，但其性能并不是最优的，存在一些长尾延迟问题，再就是在最新的 RFC 9113 中已经废弃了 stream prioritization scheme，而这个概念是在 RFC 7540 引入的。更详细的讨论可以参考 https://github.com/golang/go/issues/58804#issuecomment-1470330633。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:3:3","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"Etcd 相关问题 Etcd 针对上述问题做了如下处理 [v3.4+] Change http server frame scheduler to random. [v3.4+] Move http server to separate port by passing --listen-client-http-urls. [v3.6+] Use customised connection multiplexer for etcd [v3.4+] When ready for production use, change to new frame scheduler round-robin algorithm. 先通过修改为 random write scheduler 来避免饥饿问题，同时通过暴露新的参数 --listen-client-http-urls 来解决 TLS 模式下通过 http server 处理 grpc handler 时受到 golang 影响的问题，最后把 random write scheduler 修改为 round robin write scheduler。而在 v3.6 及之后的版本中，会通过自定义 io 多路复用功能的方式替换了当前的实现（当前通过直接引用 cmux 包实现），由于这个改动较大，不适合放在已经正式 release 的 v3.4 和 v3.5 中。 最终在 v3.4.28 及以上、 v3.5.10 及以上的版本中问题得以彻底解决，如果还在使用 v3.4 或者 v3.5 较低版本的话，建议升级到上述版本。 这里重点提一下第二个改动，通过新增 --listen-client-http-urls 参数来规避问题，其原理是统一 etcd 服务端对 grpc 请求的处理，如果没有指定此参数，保持和之前实现一致的行为，如果同时定义了 --listen-client-urls 和 --listen-client-http-urls 则对前者使用 grpc server 对后者使用 http server 提供服务。为什么用了 random write scheduler 还要拆开呢？ Even with random write scheduler grpc under http server can only handle 500 KB with 2 seconds delay. On the other hand, separate grpc server easily hits 10, 100 or even 1000 MB within 100 miliseconds. 可以看到两种不同 server 类型的性能差异巨大，所以在升级 etcd 版本的基础上，设置好相关参数来为 TLS 也通过 grpc server 服务来提升性能，尤其是 k8s 集群规模较大的场景。 强烈推荐看一下第二个实现的 PR，里面涉及到如何设置这个两个参数，同时也可以感受到大佬们做 code review 时的细致，里面有一个非常细节的闭包导致的 bug，都被揪了出来，但由于与此实现无关，是一个历史 bug，就不在这里介绍了。 最后，也可以看出来其实并不是所有版本的 k8s 都会受此问题的影响的，取决于两个因素，影响也不同。 k8s 使用的 golang 版本如果有问题的话，影响的是客户端访问 kube-apiserver； etcd 使用的 golang 版本有问题的话，影响的是 kube-apiserver 从 etcd 获取数据； ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:3:4","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"小结 最终建议在 etcd 升级，修改相关参数的同时，也要注意下使用的 k8s 版本会不会有这个问题，主要就是看编译时使用的 golang 版本。 至此，我们知道了 kube-apiserver 和 etcd 建立连接的过程，对内存的影响，存在的问题以及如何修复等社区进展。由于 kube-apiserver 不管有多少实例，每个实例都需要与 etcd 建立连接，所以无法通过增加 kube-apiserver 实例数来缓解这个问题。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:3:5","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"client-go 与 kube-apiserver 的连接 除了 kube-apiserver 与 etcd 会建立连接外，客户端也会与 kube-apiserver 建立连接，那么维护这些连接会不会也会造成内存使用明显增长的问题呢？这就需要我们了解 client-go 建连机制了，可以借机看看自己是否真的搞懂 client-go 与 kube-apiserver 建立连接这部分的内容了。 client-go 通过 Config 结构维护访问 kube-apiserver 使用的配置，与此问题的对应的关键属性是 Transport，注意和上面 kube-apiserver 访问 etcd 提到的 transport 完全不一样的概念，不要混淆，后者是 kube-apiserver 内的自定义结构。如果没有显示的为 Config 设置 Transport，则其使用的是 http.DefaultTransport，默认使用 HTTP/2，考虑以下场景：未显示指定 Transport，通过 Config 创建了 clientset，使用 clientset 创建了 informerFactory，并同时针对不同的资源启动了对应的 informer，那么 client-go 会与 kube-apiserver 建立多少条连接呢？ 要回答这个问题，就需要了解 HTTP/2 io 多路复用的实现。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:4:0","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"HTTP/2 IO 多路复用 io 多路复用的意思是每个连接上有多个 stream，而上面提到的针对不同资源都使用 informer 与 kube-apiserver 建立连接，最终可能是复用了已经存在的连接，只不过在其中多了一个 stream 而已。这也是最开始提到 kube-apiserver 与 etcd 建立连接时提到的 Per Etcd Per Client 而不是 Per Etcd Per Connection 的原因，因为即使是一个 Client 最终到底会建立多少连接，和请求量有关，也和服务端 MaxConcurrentStreams 参数设置有关，这个参数表示每个连接上最多可以有多少流，默认为 250。假如使用默认值的情况下，客户端同时发起了 250 个请求，那理论上只需要 1 条连接就能搞定，但如果你去实际测试一下话，你会发现有的时候会有一个连接，有的时候会有两个连接，这是为什么呢？ 这就需要去理解 HTTP/2 的实现了，客户端在尝试与服务端建立连接时 首先获取连接，会先尝试从缓存中根据请求的 addr（host:port） 获取，如果没有获取到的话，会初始化一个连接（此时尚未与服务端建立连接，只是存在于客户端的一个数据额结构），每个连接也有个 maxConcurrentStreams 参数，默认值为 100，硬编码，无法修改； 接着会根据如下公式判断是否需要新建新的连接 maxConcurrentOkay = int64(len(cc.streams)+cc.streamsReserved+1) \u003c= int64(cc.maxConcurrentStreams)； 如果尚未与服务端建立连接的话（第一个 stream），就开始执行 Dial 的操作，之后与服务端建连成功后，先协商 setting，其中就包含了上面提到的服务端的 MaxConcurrentStreams 参数，会覆盖了连接初始时的 maxConcurrentStreams 的值，协商只有在每个连接第一次与服务端建立连接时进行，之后复用这个连接的时候不再需要重新协商的过程； 之前每有请求过来都是重复上面的步骤，按需创建新的连接。 并发 250 个请求，有时是一个连接，有时是两个连接的原因：由于请求是并发创建的，会出现在某一时刻一批请求同时执行到了步骤二，而这时客户端与服务端协商虽然在第一个请求到来后就已经开始，但涉及到网络传输有可能协商并未完成，也就是说客户端的 maxConcurrentStreams 还是 100，所以当第 101 个请求来的时候，客户端判断觉得需要新建一个连接了，然后就新建了一个连接，之后协商完成，客户端本地的 maxConcurrentStreams 变成了 250，后续的请求到来的时候就又可以继续复用已有的连接了。最终就会造成实际的连接数 \u003e= 请求并发 / MaxConcurrentStreams，如果协商在第 101 个请求到来之前已经完成，那么创建的连接数就是计算之后的结果，不会多。 如果先发起一个请求，得到返回结果后再并发创建 249 个请求的话，那么连接数就会始终是 1，因为在第一个请求里面已经完成了协商，客户端的 maxConcurrentStreams 已经被设置了 250，正好可以处理剩下的 249 个请求。 kube-apiserver 早期为 MaxConcurrentStreams 设置了默认值 250，并且暴露了参数可以在外部修改，而在 1.29 的发布中，将其默认值修改为了 100，同时 backport 回了从 v1.25 及之后的所有的版本，这个修改和 golang 的安全漏洞 CVE-2023-44487 and CVE-2023-39325 有关。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:4:1","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"CVE-2023-39325 一个恶意的 HTTP/2 客户端，迅速创建请求并立即重置它们，可能导致服务器资源的过度消耗。尽管请求的总数受限于 http2.Server.MaxConcurrentStreams 设置，但在处理中的请求被重置允许攻击者在现有请求仍在执行时创建新请求。 有了上面的分析，理解这个漏洞及修复就会简单一些。漏洞产生的原因是虽然客户端在与服务端协商后会拿到服务端参数 MaxConcurrentStreams 的值，可以保证客户端单个连接上的 active stream 数量不超过这个阈值，但是有可能会导致服务端对应连接上的 stream 数量超过这个阈值。因为客户端虽然重置了请求，但可能请求在服务端还在运行中，客户端重置之后立马就可以重新发起请求，最终导致服务端资源被消耗殆尽，算是服务端实现的一个漏洞。 在 golang/net v0.17.0 中修复此漏洞，http2: limit maximum handler goroutines to MaxConcurrentStreams。实现原理是在服务端添加相关逻辑，判断执行中的请求是否超过了 MaxConcurrentStreams 的值，没有超过的话就直接执行，超过的话则会入队列，等到运行中的某个请求完成之后再从队列中取一个请求开始处理。如果队列里面积压的未处理请求数量超过 MaxConcurrentStreams 的 4 倍的话，服务端会报错 too_many_early_resets 并返回 ErrCodeEnhanceYourCalm，然后强制关闭连接。 golang 的上述修复 packport 到了 golang v1.20 和 v1.21 中，之前版本的 golang 仍然存在这个问题，但是由于 golang 版本维护策略只维护最新的两个版本，如果使用的还是之前 v1.19 或者之前的版本的话，可以升级 golang 版本，或者升级 x/net 版本到 v0.17.0 来解决。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:4:2","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":"小结 Kube-apiserver 刻意把 MaxConcurrentStreams 从 250 缩小到 100，是为了降低服务端可能受到的影响，在受到攻击时关闭连接的时候每个连接上最多 100 * 4 个请求，如果还是用 250 的话，就会有 250 * 4 个请求。 至此，应该就可以回答本节最开始提出的在未显示指定 Transport，通过 Config 创建了 clientset，使用 clientset 创建了 informerFactory，并同时针对不同的资源启动了对应的 informer，client-go 会与 kube-apiserver 会建立多少条连接的问题了。kube-apiserver 的连接多数来自于 daemonset 性质类的客户端请求，有了 io 多路复用之后，单个 agent 进程发起的请求可以通过少量的连接即可完成，这可以大大减少 kube-apiserver 的连接数，同时由于 kube-apiserver 可以多实例负载均衡（虽然不是那么的均衡），但至少可以通过增加实例数来分担单个实例上连接数过多的风险，这和 kube-apiserver 作为客户端去和 etcd 建立连接的场景完全相反。 总结 本篇分别从 kube-apiserver 连接 etcd，client-go 连接 kube-apiserver 的角度分析了连接数对内存的影响，在此过程中又引出了一连串的 Etcd，Golang，HTTP/2 中存在的一些问题和漏洞，这些问题都是优化连接数时不得不考虑或者提前解决的问题。同时也给出了问题引入和修复的方式或者版本，目前线上主流运行中的 k8s 集群和使用的 etcd 大部分应该都有相关问题，可以考虑按需升级处理。 ","date":"2023-12-18","objectID":"https://www.likakuli.com/posts/connection/:4:3","tags":["kubernetes"],"title":"kube-apiserver 连接数也会影响内存？","uri":"https://www.likakuli.com/posts/connection/"},{"categories":["源码分析"],"content":" 本篇介绍一个可能连骨灰级 K8S 玩家都很少关注的属性 - SelfLink，以及这么一个名不见经传的小趴菜是怎么影响 kube-apiserver 性能的。大家可能没注意过他，但在分析 kube-apiserver 内存消耗时肯定见到过令人闻风丧胆的 DeepCopyObejct，本文会介绍他们之间的连续 背景 前面已经有了一系列有关 informer 和 kube-apiserver 的文章，定位和内容深度偏中上，如果没有基础的话可能不容易看懂。本篇介绍一个可能连骨灰级 K8S 玩家都很少关注的属性 - SelfLink，以及这么一个名不见经传的小趴菜是怎么影响 kube-apiserver 性能的。 作用 SelfLink 到底干什么的呢？ SelfLink is a URL representing a given object. It is part of ObjectMeta and ListMeta which means that it is part of every single Kubernetes object. SelfLink 从哪个版本引入的，存在的意义是什么，已经不得而知了，但可以明确的是从 v1.24 开始被正式废弃了。那为什么还要提一下他呢，因为他虽已不在江湖，但江湖上仍流传着他的传说。这个属性不仅没什么作用，甚至还有一些副作用，尤其你如果使用的是 v1.24 前的版本，更要注意下。 被废弃 被废弃的原因有两点： 在 generic-apiserver 中以非常特殊的方式处理 - 它是在序列化对象之前设置的唯一字段（因为这是设置它所需的所有必要信息的唯一位置）； 具有不可忽略性能影响 - 构造该值需要执行几次内存分配； 原理 ","date":"2023-12-13","objectID":"https://www.likakuli.com/posts/selflink/:0:0","tags":["kubernetes"],"title":"k8s 骨灰级玩家都不知道的属性 - SelfLink","uri":"https://www.likakuli.com/posts/selflink/"},{"categories":["源码分析"],"content":"DeepCopyObject 上一篇 kube-apiserver 内存优化进阶 中介绍了从序列化的角度去优化 kube-apiserver 的内存使用，还不懂的建议先看前一篇。SelfLink 的处理就在序列化之前。 // ServeHTTP serves a series of encoded events via HTTP with Transfer-Encoding: chunked // or over a websocket connection. func (s *WatchServer) ServeHTTP(w http.ResponseWriter, req *http.Request) { ... for { select { case \u003c-done: return case \u003c-timeoutCh: return case event, ok := \u003c-ch: ... obj := s.Fixup(event.Object) if err := s.EmbeddedEncoder.Encode(obj, buf); err != nil { // unexpected error utilruntime.HandleError(fmt.Errorf(\"unable to encode watch object %T: %v\", obj, err)) return } ... } } } // TODO: the functionality in this method and in WatchServer.Serve is not cleanly decoupled. func serveWatch(watcher watch.Interface, scope *RequestScope, mediaTypeOptions negotiation.MediaTypeOptions, req *http.Request, w http.ResponseWriter, timeout time.Duration) { ... server := \u0026WatchServer{ ... Fixup: func(obj runtime.Object) runtime.Object { result, err := transformObject(ctx, obj, options, mediaTypeOptions, scope, req) if err != nil { utilruntime.HandleError(fmt.Errorf(\"failed to transform object %v: %v\", reflect.TypeOf(obj), err)) return obj } // When we are transformed to a table, use the table options as the state for whether we // should print headers - on watch, we only want to print table headers on the first object // and omit them on subsequent events. if tableOptions, ok := options.(*metav1.TableOptions); ok { tableOptions.NoHeaders = true } return result }, TimeoutFactory: \u0026realTimeoutFactory{timeout}, } server.ServeHTTP(w, req) } // transformObject takes the object as returned by storage and ensures it is in // the client's desired form, as well as ensuring any API level fields like self-link // are properly set. func transformObject(ctx context.Context, obj runtime.Object, opts interface{}, mediaType negotiation.MediaTypeOptions, scope *RequestScope, req *http.Request) (runtime.Object, error) { if co, ok := obj.(runtime.CacheableObject); ok { if mediaType.Convert != nil { // Non-nil mediaType.Convert means that some conversion of the object // has to happen. Currently conversion may potentially modify the // object or assume something about it (e.g. asTable operates on // reflection, which won't work for any wrapper). // To ensure it will work correctly, let's operate on base objects // and not cache it for now. // // TODO: Long-term, transformObject should be changed so that it // implements runtime.Encoder interface. return doTransformObject(ctx, co.GetObject(), opts, mediaType, scope, req) } } return doTransformObject(ctx, obj, opts, mediaType, scope, req) } func doTransformObject(ctx context.Context, obj runtime.Object, opts interface{}, mediaType negotiation.MediaTypeOptions, scope *RequestScope, req *http.Request) (runtime.Object, error) { ... if err := setObjectSelfLink(ctx, obj, req, scope.Namer); err != nil { return nil, err } ... } 总结下就是 WatchServer 中包含一个 FixUp 方法，用来在序列化最终要返回的 event 之前做一些操作，其中就包括为 event.object 设置 SelfLink，这是一个非标操作，整个过程只有他在序列化前做了特殊处理。 但为什么设置 SelfLink 会对性能有影响呢，不就是一个简单的赋值操作吗？这就涉及到上一篇介绍过的 CacheableObject 对象了，对于非 CacheableObject 没有影响。上一篇中我们提到了一个神奇的地方 cacheWatcher 的 input chan 的 event 对象的 object 有可能是正常的资源对象，例如 Pod，也有可能是 CacheableObject 对象，而真正的资源对象则保存在 CacheableObject 的 object 中 所以上面代码 transformObject 在处理时也判断了类型，我们只关注 CacheableObject 这部分 doTransformObject(ctx, co.GetObject(), opts, mediaType, scope, req) 其中调用了 co.GetObject() 从 CacheableObject 中获取真实的资源对象（如 Pod），而这个方法会对资源对象执行一个 deepcopy 操作 // GetObject implements runtime.CacheableObject interface. // It returns deep-copy of the wrapped object to return ownership of it // to the called according to the contract of the interface. func (o *cachingObject) GetObject() runtime.Object { o.lock.RLock() defer o.lock.RUnlock() return o.object.DeepCopyObject().(metaRuntimeInterface) } 针对 v1.24 之前的版本，在创建 cachingObject 对象时就已经对资源对象做了一次 deepcopy 操作了 // newCachingObject performs a deep copy of the given object and wraps it // into a cachingObject. // An ","date":"2023-12-13","objectID":"https://www.likakuli.com/posts/selflink/:1:0","tags":["kubernetes"],"title":"k8s 骨灰级玩家都不知道的属性 - SelfLink","uri":"https://www.likakuli.com/posts/selflink/"},{"categories":["源码分析"],"content":"缘起 严格的说，上面刚提到的\"两次\"其实并不严谨，更准确的说是一到两次。这里就又涉及到了一个细节： func setCachingObjects(event *watchCacheEvent, versioner storage.Versioner) { switch event.Type { case watch.Added, watch.Modified: if object, err := newCachingObject(event.Object); err == nil { event.Object = object } else { klog.Errorf(\"couldn't create cachingObject from: %#v\", event.Object) } // Don't wrap PrevObject for update event (for create events it is nil). // We only encode those to deliver DELETE watch events, so if // event.Object is not nil it can be used only for watchers for which // selector was satisfied for its previous version and is no longer // satisfied for the current version. // This is rare enough that it doesn't justify making deep-copy of the // object (done by newCachingObject) every time. case watch.Deleted: // Don't wrap Object for delete events - these are not to deliver any // events. Only wrap PrevObject. if object, err := newCachingObject(event.PrevObject); err == nil { // Update resource version of the object. // event.PrevObject is used to deliver DELETE watch events and // for them, we set resourceVersion to \u003ccurrent\u003e instead of // the resourceVersion of the last modification of the object. updateResourceVersion(object, versioner, event.ResourceVersion) event.PrevObject = object } else { klog.Errorf(\"couldn't create cachingObject from: %#v\", event.Object) } } } Cacher 消费其 incoming chan，在去给每个筛选后的 cacheWatcher 发送 event 时，会执行 setCachingObjects 将资源对象做个封装，封装成 cachingObject，并将 cachingObject 设置为 event 的 Object 或者 PrevObject，具体设置到哪个属性上面，和 event type 有关，可以看上述代码，对于非 Deleted 类型，设置的是 event.Object，而对于 Deleted 类型的事件来说，设置的是 PrevObject，因为 Deleted 事件本身并不具备有效负载。这又和之前提的相呼应 cacheWatcher 的 input chan 的 event 对象的 object 有可能是正常的资源对象，例如 Pod，也有可能是 CacheableObject 对象，而真正的资源对象则保存在 CacheableObject 的 object 中 继续看上面的代码，在处理 Deleted 类型时，还做了一个操作 updateResourceVersion，其作用是将 event 的 resourceVersion 设置给 object，为什么要做这个处理呢？不知道大家还记不记得之前那篇 kubernetes 月光宝盒 - 时间倒流 的文章，这里正好对应了里面介绍的第二个时间回溯现象 PrevObject 的 ResourceVersion 是一个过去时的值，例如连续创建三个 Pod，再删除第一个 Pod，则此时返回的 watchEvent 的 Object 对象的 ResourceVersion 是第一个 Pod 创建后的 ResourceVersion，但实际情况是最新的 ResourceVersion 已经随着后两个 Pod 的创建而递增了。也就是此时客户端 watch 到一个 delete event 之后，客户端 informer 所使用的 reflector 内部维护的 resourceVersion 已经不对了，是一个历史值，如果此时发生一个问题（例如网络闪断）需要去重新 watch 时，会使用这个错误的 resourceVersion，也就是这个版本之后的所有符合条件的事件会再次接收并处理一遍，真正的回到过去，历史重演。 在 v1.9 修复这个\"惊天大 bug” 的时候，是在 cacheWatcher 消费自己 input chan 中的 event 发送给其 result chan 时处理的 func (c *cacheWatcher) convertToWatchEvent(event *watchCacheEvent) *watch.Event { if event.Type == watch.Bookmark { return \u0026watch.Event{Type: watch.Bookmark, Object: event.Object.DeepCopyObject()} } curObjPasses := event.Type != watch.Deleted \u0026\u0026 c.filter(event.Key, event.ObjLabels, event.ObjFields) oldObjPasses := false if event.PrevObject != nil { oldObjPasses = c.filter(event.Key, event.PrevObjLabels, event.PrevObjFields) } if !curObjPasses \u0026\u0026 !oldObjPasses { // Watcher is not interested in that object. return nil } switch { case curObjPasses \u0026\u0026 !oldObjPasses: return \u0026watch.Event{Type: watch.Added, Object: getMutableObject(event.Object)} case curObjPasses \u0026\u0026 oldObjPasses: return \u0026watch.Event{Type: watch.Modified, Object: getMutableObject(event.Object)} case !curObjPasses \u0026\u0026 oldObjPasses: // return a delete event with the previous object content, but with the event's resource version oldObj := getMutableObject(event.PrevObject) // We know that if oldObj is cachingObject (which can only be set via // setCachingObjects), its resourceVersion is already set correctly and // we don't need to update it. However, since cachingObject efficiently // handles noop updates, we avoid this microoptimization here. updateResourceVersion(oldObj, c.versioner, event.ResourceVersion) return \u0026watch.Event{Type: watch.Deleted, Object: oldObj} } return nil } 可以看到在最后一个 case 分支的处理逻辑中，仍然存在一个 updateResourceVersion 的操作，仍然是使用了 event.ResourceVersion 去覆盖了 oldObject 也即是上面提的 PrevObject 的 ResourceVersion 属性。在 v1.9 中其实只有后面这一次的调用，而在 v1.24 中存在着两次调用，上面代码中也特别注释了如果 oldObje","date":"2023-12-13","objectID":"https://www.likakuli.com/posts/selflink/:2:0","tags":["kubernetes"],"title":"k8s 骨灰级玩家都不知道的属性 - SelfLink","uri":"https://www.likakuli.com/posts/selflink/"},{"categories":["源码分析"],"content":"到底几次 所以到底是少了几次 deepcopy 呢？ 这要根据 event type 来看，对于非 Deleted 类型的 event 来说，由于他们并不需要执行 updateResourceVersion 的操作，也就不会触发 deepcopy，而 SelfLink 是需要两次 deepcopy 的，所以是多了两次 deepcopy，而针对 Deleted 类型来说，由于其本身就需要一次 deepcopy（虽然两次 updateResourceVersion，但第二次属于 noop，直接返回），所以只是多了一次 deepcopy。 综上，对于 Deleted 类型的 event，去掉 SelfLink 可以减少一次资源对象的 deepcopy，而针对其他类型的 event，去掉 SelfLink 则可以减少两次 deepcopy。 总结 通过分析 SelfLink，我们又重新复习了 Cacher、cacheWatcher 甚至远古时期的惊天大\"bug\"，分析了到底他对性能有什么影响，去掉他之后优化了多少。 不知道你有没有发现，过去这么多篇的文章并不是独立的，他们之间总是存在着千丝万缕的联系，而这些关系通过各种细节串联起来，点到线，线到面，最后成为一个网，或者一个图，成为知识体系。他不仅具备空间属性，还具备时间属性。如果你不知道之前版本的问题或者实现的话，单看当前的实现可能不容易体会到他的巧妙之处，或者印象并不深刻。 从问题中来，沿着时间轴把相关的线索都梳理一遍，享受把这些蛛丝马迹串起来的过程，享受知识体系化的过程，体会并享受大佬们解决问题的思路。 谁说 K8S 越来越复杂？我看挺有意思的嘛，而且移除 SelfLink 不也是在做减法。 最后，针对上述描述如有问题，或者想交流的话，可以添加笔者 VX：YlikakuY，班门弄斧，欢迎互喷~ ","date":"2023-12-13","objectID":"https://www.likakuli.com/posts/selflink/:3:0","tags":["kubernetes"],"title":"k8s 骨灰级玩家都不知道的属性 - SelfLink","uri":"https://www.likakuli.com/posts/selflink/"},{"categories":["思考"],"content":" 从容器调度作为入口，尝试看清问题的本质。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:0:0","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"容器调度 前一篇中介绍了大规模容器平台的资源预算、精算、核算，本篇围绕容器调度开展。容器调度旨在有效地协调和管理系统中的有限资源，在保障服务 SLO 的前提下，满足任务和服务的需求。对于从事过容器调度相关工作的朋友来说，对超卖一定不陌生。容器平台一般都会对运行在其中的服务做一定程度的超卖，来达成平台本身的盈利目标。 如果是使用 K8S 的话，K8S 在每个 Container 中定义了 Request 和 Limit 两个属性分别用来代表单个 Container 可以使用的资源上限（Limit），以及调度侧调度器在进行调度时使用的资源大小（Request）。 业务级别的超卖一般指的是业务所申请的资源规格（Limit）大于 K8S 在调度时所使用的值（Request），整机级别的超卖则是其上所有业务的资源规格（Limit）之和大于整机的容量（Capacity）。 以 CPU 这种可压缩资源为例，业界常见的超卖做法如下： 基于 Request 和 Limit 的静态超卖，支持在不同维度自定义超卖比，比如业务申请 8c，调度时按 4c 计算； 基于真实负载的调度，即调度考虑服务历史运行周期的资源使用情况，使用一些预算算法预测未来的资源使用情况，跟据这些值来进行调度，比如业务申请 8c，但根据计算和预测，其最高只会用到 3c，调度时按照 3c 计算； 这是两种常见的调度策略，还存在基于以上两类调度策略的众多变种。但大部分的调度策略中都忽视了一个问题：如何保障服务的 SLO。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:1:0","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"宏观控制面 K8S 源自 Google Borg，在设计之初就考虑到了这一点，通过 Request 和 Limit 两个值提供 QoS 机制，把保障业务 SLO 的能力暴露出去，交给 K8S 使用者来实现。同时提供了三种类型的 QoS：Guaranteed，Burstable，BestEffort。 Guaranteed：Pod 内的所有 Container（包括 InitContainer） 都配置了 Request 和 Limit，且其 CPU、Memory 值均相等； BestEffort: Pod 内的所有 Container（包括 InitContainer） 都没有配置 Request 和 Limit； Burstable：除上述两种情况之外的所有场景； Borg 则是定义了诸多的优先级实现类似的效果，为了保持原汁原味，这里使用英文版的内容，如下 Free tier: jobs running at these lowest priorities incurno internal charges, and have no Service Level Ob-jectives (SLOs). 2019 trace priority\u003c=99; 2011 tracepriority bands 0 and 1. Best-effort Batch (beb) tier: jobs running at thesepriorities are managed by the batch scheduler andincur low internal charges; they have no associatedSLOs. 2019 trace priority 110–115; 2011 trace prioritybands 2–8. Mid-tier: jobs in this category offer SLOs weaker thanthose offered to production tier workloads, as well aslower internal charges. 2019 trace priority 116–119;not present in the 2011 trace. Production tier: jobs in this category require highavailability (e.g., user-facing service jobs, or daemonjobs providing storage and networking primitives);internally charged for at “full price”. Borg willevictlower-tier jobs in order to ensure production tier jobsreceive their expected level of service. 2019 trace pri-ority 120–359; 2011 trace priority bands 9–10. Monitoring tier: jobs we deem critical to our infras-tructure, including ones that monitor other jobs forproblems. 2019 trace priority\u003e=360; 2011 trace priority band 11. (We merged the small number of monitor-ing jobs into the Production tier for this paper.) 服务优先级已经从 0-11 扩充到了 0-450。 以上摘自 Borg：the Next Generation ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:1:1","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"微观数据面 QoS 是对 SLO 宏观层面的定义，而操作系统为不同 QoS 的 Pod 提供微观层面的 SLO 保障和隔离能力。Kubelet 对外暴露了绑核、NUMA 亲和等能力，针对 Guaranteed 类型的 Pod，其每个 Container 会独占 CPU，而所有非 Guaranteed Pod 的 Container 将会共享（也做了绑核，绑的是余下所有的核，包括预留的核）剩余的所有核，同时 Kubelet 也会根据 CPU Request 和 Limit 的值设置 CPU shares 和 quota 的值（cgroup v1 v2 对应不同的文件）。 也可以通过扩展 Kubelet 或者完全自研的方式，配合 kernel，实现更丰富的隔离、压制能力等，例如基于 RDT 的超线程隔离，基于 cgroup identity 的绝对压制能力等。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:1:2","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"能力层 前面涉及到的是基础能力，但怎么把这些基础能力暴露出去，怎么合理的使用这些基础能力，对调度效果也有至关重要的影响。一般情况下，K8S 并不会直接暴露给业务，在 K8S 之上会再封装一层对业务暴露。接受用户输入的资源规格和调度策略，转化为 K8S 可识别的参数，最终由 K8S 调度器做调度的决策，将资源分配给业务。 理想情况下，后台功能实现应该最大限度降低用户对其感知，通过完善的流程、机制和技术，为服务推荐对应的调度策略，规格和副本数，甚至自动化进行相关参数的设置和应用，同时在运行过程中遇到特殊情况可以及时的进行必要的重调度。 还有一个非常现实的问题，如何衡量调度效果的好坏或者收益。通常情况下大家直接使用使用率去衡量，但是使用率并不适合单独使用，还要结合业务 SLO 的指标。举个例子，即使在什么都不做的情况下，如果业务萧条，那使用率自然会低，如果业务增长，使用率自然会高。Borg 提供了一个用来衡量调度效果的方法：集群压缩比，即以业务实际历史运行来模拟，逐步挑选集群中的机器下线，直到找不到任何一台可以下线的机器停止，可以用总的机器数 / 最终机器数计算一个压缩比，这个值越大，说明浪费越严重。 在生产环境中，一般会刻意保留相当大的余地，以应对工作负载的增长、偶发的“黑天鹅”事件、负载峰值、机器故障、硬件升级以及大规模的部分故障（例如电源总线故障）。 这一层在实现的过程中会被简单化处理，致使 K8S 沦为简单的 CD 系统，有点买椟还珠或者杀鸡用牛刀的意思。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:1:3","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"超卖 如果资源足够多，那调度系统就没有存在的必要。调度往往都是为了解决业务运行需要的资源与平台可以提供的资源之间的矛盾而存在的，最终超卖会成为常态。 Borg It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machinesharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. 摘自 Large-scale cluster management at Google with Borg Borg 在实现超卖的时的逻辑是其并不会对高优先级服务做超卖（资源申请 Limit \u003e node capacity），甚至会少卖，以保证高优先级服务在需要其所申请的资源量的时候是可以获取到的。超卖的是上图中的回收资源，供一些低优先级服务使用。 K8S 对应最开始提到了 K8S 里面常用的基于静态 Request 和 Limit 值的超卖，以及为了降低超卖风险而增加的基于实际资源利用率的限制。相当于以实际使用率兜底，通过逐步扩大超卖比例来实现释放资源，提升资源使用率。 这种实现会有如下问题： 随着部署密度和使用率的提升，资源争抢会越发严重，而如果忽略服务等级，单纯使用 K8S 的 Request 值作为 cpu share 争抢依据，那么很有可能会出现业务高峰期低优先业务抢占太多高优先业务的 CPU 资源的情况（低优服务的 Request 值大），得不偿失； 高优先的服务真正需要使用到其所申请的资源量时，系统无法提供； 如果由于上述原因出现故障，那就是平台的原因了。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:1:4","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"降本增效 随着互联网大厂故障频出，众多的吃瓜群众矛头直指降本增笑，来讽刺去年众多的裁员（人和机器）、年终打（骨）折等降本增效的行为。但这真的是造成故障的原因吗，或者说吃瓜群众真正笑的是什么？ 降本增效是大势所趋，在这样的大环境下，各行各业似乎已经就此达成了共识。公司不是慈善机构，要保障其\"业务连续性\"，做出这种选择本也无可厚非。从实际的情况来看，在某些时间段内确实也起到了降本的功效，只不过这个过程中也伴随着一些\"副作用\"。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:2:0","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"降本 降本，意味着投入的减少，资源的减少，无论是人力、还是非人力（软件、硬件、运营、年终等）。 这都比较容易理解，比如机器少了，那本可能双活的，被搞成了单活，本来预留的应对黑天鹅事件的 buffer 被压榨，都可能导致系统无法提供业务真实的资源需求。比如裁员，可能会导致本该有的环节的缺失（QA），或者人力不足，或者人员能力缺失等造成一些潜在风险，埋下定时炸弹。又或者年终打折，这多少也会导致员工积极性的下降，甚至有一些个超出预料的行为。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:2:1","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"不降本 目前来看并没有证据说明这些故障就是降本造成的，而且在没有降本之前，并不是没有故障的发生。已经发生的问题只是整个稳定性风险里的冰山一角，降本在某些情况下可能只是催化剂，加速了故障的发生。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:2:2","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["思考"],"content":"要解决的问题 对于一个公司来说，他所做的所有决策都是有一个大前提的：业务的连续性。就如同上面讲的容器调度一样，也有一个大的前提是保障服务的稳定性，而服务的稳定性正是业务连续性的一环。 见识过去年互联网行业各厂的裁员、年终打折，裁员可能还可以解释，毕竟裁掉的一般也都是基于业务优先级考虑之后的那部分，年终打骨折这种行为，就跟容器调度里面的无差别超卖一个道理了。都是在资源紧张的情况下，都是在采用无差别\"超卖\"，结果都对稳定性产生了一定的影响。 降本增效势在必行，为了解决问题，优先级和账单是必须的，尽可能在降低影响的前提下还能有不错的效果，而这恰恰是不完善甚至不具备的能力。完善基础数据，并基于此健全成本优化决策能力，莫让降本增效真的变成降本增\"笑\"。 ","date":"2023-12-11","objectID":"https://www.likakuli.com/posts/overcommit/:2:3","tags":["思考","随笔"],"title":"从容器调度到降本增效","uri":"https://www.likakuli.com/posts/overcommit/"},{"categories":["随笔"],"content":"近期互联网故障频发，各大公众号各抒己见，指点江山，激扬文字，颇有百花齐放，百家争鸣的味道。一众吃瓜群众也是对此乐此不疲，津津乐道。 大家能够积极的从故障中去学习，这本身是一件好事，再加上一众业界大佬们颇有深度的观点和经验分享，可以让我们看到大佬们的思维方式，认识到和大佬们的差距。问渠那得清如许，为有源头活水来，向优秀者学习是提升自己的一个有效途径。 在这个过程中我也看到了一些现象，想和大家一起探讨探讨。笔者之前曾有幸参加了滴滴弹性云平台的建设工作，对里面的情况还算有一定的了解，虽然已经离开了两年多的时间。 这里笔者并不会介绍真实原因，也不会涉及到什么技术问题，单纯就目前看到的一些文章表达一下自己的观点，可以理解为本篇并不是阐述从滴滴的故障中我们能学到什，已经有众多大佬分析过了，也已经比较全面了。这里主要想跟大家一起探讨下我们究竟从公众号或者自媒体的文章里面学到了什么，或者当我们看到这些内容的时候有哪些是需要特别注意的地方。 脱离实际 先不说故障原因是否真的如网上所说，假设事故原因就是升级造成的，里面的一些分析也有值得商榷的地方。想就两点来探讨： 选择原地升级方案就是草台班子吗？ 爆炸半径太大了吗？ ","date":"2023-12-06","objectID":"https://www.likakuli.com/posts/bomb/:0:0","tags":["思考","随笔"],"title":"让子弹飞一会儿","uri":"https://www.likakuli.com/posts/bomb/"},{"categories":["随笔"],"content":"原地升级就是\"草台班子\"吗? Kubernetes 1.12是2018年7月发布的，在2019年7月8日就终止支持了。也就是说在2023年10月的时候，滴滴在生产环境使用一个社区已经废弃4年多的老版本。不论是看可靠性，还是看安全性，这都是非常草台班子的做法。 在五年没有升级版本的历史债务下，滴滴团队决定孤注一掷，直接从1.12 升级到 1.20。但是 Kubernetes Deprecation Policy[5] 很清楚的说了 Beta API 有可能在三个小版本之后就被删除。一次性升8个版本，风险完全不可控，也完全没有必要。 在冒险跨版本升级的前提下，还进一步选择原地升级，断绝 Plan B 的可能性，是一个更加费解的策略。实际上 Kubernetes 集群的蓝绿升级非常成熟，出问题的时候也更容易挽救。原地升级除了可以节约几个机器资源之外，没有其他好处。而升级Node不重建容器的做法，极容易引入兼容性问题，给自己的安全/调试/性能都带来极大的困难。 脱离滴滴的实际情况去谈 k8s 升级的话，笔者非常认同上述说法。但如果考虑到实际情况的话，就显得有些站不住脚了。 首先，滴滴内部对 k8s 做了大量的定制化工作，一来是为了兼容之前物理机时代业务（用户）的使用习惯，降低上云过程中引入的体验上的差异，套用俞军老师的公式：产品价值 =（新体验 - 旧体验）- 迁移成本；二来是因为 k8s 提供的更多的是一些通用能力，而在具备一定规模的公司在使用过程中或多或少会有一些既有功能无法满足存量需求的情况。 滴滴内部的一些定制比如为 Pod 固定宿主机，固定容器 IP 等功能（这些都可以在网上搜索到，之前已经有不少的相关分享）在很大程度上限制了升级方案的选型，甚至影响了故障恢复方案的选型。倒不是说不能采用社区的蓝绿，而是也需要做一些相关准备，不是单纯新搭建一套集群直接就能去迁移业务过去的，我相信这一点大佬们应该也是对比了两个方案，做了充分评估之后才选择的原地升级方案。而且在更老的版本升级的时候也才用过过原地升级的方案。如果了解了现状之后，可能就会觉得原地升级的方案也是存在一定的可取之处的，虽然风险确实高，但这个风险是相对的，跟人的能力有关，只要风险可控，那选择原地升级也无可厚非。 当然积累这么多个版本不升级，确实可以算一个问题，但如果有节奏的升级，升级次数多了即使单次升级出问题的概率变小了，但是乘以升级次数之后得到的从某个初始版本到最终版本的出问题的期望值是否真的会变小呢，这不好衡量，唯一可以确定的是这样做的话单次升级的风险更低了。 看过火影忍者的小伙伴可能会有印象，作为医疗忍者体系的开创者，纲手定下了医疗忍者必须遵守的三项铁律，但是纲手是可以打破这三项铁律的，因为她还立下了第四条规则，那就是学会了百豪之术的医疗忍者，可以打破以上所有的规则。 方案选型，风险大小都是相对的，既要考虑常规做法，也要考虑人员能力，最重要的还是要结合现状，切忌脱离实际的空谈。官网给出的指导，业界的通用方案是怎么做的，那些都是基础，如果忽略实际情况完全按照这些来那就有点主观主义的味道了，完全按照官方文档来，那就是教条（本本）主义，按照经验来，那就是经验主义，主观主义的两个极端。实践是检验真理的唯一途径，虽然出了问题，但并不是在升级过程中出现的，从 1.12 直接升级 1.20 也不是不可行的，已经在客观实践中得到了检验证明这是行得通的，只是需要更加完善的流程，先把三板斧做好再上线。来自苏联的德国军事顾问李德指挥的第五次红军反围剿失利，损失惨重，倒是一直被打压，被嘲讽的人站了出来挽狂澜于既倒，扶大厦之将倾。 ","date":"2023-12-06","objectID":"https://www.likakuli.com/posts/bomb/:1:0","tags":["思考","随笔"],"title":"让子弹飞一会儿","uri":"https://www.likakuli.com/posts/bomb/"},{"categories":["随笔"],"content":"爆炸半径 Kubernetes的多集群管理工具已经非常成熟了，而且滴滴其实有多集群，所以维护超大集群没有任何意义。作者自己也清楚“爆炸半径大”，那么合理的做法就是把集群拆成合理大小的。比如把两个一万节点的集群拆成十个两千节点的集群，管理成本没有增加，而运行风险和爆炸半径得到极大的降低。 首先，笔者作为 karmada member，之前也在公司内做过一些多集群管理的实践和落地的工作，对\"多集群管理工具已经非常成熟“这个结论持保守态度，还是要看内部需求，再成熟的软件也是去为了满足需求，解决问题的。业界开源的多集群管理工具里面，karmada（CNCF首个多云容器编排项目） 算是做的很好的了，其他还有例如 clusternet（CNCF sandbox 项目），OCM 等，笔者在落地过程中也遇到了一些功能和性能方面的问题，当然这与所在公司实际业务场景有关，也感谢 karmada 社区小伙伴的积极响应和支持。 “滴滴其实有多集群” 这个结论可能也是受到了网上那些公众号的影响了，不能说没有，但可能和我们认知里面的多集群管理不一样。顶多是支持业务部署到多个集群里面而已，在没有这些多集群管理工具的时候，业务做部署/变更时只要选择多个集群，也是可以部署到多个集群里面的，这是\"伪\"多集群。这里就不得不再提一下国内的技术氛围，充斥着 PPT 实现，笔者之前在被面试和面试别人的时候都遇到过这些，比如早些年很多公司对外宣称的在 3 系内核上已经支持了 buffer io 隔离和限速的能力，大部分是 PPT 实现而已。 “维护超大集群没有任何意义\"，这个结论有待商榷，从事调度和资源利用率提升相关工作的小伙伴们应该会对此深有感触，统一资源池有利于资源的统筹规划，提升资源利用效率，从而降低成本。我们需要辩证的去看问题，即使是有了多集群管理能力了，目前的多集群实现方案，无论哪个，在遇到根据集群剩余资源调度时有一个共通的技术问题：多集群管控面的调度只是到了集群维度，或者说对应的集群里面需要扣减一定的资源，但调度本身是和集群每个节点的资源有关的，从工作负载在控制面调度完下发到业务集群，到工作负载在业务集群完成调度的这段时间内发生在控制面的其他调度行为，其决定是有潜在问题的，因为这段时间在控制面看到的业务集群的真实资源还没有完全扣减了被下发的工作负载的消耗。类比集群内的调度，单集群的调度器通过 assume 的机制来解决这个问题，即单集群调度完就已经知道容器对应的宿主机了，他提前在调度器缓存内为对应的宿主扣除这个容器所占用的资源，而不用等真正的 Bind 完成，调度器收到 pod update 事件后更新完 cache 再去调度；多集群的调度就没法这么做了，因为控制面的调度决策完全不知道最终业务集群的调度结果，他需要等到业务集群里面真的调度完才能才能知道业务集群资源分布情况。 拆成合理大小的集群，这一点非常认同，关键就在合理二字。但多少算合理，还是得按照实际的业务规模和管理需求看。早些年阿里的对外分享动辄上万节点，也经常会有人提问，为什么要搞这么大的集群，普遍的回答就是这是根据内部业务规模评估得到的结论。所以多大合适，这个问题不光涉及到 k8s 管理人员，还和上面所承接的业务规模密切相关，这会涉及多方合作问题，不是平台自己就能搞定的，但无论如何，评估合理的可以接受的集群规模仍然是一件值得做的事情。 相比控制爆炸半径，一个更亟需解决的事情是管控面和业务的故障域隔离。把一些运维系统或平台与业务从一个集群中拆出去，即使业务集群挂了，控制面的平台至少还活着，还可以做一些操作，当然这个还是建立在那些运维平台本身功能正常的前提下的，如果他们本身实现就有问题，那就是另外的话题了。 有趣的现象 在大家讨论相关问题的时候，有一个有意思的现象。SRE 和运维似乎在吐槽开发，而开发似乎又在吐槽 SRE 和运维。 工程师都应该知道，一个工程项目的资源总是有限的，工程师的任务永远都是在给定时间和成本的多重限制下，满足既定的质量要求。如果一个目标需要“大量的投入”，那么决策者在衡量投入产出比之后，降低该目标，是非常正常的项目管理动作，没有任何值得抱怨的。 笔者非常认同这个观点，不管什么方案，谁出的方案，很多时候都是在一定限制，或者一定条件下做的决定，这些限制可能是暂时的，也可能是长期的，可能是和人有关，也可能和组件功能有关。我们经常站在当前的时间点，以现状去评估和衡量早前的方案，得出方案有问题的结论。这有一部原因是随着时间推移，之前做方案时的那些限制条件或者约束已经不存在或者微乎其微了，但我们并没有及时的去调整这个方案，这本身也是需要决策者保持关注的。但公司里面普遍的现象就是\"能跑就别动”，“人和程序有一个能跑就行”。久而久之就会积重难返，直到一个大故障的爆发，或者幸运的话可能会安享晚年，亦或者在不为人知的地方悄然逝去。 很多方案是折中的产物，似乎每个方案的制定者或者决策者都有不得不做妥协的地方，好一点的可能还会列一下排期，按优先级分期实现。但有意思的是连最初的方案制定都因为各种原因而折中处理，更别提上线后的迭代了，有几个能把历史的坑去填了的。之后就会进入大家熟悉的流程里面，就看哪个倒霉蛋接手了。 最后 在看了各位大佬们的分析后感觉认知又得到了提升。由于笔者也在发一些东西出来做分享，自己深知在互联网自媒体井喷式发展的时代，自媒体，尤其技术类的，保持自身的专业性，严谨性理因是享受自媒体红利这项权利时应尽的义务。同样作为吃瓜群众，面对众说纷纭的消息，提升我们辨别是非，去伪存真的能力，保持自己不迷失在乱花渐欲迷人眼的自媒体世界里面，权当做韭菜的自我修养吧。 为什么会发这篇，可能和作为技术人员爱钻牛角尖有关吧，也是借机表达自己的一些看法，如果能得到大佬们的指点，和更多小伙伴交流，就更好了。最近又有一个有关数据库是否应该容器化的话题火了，又开始了神仙打架，战况激烈。还有一篇 k8s 越来越复杂的文章，然后有的人开始拿别人的一些文章在那里断章取义，诱导读者。 以上内容基于笔者已有认知所阐述，可以看到几乎都在针对特别具体，细节的地方阐述观点，没有扩散，因为笔者清楚的知道自己在某些方面仍然是个菜鸡，不敢高谈阔论。针对以上内容如果有问题，欢迎讨论互喷（我假装听不见）。 ","date":"2023-12-06","objectID":"https://www.likakuli.com/posts/bomb/:2:0","tags":["思考","随笔"],"title":"让子弹飞一会儿","uri":"https://www.likakuli.com/posts/bomb/"},{"categories":[],"content":"SRE 运维进阶之路：https://clay-wangzhi.com/ ","date":"2023-12-02","objectID":"https://www.likakuli.com/friends/:0:0","tags":[],"title":"友链","uri":"https://www.likakuli.com/friends/"},{"categories":["源码分析"],"content":"背景 前面我们已经对 kube-apiserver 内存消耗 进行了阐述，文中最后提到了使用流式的请求来支持 List 的效果，从而实现对于单个请求来说，空间复杂度从 O(n) 转换成 O(1)，也讲述了其原理和流程。本篇从更细节的角度分析其在内存分配，序列化等方面做的进一步优化。 为了方便大家理解，前面多篇已经做了铺垫，建议按如下顺序阅读 kubernetes 月光宝盒 - 时间倒流 你真的搞懂 Informer 了吗？ kube-apiserver 又 OOM 了？ Kubernetes 陈年老 bug - Stale Read 从 ListWatch 到 WatchList 原理 内存优化是一个经典问题，在看具体 K8S 做了哪些工作之前，可以先抽象一些这个过程，思考一下如果是我们的话，会如何来优化。这个过程可以简单抽象为外部并发请求从服务端获取数据，如何在不影响吞吐的前提下降低服务端内存消耗？一般有几种方式： 缓存序列化的结果 优化序列化过程内存分配 数据压缩在这个场景可能不适用，压缩确实可以降低网络传输带宽，从而提升请求响应速度，但对服务端内存的优化没有太大的作用。kube-apiserver 已经支持基于 gzip 的数据压缩，只需要设置 Accept-Encoding 为 gzip 即可，详情可以参考官网介绍。 当然缓存序列化的结果适用于客户端请求较多的场景，尤其是服务端需要同时把数据发送给多个客户时，缓存序列化的结果收益会比较明显，因为只需要一次序列化的过程即可，只要完成一次序列化，后续给其他客户端直接发送数据时直接使用之前的结果即可，省去了不必要的 CPU 和内存的开销。当然缓存序列化的结果这个操作本身来说也是会占用一些内存的，如果客户端数量较少，那么这个操作可能收益不大甚至可能带来额外的内存消耗。kube-apiserver watch 请求就与这个场景非常吻合。 下文会就 kube-apiserver 中是如何就这两点进行的优化做一个介绍。 实现 下文列出的时间线中的各种问题和优化可能而且有很大可能只是众多问题和优化中的一部分。 ","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:0:0","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["源码分析"],"content":"缓存序列化结果 ","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:1:0","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["源码分析"],"content":"时间线 早在 2019 年的时候，社区有人反馈了一个问题：在一个包含 5000 个节点的集群中，创建一个大型的 Endpoints 对象（5000 个 Pod，大小接近 1MB），kube-apiserver 可能会在 5 秒内完全过载； 接着社区定位了这个问题，并提出了 KEP 1152 less object serializations，通过避免为不同的 watcher 重复多次序列化相同的对象，降低 kube-apiserver 的负载和内存分配次数，此功能在 v1.17 中发布，在 5000 节点的测试结果，内存分配优化 ~15%，CPU 优化 ~5%，但这个优化仅对 Http 协议生效，对 WebSocket 不生效； 3 年后，也就是 2023 年，通过 Refactor apiserver endpoint transformers to more natively use Encoders #119801 对序列化逻辑进行重构，统一使用 Encoder 接口进行序列化操作，早在 2019 年就已经创建对应的 issue 83898。本次重构同时还解决了 2 提到的针对 WebSocket 不生效的问题，于 1.29 中发布； 所以如果你不是在以 WebSocket 形式（默认使用 Http Transfer-Encoding: chunked）使用 watch，那么升级到 1.17 之后理论上就可以了。 ","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:1:1","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["源码分析"],"content":"原理 新增了 CacheableObject 接口，同时在所有 Encoder 中支持对 CacheableObject 的支持，如下 // Identifier represents an identifier. // Identitier of two different objects should be equal if and only if for every // input the output they produce is exactly the same. type Identifier string type Encoder interface { ... // Identifier returns an identifier of the encoder. // Identifiers of two different encoders should be equal if and only if for every input // object it will be encoded to the same representation by both of them. Identifier() Identifier } // CacheableObject allows an object to cache its different serializations // to avoid performing the same serialization multiple times. type CacheableObject interface { // CacheEncode writes an object to a stream. The \u003cencode\u003e function will // be used in case of cache miss. The \u003cencode\u003e function takes ownership // of the object. // If CacheableObject is a wrapper, then deep-copy of the wrapped object // should be passed to \u003cencode\u003e function. // CacheEncode assumes that for two different calls with the same \u003cid\u003e, // \u003cencode\u003e function will also be the same. CacheEncode(id Identifier, encode func(Object, io.Writer) error, w io.Writer) error // GetObject returns a deep-copy of an object to be encoded - the caller of // GetObject() is the owner of returned object. The reason for making a copy // is to avoid bugs, where caller modifies the object and forgets to copy it, // thus modifying the object for everyone. // The object returned by GetObject should be the same as the one that is supposed // to be passed to \u003cencode\u003e function in CacheEncode method. // If CacheableObject is a wrapper, the copy of wrapped object should be returned. GetObject() Object } func (e *Encoder) Encode(obj Object, stream io.Writer) error { if co, ok := obj.(CacheableObject); ok { return co.CacheEncode(s.Identifier(), s.doEncode, stream) } return s.doEncode(obj, stream) } func (e *Encoder) doEncode(obj Object, stream io.Writer) error { // Existing encoder logic. } // serializationResult captures a result of serialization. type serializationResult struct { // once should be used to ensure serialization is computed once. once sync.Once // raw is serialized object. raw []byte // err is error from serialization. err error } // metaRuntimeInterface implements runtime.Object and // metav1.Object interfaces. type metaRuntimeInterface interface { runtime.Object metav1.Object } // cachingObject is an object that is able to cache its serializations // so that each of those is computed exactly once. // // cachingObject implements the metav1.Object interface (accessors for // all metadata fields). However, setters for all fields except from // SelfLink (which is set lately in the path) are ignored. type cachingObject struct { lock sync.RWMutex // Object for which serializations are cached. object metaRuntimeInterface // serializations is a cache containing object`s serializations. // The value stored in atomic.Value is of type serializationsCache. // The atomic.Value type is used to allow fast-path. serializations atomic.Value } cachingObject 实现了 CacheableObject 接口，其 object 为关注的事件对象（例如 Pod），serializations 用来保存序列化之后的结果，Identifier 是一个标识，代表序列化的类型，因为存在 json、yaml、protobuf 三种序列化方式。 cachingObject 的生成在上图 Cacher dispatchEvent 消费自身 incoming chan 数据，将 event 发给所有相关的 cacheWatchers 的时候，会将事件对象转化为 cachingObject 发给 cacheWatcher 的 input chan。最终的 Encode 操作是在 serveWatch 方法中将最终的对象进行序列化时调用的，会先判断是否已经存在序列化的结果，存在则直接复用，避免重复的序列化。 注意： 上图 wrap into cachingObject if len(watchers) \u003e= 3 已成为过去式，新的代码逻辑中已经去掉了后面的判断，不管 watchers 数量，统一都进行 cachingObject 的封装； 并没有对 Init Event（watchcache 中的全量数据） 进行 cachingObject 的封装，只有发给 Cacher incoming chan 的数据会转化为 cachingObject。也就是说这个优化对 Get/List 请求完全无效，因为他们是直接从 watchcache 返回数据的，针对 Watch 请求，也将会有部分数据在返回时没有复用已有序列化结果，因为仍然可能会有部分 Init Event 数据是从 watchcache 获取并返回的，这是一个很神奇的地方，cacheWatcher 的 input chan 的 event 对象的 object 有可能是正常的资源对象，例如 Pod，也有可能是 CacheableObject 对象，而真正的资源对象则保存在 CacheableObject 的 object 中； 为什么不把 Init Event 也覆盖了，KEP 1152 中给的说法是先实现 Cache incoming c","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:1:2","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["源码分析"],"content":"优化内存分配 ","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:2:0","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["源码分析"],"content":"时间线 reduce the number of allocations in the WatchServer during objects serialisation #108186，主要针对 protobuf 进行优化，对于 json 和 yaml 序列化无效，2022 年随着 v1.24 发布，protobuf 一般是内部组件使用，而外部组件访问 k8s 时一般都是使用 json 或者 yaml 序列化； Do not copy bytes for cached serializations #118362，自定义 SpliceBuffer，避免对 cachingObject 的序列化结果进行深拷贝，2023 年随着 v1.28 发布； Refactor streaming watch encoder to enable caching #120300，这个修复是在已有的缓存资源对象的序列化结果的基础上，把 Event 的序列化结果也做缓存，因为最终返回给客户端的是 Event 而不是资源对象； ","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:2:1","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["源码分析"],"content":"原理 针对 2，巧妙地定义了 SpliceBuffer 通过浅拷贝的方式有效的优化了内存分配，避免 embeddedEncodeFn 对已经序列化后的结果 []byte 的深拷贝； // A spliceBuffer implements Splice and io.Writer interfaces. type spliceBuffer struct { raw []byte buf *bytes.Buffer } // Splice implements the Splice interface. func (sb *spliceBuffer) Splice(raw []byte) { sb.raw = raw } Benchmark 效果显著 go test -benchmem -run=^$ -bench ^BenchmarkWrite k8s.io/apimachinery/pkg/runtime -v -count 1 goos: linux goarch: amd64 pkg: k8s.io/apimachinery/pkg/runtime cpu: AMD EPYC 7B12 BenchmarkWriteSplice BenchmarkWriteSplice-48 151164015 7.929 ns/op 0 B/op 0 allocs/op BenchmarkWriteBuffer BenchmarkWriteBuffer-48 3476392 357.8 ns/op 1024 B/op 1 allocs/op PASS ok k8s.io/apimachinery/pkg/runtime 3.619s 针对 3，严格来说这个 pr 不是用来优化内存分配的，而是来解决 issue 110146 的提到的 json 序列化时 json.compact 导致的 CPU 使用率过高的问题，随着 v1.29 发布。问题产生的原因是虽然上面提到了通过 cachingObject 来缓存资源对象的序列化结果，但最终发回到客户端的是 Event 对象，还是需要做一次 Event 的序列化操作，而 json.compact 会在每次 Marshal 后被调用，这是 golang 自带的 json 序列化的实现，可以参考 golang json 源码。这个修复是在缓存资源对象的序列化结果的基础上，把 Event 的序列化结果也做缓存，用来规避 json.compact 带来的影响。 这个 PR 涉及到的改动较大，笔者目前对其实现仍然存在一些疑问，已经提了 issue 122153 咨询社区，等搞清楚后可以再专门安排一篇来讲讲这个实现，这块涉及到了 watch handler 的整个序列化逻辑，Encoder 的嵌套非常深，连 google 大神在 review 代码时都有如下感叹 笔者在看这块代码时被接口的来回跳转搞晕了，写了个 unit test 来一步步调试才搞清楚这些 Encoder，真的是层层嵌套，梳理如下，可以感受下这五层嵌套 watchEncoder —\u003e watchEmbeddedEncoder ​ —\u003e encoderWithAllocator ​ —\u003e codec ​ —\u003e json.Serializer 他们都实现了 Encoder 接口… 类似 cachingObject 序列化，对 Event 进行序列化同样需要额外的内存空间，但可以避免对每个 Event 进行多次序列化带来的内存消耗和 CPU 消耗，所以也起到了内存优化的作用。 效果 通过 WatchList 以及上述的种种优化，社区给出了优化效果 优化前 优化后 最后 Kube-apiserver 内存优化系系列包含前面的铺垫，到此也 6 篇了，如果把这其中涉及到的知识都搞懂了，对 kube-apiserver 的理解一定可以上一个台阶，后续也会持续关注这块的内容，不定时补充~ 序列化，听上去简单，调个方法的事情，但用好了也不容易，往往这种地方最能体现能力，寻常见功力，细微见真章，看看大牛写的代码，领会其中的设计和思想，总结转化吸收为我所用。 k8s 使用起来容易，用好了不容易，搞明白背后是怎么回事难。项目经过 10 来年的迭代，无论代码量还是复杂度上面都已经比较恐怖了，而且还在不断地迭代更新，但路虽远，行则将至，事虽难，做虽然不一定成吧，不做一定成不了。 Talk is cheap, Show me the code and PPT 最后，欢迎加笔者微信 YlikakuY，一起交流前沿技术，行业动态~ ","date":"2023-12-02","objectID":"https://www.likakuli.com/posts/serialisation/:2:2","tags":["kubernetes"],"title":"kube-apiserver 内存优化进阶","uri":"https://www.likakuli.com/posts/serialisation/"},{"categories":["随笔"],"content":"继阿里云之后，滴滴崩了上了热搜，故障原因了解了一些，会在文章最后谈到。近期国内多个公司发生了 P0 事故，当然也包括我司，只不过可能不出名，很多人不知道而已。本文聊一聊我对这些故障的理解。首先做个声明，下文都是根据个人所了解到的信息以及个人的认知所写，并没有消遣的意思，可能会有一些偏激，如果问题，欢迎互喷。 ","date":"2023-11-28","objectID":"https://www.likakuli.com/posts/stability/:0:0","tags":["思考","随笔"],"title":"从故障中我们学到了什么？","uri":"https://www.likakuli.com/posts/stability/"},{"categories":["随笔"],"content":"故障年年有，今年特别多 从年初到现在，小红书，京东，语雀，阿里云，再有昨天的滴滴，都是 P0 故障，小红书和京东都是拿相关人祭天，语雀和阿里云的处理不清楚。故障原的话，对阿里云、京东、滴滴的故障原因有一些了解。 京东的故障个人感觉被裁的研发同学算是背锅的，系统太脆弱了经不起任何的风吹草动，就看谁是那个倒霉蛋来触发这个隐藏彩蛋，像这种服务端不去限流、熔断导致的问题，个人看法就是一律服务端背锅，别管是谁调用导致的。 阿里云的比较出名，影响范围非常广，从故障处理时间线来看整体问题定位和恢复速度还是比较快的。至于故障的原因，从官网给的通告看，算是一个比较低级的问题，白名单生成内容出错，问题影响范围之大本身还是和故障的组件有关，相当于入口出了问题。 前段时间阿里云的故障，不乏大佬出来讲自己的认知，如前阿里的毕玄大佬，总结了一篇《稳定性，难的不是技术，而是》，分别从代码、设计、变更层面给出了做好稳定性的通用指导思想，最后也给出了核心观点： 做好稳定性，难的不是技术，而是…… 大家可以看下，是不是在很多故障复盘的改进措施里，都会重复看到上面的影子，说明其实指导思想、解决方案是不缺的，技术上真的没有那么的难，想当年淘宝稳定性很差的某一年，新任 CTO 上来后把稳定性列在了第一考核指标，三个月后淘宝的稳定性就提升了 N 个档次。 但难就难在没有银弹，只能靠大量的细节来落地做好稳定性这个系统工程的事情，这意味着的是大量的投入，能不能在稳定性这件事上保持持续的一定的投入，甚至当成做业务功能实现一样的必须的投入，这才是真正做好稳定性最难的，毕竟就算有指导思想、解决方案、能力，但没有相应的投入，那自然只能有一定的取舍，最终呢，总是要还的。 很多做过稳定性这事的人都知道，做这个事情最麻烦的是很难被认可，做的好，不出问题，不懂的人不知道你做了什么，出了问题的时候觉得你到底做了什么，所以会看到很多公司都是运动式地做稳定性，一阵一阵的。 要解决这个问题其实挺难的，我看到做得好一些的，基本是因为稳定性对业务而言是致命的，例如某些行业，稳定性出问题，业务基本就没法进行下去了；还有就是某些处于非常激烈竞争，但又有关键时间点的业务，例如外卖。 只有把稳定性当成业务的功能实现一样，有相应的人员配备和投入，例如做一个业务可能需要多少人，相应的稳定性这块也固定投入多少人，你说到底多少比例合理呢，其实也说不太清楚，但这种简单粗暴的方式其实是最有效的，当然，是不是要把稳定性上升到这样的高度，也需要根据业务的性质、业务所处的阶段来具体判断，以及有这样的投入的情况下，怎么去评判相应职责的团队也仍然是个很复杂的话题。 简单总结一下就是 人性：医之好治不病以为功，搞稳定性就是费力不讨好，不出问题，没人会知道你的存在和重要性； 投入：既要有系统的方案，又要有大量的细节，还有持续投入，无论是技术上、人员配备上都是如此； ","date":"2023-11-28","objectID":"https://www.likakuli.com/posts/stability/:1:0","tags":["思考","随笔"],"title":"从故障中我们学到了什么？","uri":"https://www.likakuli.com/posts/stability/"},{"categories":["随笔"],"content":"故障驱动了没？ 运动式的稳定性，可以理解为我们常说的故障驱动，虽然听起来很挫，真正能做好的也不容易，往往是故障也有了，没有驱动，或者驱动也有了，但没有结果或者没有该有的效果，付出远大于回报，不知道驱动了半天驱动了个啥，该故障还是故障，甚至是同样的故障。 这一点我认为和故障的复盘、负向激励机制有关。 通过复盘来驱动改进，设立改进项，并且跟踪改进项的状态，同时结合上面提到的持续投入，定期的 review。但现实有多少复盘是在糊弄鬼，甚至为了推卸责任，故障报告都会写的含糊其辞的，都在尽可能的大事化小，小事化了。这就从根本上导致改进项不全，甚至发现不了根本问题，也就为后续此类问题的再次发生埋下伏笔，变化的可能只是不同的诱因而已。 国内一般处理 P0 故障先拿相关人员祭天，甚至对应业务负责人也会受影响（当然处理结果除了公司的规章制度外，还和你是不是嫡系，有没有人保，是不是顶风作案等，不好意思，我总是喜欢说实话），但这至少在一定程度上会促进大家保持对稳定性的敬畏之心。当然**负向激励并不是目的，只是一种手段。**并不是所有的公司都有相关的手段，这在一定程度上不利于稳定性的提升，虽然看起来好像很人性一样，但公司不是靠爱发电，业务做好多发福利比出了问题不追责人性化多了。 ","date":"2023-11-28","objectID":"https://www.likakuli.com/posts/stability/:2:0","tags":["思考","随笔"],"title":"从故障中我们学到了什么？","uri":"https://www.likakuli.com/posts/stability/"},{"categories":["随笔"],"content":"如何止损？ 我们并不是追求百分之百的稳定性，这也不现实，稳定性的提升越到最后，哪怕提升一点，付出也会非常大，ROI 就会比较低，具体到多少合适，还是取决于具体的业务特性，业务对稳定性的容忍程度。在只能降低故障发生概率但无法避免的情况下，如何快速止损也是需要考虑的。 例如公有云上一般会有 SLA，最终会将故障的影响换算为钱，快速止损意味着减少公司的损失。这里也有两方面值得考虑： 如何缩短故障时长？ 如何降低资损？ 第一点比较好理解，就是快速恢复，恢复的越快影响就越小，这很容易理解。站在技术的角度看，第二点很容易被忽略，即不同的业务 SLA 不同，可能有的业务可以接受故障一段时间，或者服务不可用相同时长，不同服务约定的赔偿价格不同或者不同服务在正常时能带来的收益不同，那在进行故障恢复时，就需根据这些数据来排序，先后恢复这些服务。 缩短故障时长本质还是为了降低故障影响（资损），他更像是一种手段，而不是目的。但我们在处理故障时往往舍本逐末，或者没有这个意识，或者有意识，但确实也不知道要先恢复那些服务。 当然如何解决问题，还是需要专业的人出马，靠 ppt 解决故障不现实，至少现在还不现实。这个阶段就能看出来到底谁是老板，谁是干活的，以及人的能力（战斗力）如何。当然出问题，或者恢复时间长也并代表就是没有能力强的人，有没有能力和能力有没有被充分利用起来是两个概念，找到有能力的人来解决问题也是止损甚至避免故障的一个途径。 ","date":"2023-11-28","objectID":"https://www.likakuli.com/posts/stability/:3:0","tags":["思考","随笔"],"title":"从故障中我们学到了什么？","uri":"https://www.likakuli.com/posts/stability/"},{"categories":["随笔"],"content":"聊一聊 k8s 有几个 k8s 相关的 P0 故障，作为一个搞了 7 年容器，开发运维都搞的老司机，重点聊聊这几个。 我司的两个 P0 级别的故障（删集群、删库），以及滴滴的这次 P0 故障，都是发生在容器平台，平台异常影响也上面跑的业务。而且这种影响都是集群甚至机房级别的，一挂就是一片，恢复起来也慢。有几个问题值得思考一下： 我们到底在以什么样的方式使用容器？ 为什么故障范围如此之大，恢复如此慢？ 第一个问题，滴滴（前司）在 k8s 的使用方面有一些定制化的工作，为了适配存量的系统，兼顾用户物理机时代的使用习惯，加上最初使用 k8s 版本老一些，扩展能力有限，最终定制的功能会多一些，之前也有一些技术大牛的存在能掌控这些。相对来说我司目前对 k8s 的使用上定制化的开发就少很多了，大部分用的还是原生的组件。在人员组成上，不管哪个公司，真正搞懂 k8s 的并不多，绝大多数还是停留在上层的使用上，一方面是因为 k8s 的复杂性，另一方面也和具体公司的某些机制或者制度有关。最终的现象就是可能看起来搞容器的人很多，但都停留在表面，或者某一方面，都在围绕 k8s 做事情，但真正对 k8s 的掌握不足。 第二个问题的核心在于 k8s 的生命式 API 的设计，在 k8s 之前的物理机或者虚机时代，一般的系统都是命令式的，也就是说一般情况下控制面出现问题的话，影响的是增量的服务，存量的不受影响。但 k8s 并不是这样，反而控制面出问题的时候，存量的服务非常容易受到影响。同时 k8s 中存在着大量的自动操作，需要使用者格外关注，稍不注意就会户问题。比如节点 down 了之后的自动驱逐，节点有资源压力时的自动驱逐，k8s 升级时的各种 checksum 校验引发的资源泄露甚至容器重启等等，这都需要使用者或者管理员对 k8s 有一定的掌握，在设计平台或者进行操作时来规避这些问题。哪怕是是在无法避免，出问题时能快速定位快速止损。 k8s 使用起来容易，管理好了难。 作为 Operator 的开发者，如果只是想基于 k8s 之上实现功能，都有比较经典的模板或者工具直接使用，比如写个 crd 和 operator，通过 Informer 实现增删改查等等。但实现的效果却可能会非常大的差异，无论是从自身组件看还是从对 k8s 的影响看，例如之前见过一段代码，会 for 循环针对每个 namespace 建立一个 informer 并添加对应的 event handler 做一些逻辑处理。说他不能用吧，也能用，但是我们需要清楚他潜在的问题，即在集群内 ns 数量变多时，goroutine 数量会变多，在有几万个 ns 时，就会有几万个 goroutine，这肯定是会有问题的，同时几万个 informer 都会对 kube-apiserver 发起 List \u0026 Watch 请求，会不会触发客户端的限流，会不会触发服务端限流，会不会增加服务端的内存压力等等的问题，都是值得考虑的，但是在你不了解 informer 原理，不了解 kube-apiserver 内存消耗的时候是根本没有意识去关注这些问题的。 作为集群管理员，提供一个途径让业务能够高效且体验良好的使用 k8s 很重要，但也需要清楚的知道在上面跑的业务对 k8s 的影响，不只是接入业务就万事大吉了，平台运营需要考虑准入问题，即作为管理员，要清楚跑上来的业务是否会危害到集群的健康，比如上面的那个服务要是跑上来，集群可能就有挂的危险。接入是最简单的，管理才是核心，平台需要提供接入规范，例如用户程序都需要用到哪些集群资源，以什么样的方式访问，QPS 什么量级等，并且提供对应的工具帮助用户自动生成相关数据，组织为对应格式的文件，随着部署文件一并提交，平台侧需要对这些应用进行审核，审核之后的应用才允许被部署。 k8s 提供了通用能力，也就意味着使用者需要根据自己的实际情况来做管理，对管理员的要求就会提高。比如上面提到的一些自动的操作，在没有控制能力的情况下，是否可以先关闭，这是一个值得考虑的问题，众多的问题都是因此而起。但这对于没有经验的人来说，似乎又是必定会踩的坑，这也能侧面反映出来除了写 API 之外，需要有经验有能力的人。 总结的话就是 k8s 虽然已经开源这么多年，很多公司都在用了，但真的是能搞懂的人并不多，再加上设计机制，以及复杂性，在对他缺乏把控力的情况下出问题，影响范围广，回复时间长，似乎也就可以理解了。 ","date":"2023-11-28","objectID":"https://www.likakuli.com/posts/stability/:4:0","tags":["思考","随笔"],"title":"从故障中我们学到了什么？","uri":"https://www.likakuli.com/posts/stability/"},{"categories":["随笔"],"content":"最后 这个行业最不缺的就是事后诸葛亮，和写 PPT 的人，真正缺的还是那些挽狂澜于既倒，扶大厦之将倾的人。当然并不是说写 PPT 不好，每个公司或者职位有自己的生存之道，无可厚非，向上管理，汇报也是必要的职场能力，但这本不该是技术人员的重点或者全部。 愿这个世界对程序员友好一些，尤其这些刀口舔血的人。愿每个人都能被温柔以待，做自己喜欢做的事情，做有价值的事情，做有趣的事情，摆脱无效的内卷和精神内耗。卷不过，那就躺平，躺不平，那就一起写 blog 吧，哈哈… （上文中提到了滴滴故障的原因，有一些东西不方便写，对 K8S 或者各种故障感兴趣的朋友可以加我 WX：YlikakuY，一起交流，一起进步） ","date":"2023-11-28","objectID":"https://www.likakuli.com/posts/stability/:5:0","tags":["思考","随笔"],"title":"从故障中我们学到了什么？","uri":"https://www.likakuli.com/posts/stability/"},{"categories":["系统设计"],"content":" 转自知乎 https://zhuanlan.zhihu.com/p/408731614，略有修改 背景与挑战 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:0:0","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"背景 大型互联网公司内部资源池非常庞大。Google，Facebook 等公司都有统一的容器平台管理庞大的共享资源池，如 Borg，Twine 等等。大规模共享资源池可以提供许多好处，比如： 应对单个任务突发资源上升需求； 削峰填谷，混合部署，提高资源利用率； 多地域提高可用性等能力； ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:1:0","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"挑战 平台不是全能的，许多理想需求和成本有矛盾性，不能无条件都满足。 平台为了提高利用率，减低成本，通常会采取某种超卖（overcomitment）策略：让集群用的比较满，不能无限满足突发的资源需求； 用户总是希望只为使用付费：在需要资源时快速获取，不需要时快速释放，这会导致平台难以预估平台需要准备的资源。过多预留会导致资源浪费，预留不足会导致资源补充（分钟级）耽误业务的紧急扩容需求； 用户的任务是多种多样的：有些需要高可用性、资源的高保障，有些可以低频率的被抢占，有些可以排队，但是不能被抢占。 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:2:0","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"技术层面 目前隔离技术并不完美：在网络层，存储层等还有不少挑战； 调度能力有上限：扩缩容的速度和资源需求的规模有一定关系，可能是毫秒级，也可能是分钟级，甚至需要人工干预； 平台很难通过简单的 SLO 和业务达成互相理解的协议；https://storage.googleapis.com/pub-tools-public-publication-data/pdf/f647d24ee7eeb338acebf1eb73a5d11b357620b0.pdf ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:2:1","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"管理层面 共享资源池的用户较多，平台侧消耗在沟通的沉默成本巨大； 总体预算由少数中高层负责，如何高效合理的分配给大量具体项目的研发人员； 方案 解决上面的挑战需要从两个方面着手 机制设计 平台技术研发 平台技术研发方案有大量的资料讨论，然而顶层机制的讨论却非常少。常常导致负责技术的研发人员常常处于两难的\"脑裂\"的状态。本文主要讨论资源的预算和分配这些机制的设计思路。 综合考虑上述原因，除了搭建搞水平的技术，还有另外一个必要前提条件：预算、分配、核算机制。通过达成共识的机制，让业务（Consumer Based Expectiation）和平台（Service Level Expectation）通过技术手段做到成本和效率的平衡。 具体的，这套机制需要解决如下关键问题，其中着重关注问题 2 ~ 5： 预算分配给业务 共享池上的众多业务如何申请每个周期的预算总额； 业务预算分配给项目 获得预算的业务如何有效的将预算分配给具体需要使用资源的项目，比如视频业务需要转码项目，视频推荐排序项目，存储后台项目等。参与到项目中的研发需要使用共享资源池的算力来完成项目； 项目任务分级配额 研发为某个项目开发代码，提交计算任务到容器平台上会有不同的分级，对应不同的风险和定价策略。有些任务需要 “更可靠” 的资源预留，对应会有较高的定价。而部分批任务，离线任务可以运行在一定时间段内承担部分资源无法获得的情况，对应有较低的价格。具体表现为基于分级的差异化 SLO 和定价； 项目根据自己任务的实际情况，使用不同等级任务的配额； 精算系统 通常公司内精算系统在满足业务实际需求前提下，以最大化资源利用率为优化目标。而公有云则更倾向于最大化收入为目标； 基于历史、当前和预测的资源用量，平台技术能力等数据； 预测共享资源池还可以售卖的各级资源额度，以及对应的 SLO 和价格； 核算 资源用量可以归属到组，通过组可以回溯到上游平台，业务，实体人等维度； ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:2:2","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"预算 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:3:0","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"预算分配给业务 预算分配给业务往往是高层的决策。通过年度预算等和定期调整的方式进行。一般会参考： 往年的成本； 对于后续业务成长速度，规划，技术发展的预期； 预算中的一部分会被分配给研发项目，用来确定在容器平台中能够使用的资源。 预算的原始单位可能是具体的金额： 金额具体如何转换成容器平台的物理资源通常是不公开的。可能会被加密成其他单位，比如 “人时”； 换算的公式也可能是动态调整的，受众多因素影响：物理资源的成本变化，资源在不同区域的空闲情况，容器平台底层技术能力，用量是否突发超过预算等等； 比如一个亿的预算中，具体可能会表现成 10000 算力单元在容器平台能购买某个等级一定量的 CPU 核。 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:3:1","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"业务预算分配给项目 每个业务的发展都需要开展研发项目。比如接入层，后台的存储服务，训练召回排序模型，推理服务等等。这些项目最终被拆分到研发身上。他们为了完成工作需要在共享资源池上提交任务。 因此，需要将业务的预算拆解到各个项目上。这里主要通过组（group）的媒介来完成。一个或多个业务线将预算的一部分划归给某个具体的组，研发加入组后就可以用组的身份去容器平台提交任务了。具体如下： 创建组，将业务预算（可能转换成对应容器、存储等的资源单位）绑定到某个组上； 研发加入组，通过组的身份提交任务到容器平台； 容器平台得知组需要使用多少资源，去预核算确认这个组拥有充足的预算； 如果预算充足，容器平台调度任务； “组” 的媒介提了许多好处，也有许多细节 “组” 解耦了公司的管理树，可以方便让多个业务协同共享资源，多个部门研发协同开发； “组” 又可以进一步分类为 “资源组”，“人员组”。资源组仅有预算和身份能力，可以包含人员组，人员组可以包含人。解耦了资源和人的关系，人可以灵活加入和离去，但是项目可以持续的进行，使用资源； “组” 也能解耦平台，多个平台基于统一的组体系，可以大幅简化跨平台的使用，结算等问题； ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:3:2","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"项目任务分级配额 前面的流程中存在一个比较大的问题，研发对于每个任务需要使用多少资源的判断是不准确的，而且通常是非常不准确。运行程序的研发和开发程序的研发往往不是一群人。即使研发运行的是自己的代码，也难以判断准确。具体表现如下： 程序运行本身有不确定性，CPU 和内存的使用难以准确预估； 在线服务存在潮汐现象，研发往往按照峰值资源用量来申请资源； 为了保障在线服务的稳定性和不可预期的调用量增加，研发倾向于申请更多的资源； 综上，假如容器平台为所有计算任务预留的资源和研发申请的资源一样，必然导致巨大的资源浪费。经验性： 在峰值，申请的资源也比实际使用多出 30% 以上； 在波谷，申请的资源可能比实际使用多出的 70% 以内； 如下图所示： Usage 是实际程序运行使用的 CPU，会不断波动。 Allocation 是用户提交程序时申请的 CPU。通常超过 Usage，保障程序的运行速度等。 Reservation 是平台为用户预留的总 CPU。可能因为用户还没来得及提交，或者有任务停掉等原因，比 Allocation 要多。 Capactity 是整个集群实际物理 CPU 总量。通常会比 Reservation 多，可以被卖给其他用户。 超卖 由于所有任务不太可能同时到达峰值（大多数情况都不再峰值），平台如果按峰值的总和预留资源会造成巨大的浪费。为了让资源池的利用率高起来，容器平台一般会对资源进行超卖。 举个例子，平台有 100 万核，卖给 A 业务 50 万核，A 大概率不会用到 50 万核，那么平台可以再卖 80 万核给 B 和 C。因为 B 和 C 大概率也不会用到 80 万核，所以平台的 100 万核（真实资源）大概率不会被 A，B 和 C 用满。 在大多数情况下，A，B，C 和平台都很 happy。A，B，C 感觉自己便宜的买到了更多东西。平台 “无中生有”，降低了公司的运营成本。 这有点像银行发放贷款，银行获得了100 万的储蓄，可以借 90 万给别人。只要存钱的人不一起来要这 100 万（大概率），银行就是安全的。 问题 前面反复提到了概率的问题。当某些特殊情况发生时，平台，A，B，C 可能就不太 happy了。比如，A 业务出了个爆款，用户量爆增，真的用到了 50 万核（甚至更多）。但是 B 和 C 已经用了 60 万核了（卖给他们 80 万）。A 现在只能用到 40 万核。。。A 会非常不 happy！ 真实情况，很多公司平台如果有 100 万核，可能根本就不知道 A，B，C 分别预算多少。当任意一个业务量暴涨时，都会问平台要更多资源。而平台交不出来的话，就需要承担业务的损失。 一种比较简单的办法：平台囤积部分资源不用（小金库），假装资源已经都分配了。当 A，B，C 某个业务需要使用时，再分配这部分资源。这样会导致 A，B，C 都需要为这部分囤积浪费的资源买单，提高了单位资源成本。 分级配额机制 更合理的做法应该是分级，对不同级别基于对应的 SLO 和定价。类似 AWS 上的 Reserved Instance，OnDemand Instance 等。 SLO 一般会考虑两个维度： Obtainability：用户想获得配额的资源时，有多大概率是能给的； Availability：用户的任务正在运行时，有多大的概率资源要被回收； 基于以上两个维度，有 3 种场景的 SLO Reserved（Or Prod）：就是 Obtainability 和 Availability 是接近100%的，有较高定价； Economy（Or Batch）：研究表明，Obtainability 和 Availability 会在 98% 以上时，平台可以超卖相当可观的资源。而 98% 对于大多数离线任务，甚至部分在线服务都是可以接受的； Opportunistic（Or Free）：没有 SLO，但是在集群的缝隙通常也能获得一定的资源； Pooling 机制 这是一种附加策略，单个组的预算是非常有限的，Pooling 是一种互助协议。如果 A 和 B 签订了协议，那么当 A 超过了配额时，A 可以临时借用 B 没有用的配额。 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:3:3","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"精算系统 前面提到的分级配额机制，通常需要一个精算系统的支持，对于大规模容器平台，可能有数百万到数千万核的算力资源。1% 的优化都能带来显著金钱上的节省。 精算系统可以和调度系统解耦，判断每台物理机在一定概率条件下，实际剩余的空闲资源。或者判断每个容器在一定概率条件下，实际使用的资源。(Usage vs Limit vs Capacity） 大致工作流程： 获得如下数据 资源池总体的容量情况 历史，现在和对未来任务资源使用的预测 平台的调度、隔离能力等技术指标 数据科学家基于数据制定大致的分级策略，比如上文提到的 Prod, Batch, Free； 精算系统计算出，在保障 SLO 的前提下，资源池的不同等级的资源还有多少可以被售卖； 假设平台有 100W 核，A，B，C 三个组过来申请配额： A，B，C 为在线服务 A1，B1，C1 分别申请了 20W Prod，那么集群剩下 40W； 精算系统通过测算，认为 99% 的概率下，集群能在 10 分钟内能够释放 60W 核的资源出来。或者说，这 60W 中，A1, B1, C1 同时需要使用 20W 中的一部分，导致只剩下 40W~60W 的概率小于 1%； 平台会将 60W 核分配给 A，B，C 的 Batch。假设 A2，B2，C2 分别各申请获得 20W Batch； Free 配额可以一定概率在低谷期获得 20W 的碎片资源，白天某些时段获得 5W 碎片资源，依然有利可图； ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:4:0","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"Priority 和 Quota 优先级和配额用于防止运行的比实际能容纳多的这种负载情况。每个任务都有一个 priority 优先级，一个小的正整数。高优先级的 task 可以在牺牲较低优先级的 task 来获取资源，甚至是以抢占方式。 可以为不同用途定义不同的优先级：监控、生产、批处理和 best effort。 针对生产级别的任务是禁止 task 互相抢占的。优先级决定任务处于运行还是等待状态。 Quota 配额被用于确定调度哪些任务。配额表示为一段时间内（通常为几个月）给定优先级的资源量（CPU、RAM、磁盘等）。这些值指定了用户的任务在请求时间段内可以使用的最大资源量。配额检查是准入控制的一部分，配额不足情况下，任务会被拒绝调度。 高优先级的配额成本比低优先级要高。生产级别的配额仅限于集群中实际可用资源，因此用户提交满足生产级别任务运行预期的资源配额。虽然不建议用户配置超买，但是很多用户都会比实际的需要配额要大，以防止后续用户增长可能造成的资源短缺。对于超买，应对方案就是超卖。 配额分配和物理容量设计密切相关，结果反映在不同数据中心的配额价格和可用性上。 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:4:1","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"Fine-grained resource requests 支持以 ms（毫秒）为单位请求 CPU，以 bytes（字节）为单位请求内存和磁盘空间。 ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:4:2","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["系统设计"],"content":"核算 基于上文机制开发的系统，核算应该是可以完全自动化的。整个链条从最初的总预算开始到最后任务在 CPU 上运行都是在系统中可追溯的。大致过程如下 容器平台会结算每个任务运行消耗的资源； 每个运行任务可以自动关联到 “组” 上； 组的创建是来自某个项目的申请，因此可以管理到具体的事上，比如训练精排模型； 组的预算来自于一个或多个业务授予的，也可以自动化拆分； 组的成员组成也是明确的（包括它的变化记录），可以自动归属到对应的研发身上； 角色与责任 在整个链条中有如下几类角色 决策层：负责定期或按需 Review 业务的总体预算是否满足业务发展的总体开销，并且将获得的预算分配各个领域的负责人； 领域负责人：通常可能是业务某个领域负责人等角色，负责将预算分配给各个项目组，并对分配的额度是否够用负责。如果额度不够导致了容器平台拒绝提供资源，由项目 Admin 和下级负责； 项目组负责人：需要主动观测项目组下所有任务的资源使用情况，在需要时及时向领域负责人申请调整。如果没有及时调整，额度不够导致平台拒绝提供资源，由项目负责人负责； 研发：开发 Prod, Batch 等任务，优化任务的资源开销，尽可能在满足业务需求的前提下使用较低价格的资源等级； 平台：对平台的健壮性，可观测性和承诺的 SLO 负责。在满足 SLO 前提下可以拒绝提供资源； 相关资料 Long-term SLOs for reclaimed cloud computing resources Take it to the Limit: Peak Prediction-driven Resource Overcommitment in Datacenters Flex，JSSP 2018 Nines are Not Enough: Meaningful Metrics for Clouds Borg 2013 Next Borg ","date":"2023-11-15","objectID":"https://www.likakuli.com/posts/resourcemgmt/:5:0","tags":["资源管理"],"title":"大规模容器平台共享资源池的预算，分配，核算机制","uri":"https://www.likakuli.com/posts/resourcemgmt/"},{"categories":["源码分析"],"content":"背景 前面我们已经对 kube-apiserver 内存消耗进行了阐述，文中最后提到了使用流式的请求来支持 List 的效果，从而实现对于单个请求来说，空间复杂度从 O(n) 转换成 O(1)，这篇主要就分析描述其实现原理。 为了方便大家理解，前面多篇已经做了铺垫，建议按如下顺序阅读 kubernetes 月光宝盒 - 时间倒流 你真的搞懂 Informer 了吗？ kube-apiserver 又 OOM 了？ Kubernetes 陈年老 bug - Stale Read 分析 可以先设想一下如果自己去实现的话，该如何设计。Client 和 Server 端都要去适配这是必然的，因为 Informer 现在是 ListWatch 机制，服务端并不支持流式 List。因此可以有个初步的方向： Server 端支持流式 List 请求 Informer 适配 Server 端 API 的变化 客户端的适配相对简单，重点还是放在 Server 端如何实现。先回顾下之前 List 的逻辑，在前一篇 Stale Read 里面已经介绍过了。 为方面描述，下文统一使用 RV 代指 Resourceversion，本节逻辑均基于 v1.26.9 版本，且忽略分页查询，因为分页是直接走 Etcd 的。 无论是 List 还是 Watch 请求，其 query 均支持传入 RV，服务端会根据请求的 RV 的不同做相应的处理，根据 RV 的值可以分为三种情况 未设置或者显示设置 RV=\"\" RV = “0” RV = “非 0 值” 对于前两种情况，List 会直接返回 WatchCache Store 中的内容，即服务端缓存好的 Etcd 的全部相关数据。 对于第三种情况，会等待服务端缓存数据的最大版本要超过传入的 RV 之后再返回缓存内的数据，如果等待了一段时间（3s）后缓存中的数据仍然没有达到指定版本，则会报错返回 “Too large resource version”，并告诉客户端可以在 1s 之后重试。 新版中已经修复了 List Stale Read 的问题，对于前两种情况，其会先从 kube-apiserver 获取 Etcd 最新的 RV，等待 WatchCache Store 内容追平 RV 后再一次性的返回。 也就是说服务端是可以知道自己是否已经包含最新全量数据的，在这个基础上再以流式方式返回即可。当前已有的流式 API 就是 Watch，所以可以在此基础上支持 List 的效果。为什么不直接在 List 请求基础上改呢，因为改 List 的话，会涉及到太多的客户端侧的适配，List 会经常单独使用，而 Watch 基本是在 Informer 里面使用。 所以最终的工作就会变成如何使用 Watch API 实现 List 的效果，但数据仍然以流式返回给客户端，同时 Informer 修改 ListWatch 方式为只使用 Watch API 实现之前的效果。下文以详细介绍服务端实现为主，客户端适配的部分会比较简单的介绍下。 原理 通过为 Watch API 添加一个 SendInitialEvents=true 参数来支持 List 的效果。Server 端接收到 Watch 请求后判断哪些数据是应该作为 InitEvents 发送给客户端，同时在发送完这些数据之后发送一个特定的 BOOKMARK Event（带特定 Annotation 的 BOOKMARK，其 RV 对应下文的 bookmarkAfterRV）给客户端作为服务端通知客户端 InitEvents 发送完毕的标志，客户端在接收到指定 BOOKMARK Event 后，将之前接收到的所有 InitEvent 作为 List 的结果处理。 ","date":"2023-11-07","objectID":"https://www.likakuli.com/posts/watchlist/:0:0","tags":["kubernetes"],"title":"从 ListWatch 到 WatchList","uri":"https://www.likakuli.com/posts/watchlist/"},{"categories":["源码分析"],"content":"时序图 下面是基于 v1.29 代码的分析，此时 v1.29 还在 alpha 状态，提到的旧版代表 1.27 之前的版本，新版代表 v1.29。如果你看到的代码和下面描述的不一致，有可能是代码版本导致的。 从 WatchCache 开始右面四个蓝色的是在 kube-apiserver 启动的时候开始执行的，G1 G2 代表两个 goroutine，分别用来从 Etcd 获取数据，以及发送数据给客户端 CacheWatcher 的 input chan G1.1 每种资源类型对应一个 Cacher，内部包含一个 Reflector，WatchCache 作为 Reflector 的 Store 存储从 Etcd 获取到的数据； G1.2 Reflector 开启调用 Etcd List 和 Watch API 获取数据； G1.3 Reflector 利用获取到的数据更新 WatchCache 的 store 和 cyclic buffer，两者分别用来存储全量的对象和对象的最近更新事件； G1.4 在更新完 WatchCache 后，会把 Event 发送到 Cacher 的 incoming chan 中； G2.1 从 Cacher 的 imcomming chan 中消费数据发送给所有的 CacheWatcher 的 input chan，或者定时（1 ~ 1.25s）发送 RV \u003e bookmarkAfterRV 的 BOOKMARK 事件给所有的 CacheWatcher 的 input chan； 上述过程描述了服务端启动时的数据处理流程，接下来看有客户端请求时的处理流程 Reflector 首次发起 Watch 请求，query 中指定 RV=\"\"\u0026sendInitialEvent=true\u0026resourceVersionMatch=NotOlderThan\u0026AllowWatchBookmarks=true，这里无论 RV=\"\" 还是 RV=“0” 都可以实现 List 的效果，只不过相比旧版本的实现，新版里面 Watch 请求针对 RV=\"\" 做了特殊处理，解决了 Watch API Stale Read 的问题（List Stale Read 已经在前一篇中介绍过了，针对 List 提供了 FeatureGate 来控制是否开启 Consistent Read，但 Watch 这里并没有对应的 FeatureGate，也即是说新版中针对 RV=\"\" 的请求一定是 Consistent Read），服务端接收到请求后为这个请求创建对应的 CacheWachter 对象； Server 端在接收到请求后计算 bookmarkAfterRV 的值，如果 RV=“0”，则 bookmarkAfterRV 就是 WatchCache RV（WatchCache Store 数据中的最大 RV），如果 RV=\"\"，则去 Etcd 中获取最大的 RV 作为 bookmarkAfterRV，将 bookmarkAfterRV 传递给 CacheWatcher，最后 CacheWatcher 会结合 WatchCache Store 和自身 input chan 中的数据准备 InitEvents 2a 开始从 WatchCache Store 中获取需要返回的数据，此时的处理逻辑旧版本相同，返回 Store 中的全部数据，并记录 Store 数据的最大 RV 供下一步使用； 2b 消费 input chan 中的事件，对比其 RV 是否比 2a 传入的 RV 大，或者如果是 BOOKMARK 类型并且 RV 等与 2a 传入的 RV，且尚未发送 bookmarkAfterRV 的事件，则此 BOOKMARK 事件就会被当做 List 结束的标志，为其设置 Annotation: k8s.io/initial-events-end，最后发送给客户端； 至此，服务端的主要流程已经介绍完，客户端 Informer 也做了对应的适配，如果开启 WathList 功能的话，会发送 Watch 请求来获取一遍全量数据，等到接收到携带 Annotation: k8s.io/initial-events-end 的 BOOKMARK 事件后，记录其 RV，将在此期间接受并处理后的对象作为 List 的结果。最后再次以上述 RV 作为参数调用 Watch 请求，从这一步开始就是 Informer 传统意义上的 Watch 逻辑了。 ","date":"2023-11-07","objectID":"https://www.likakuli.com/posts/watchlist/:1:0","tags":["kubernetes"],"title":"从 ListWatch 到 WatchList","uri":"https://www.likakuli.com/posts/watchlist/"},{"categories":["源码分析"],"content":"数据流 图片来自 KEP 3157 watch-list，其实里面也包含时序图，不过里面的书序图画的有一些问题，和代码不一致，所以这里并没有直接使用他的时序图，而是重新画了。 可以结合上面两个图理解整个过程，上图中的 a 对应时序图中的 2a，b 对应时序图中的 2b，c 对应时序图中的 G2.1。最下面白色部分对应时序图中 G1 的逻辑，即从 Etcd 获取数据，客户端请求的处理是自上到下的，而数据返回是自下而上的。 注意 上述处理逻辑中存在很多的细节，需要额外注意下 为 Watch API 修复了 Stale Read 的问题（RV=\"\" WatchList 功能），本质上也是消除 List 的 Stale Read，只不过是在 Watch API 中实现的，这样结合上一篇，不管是直接使用 List API 还是使用 WatchList 都能避免 Stale Read 的问题； WatchCache Store 中的数据和 Cacher imcomming chan 数据是有交叉的，所以在 2a 处理完所有 Store 数据后记录了最大的 RV 传递给 2b 在处理 imcomming chan 的数据时使用，event RV \u003e RV 的非 BOOKMARK 事件才会发回客户端，这样是为了避免时间回流； CacheWatcher 的 input chan 中是不存在 RV \u003c bookmarkAfterRV 的事件的，在 G2.1 从 Cacher incoming chan 消费并发往所有 CacheWatcher input 的时候判断了如果事件类型是 BOOKMARK 且 RV \u003c bookmarkAfterRV，则直接丢弃此事件，因为 input chan 缓冲区大小有限，在其创建后 Cacher 就开始往其 input 写数据，而开始消费 input chan 是在 2a 处理完所有 Store 中的数据之后，中间存在一段时间差，事件的长短和 Store 中的数据量有关系，丢弃不必要的 BOOKMARK 事件就可以缓解 input chan 的压力，这里涉及到了为 input chan 添加事件的处理逻辑，里面包括多种特殊情况的处理，例如缓冲满了如何处理避免因为单个 CacheWatcher 而阻塞整个流程，发数据异常如何处理； 最终发回给客户端的携带特定 Annotation 的 BOOKMARK 事件的 RV \u003e= bookmarkAfterRV，这里非常值得注意，并不是等于 bookmarkAfterRV，原 KEP 时序图中此处（2c）的描述是错误的。根本原因在于 bookmark timer 的周期为 1 ~ 1.25s，也就是说每 1 ~ 1.25s 产生一个 BOOKMARK 事件，其 RV 是 incoming chan 最大 RV，正是由于这个时间间隔，结合 3 的描述，就会导致 G2.1 发送出去的第一个有效的 (进入到 CacheWatcher input chan) BOOKMARK 事件的 RV \u003e= bookmarkAfterRV。这也从侧面说明了最终在返回 bookmarkAfterRV BOOKMARK 事件之前返回的所有的携带有效负载的事件集合的最大 RV 也是 \u003e= bookmarkAfterRV 的，即虽然标记是 bookmarkAfterRV，但 List 的结果中包含比 bookmarkAfterRV 大的数据。 个人认为此处还是可以再继续优化的，可以让 List 的耗时减少一个 bookmark timer 的周期，即 1 ~ 1.25s，只需要在 2b 处理非 BOOKMARK 事件时判断 RV == bookmarkAfterRV 且尚未发送过 bookmarkAfterRV BOOKMARK 事件，此时就可以直接返回一个 bookmarkAfterRV BOOKMARK 给客户端了，对于数据量较大，返回所有数据耗时超过 Watch timeout 时间 1s 左右时可以降低超时的概率，避免重复执行 WatchList 的过程，也能在一定程度上降低内存消耗。 总结 本篇主要分析了 WatchList 的实现原理和逻辑，其中不乏一些细节处理，后续也会和社区就有关细节进一步讨论。在此 KEP 中同时还介绍了另外两个用来降低 kube-apiserver 内存压力的修改，篇幅有限，将会在下一篇中进行介绍，同时也会给出所有优化工作做完前后的效果对比。敬请期待~ 如果对上述内容不理解或者有疑问，欢迎讨论。 ","date":"2023-11-07","objectID":"https://www.likakuli.com/posts/watchlist/:2:0","tags":["kubernetes"],"title":"从 ListWatch 到 WatchList","uri":"https://www.likakuli.com/posts/watchlist/"},{"categories":["源码分析"],"content":"背景 前两篇已经介绍过 Informer 和 Cacher 的实现，也介绍了其中存在的一些问题，本篇主要针对 Stale read 问题展开，分析新版 Informer \u0026 Kube-apiserver 中是如何解决这个问题的。 如果对 Informer 和 Kube-apiserver WatchCache 还不熟悉的话，建议可以先看前两篇，或者其他有关内容讲解的文章。 细节 为方面描述，下文统一使用 RV 代指 Resourceversion，本节逻辑均基于 v1.26.9 版本，且忽略分页查询，因为分页是直接走 Etcd 的。 无论是 List 还是 Watch 请求，其 query 均支持传入 RV，服务端会根据请求的 RV 的不同做相应的处理，根据 RV 的值可以分为三种情况 未设置或者显示设置 RV=\"\" RV = “0” RV = “非 0 值” 对于前两种情况，List 会直接返回 WatchCache Store 中的内容，即服务端缓存好的 Etcd 的全部相关数据。 对于第三种情况，会等待服务端缓存数据的最大版本要超过传入的 RV 之后再返回缓存内的数据，如果等待了一段时间（3s）后缓存中的数据仍然没有达到指定版本，则会报错返回 “Too large resource version”，并告诉客户端可以在 1s 之后重试。 修复 ","date":"2023-11-04","objectID":"https://www.likakuli.com/posts/staleread/:0:0","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - Stale Read","uri":"https://www.likakuli.com/posts/staleread/"},{"categories":["源码分析"],"content":"问题分析 回顾上一篇中提到的 Stale Read 的问题 T1: StatefulSet controller creates pod-0 (uid 1) which is scheduled to node-1 T2: pod-0 is deleted as part of a rolling upgrade node-1 sees that pod-0 is deleted and cleans it up, then deletes the pod in the api The StatefulSet controller creates a second pod pod-0 (uid 2) which is assigned to node-2 node-2 sees that pod-0 has been scheduled to it and starts pod-0 The kubelet on node-1 crashes and restarts, then performs an initial list of pods scheduled to it against an API server in an HA setup (more than one API server) that is partitioned from the master (watch cache is arbitrarily delayed). The watch cache returns a list of pods from before T2 node-1 fills its local cache with a list of pods from before T2 node-1 starts pod-0 (uid 1) and node-2 is already running pod-0 (uid 2). 详情可以参考 issue 59848。 问题发生在第 6 步，当客户端重启，Informer List RV=“0” 时如果连接的 kube-apiserver 不是断开之前的实例，就有可能会触发这个问题。因为每个 kube-apiserver 实例的 watchCache Store 存储的数据在某个瞬间可能是不一致的（网络，机器负载等原因），而从上面的细节分析来看 List 请求在遇到 RV=“0” 的时候是直接返回了 watchCache Store 内容，如果其内容是落后于断连之前的 kube-apiserver 实例的缓存的，就会导致 Stale Read 的问题，虽然最终数据会一致，但还是会出现暂时的时间回流，这对于有状态服务来说影响会比较大。 ","date":"2023-11-04","objectID":"https://www.likakuli.com/posts/staleread/:1:0","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - Stale Read","uri":"https://www.likakuli.com/posts/staleread/"},{"categories":["源码分析"],"content":"原理解析 此问题已经在当前 master 版本中修复，通过 ConsistentListFromCache FeatureGate 控制，开关打开后可以解决 Stale Read 的问题，默认关闭（v1.29）。 核心代码如下 // GetList implements storage.Interface func (c *Cacher) GetList(ctx context.Context, key string, opts storage.ListOptions, listObj runtime.Object) error { ... if listRV == 0 \u0026\u0026 utilfeature.DefaultFeatureGate.Enabled(features.ConsistentListFromCache) { listRV, err = storage.GetCurrentResourceVersionFromStorage(ctx, c.storage, c.newListFunc, c.resourcePrefix, c.objectType.String()) if err != nil { return err } } ... objs, readResourceVersion, indexUsed, err := c.listItems(ctx, listRV, key, pred, recursive) ... } 判断如果客户端传递的 RV 为前两种情况，则直接去 Etcd 中获取当前最新的 RV 作为 listRV 的值传递给最终 listItems 函数。也就是说最新版本 Informer 启动的时候虽然 List 传递了 RV=“0”，但在 kube-apiserver 处理时会访问一遍 Etcd 只获取最新版本号，相当于无论客户端传递的 RV 值如何，在服务端去 watchCache Store 获取数据时，始终是携带了非 0 的 RV。 结合上面对 List 的分析，需要等后缓存中的数据达到指定版本（从 Etcd 获取到的最新 RV）后才返回，这样一来就可以保证在 List RV=“0” 正常返回数据的情况下，如论连接到那个 kube-apiserver 实例，其获取到的都是最新版本的数据，从而避免 Stale Read 的产生。其代价就是多了一次 Etcd Read 请求，虽然并不需要 Etcd 返回真实数据。 这里有个细节要注意下，上面的处理只判断了 listRV == 0 的情况，如果客户端传递过来的 RV 为第三种情况，服务端就不会再去访问 Etcd 了，这时候可能访问不同实例返回的结果仍然会出现不同的结果，但是由于服务端保证了返回的数据一定是在 listRV 之后的，也就不会出现上面 Stale Read 的问题。 篇幅有限，将会在下一篇中介绍社区是如何消除 Informer 中 List 请求从而降低 kube-apiserver 内存使用的，以及优化后的效果，敬请关注~ ","date":"2023-11-04","objectID":"https://www.likakuli.com/posts/staleread/:2:0","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - Stale Read","uri":"https://www.likakuli.com/posts/staleread/"},{"categories":["源码分析"],"content":" 代码版本：v1.26 由来 前一篇已经介绍了 Informer 的实现，Informer 对 kube-apiserver 发起了 list 和 watch 请求。我们知道大规模集群下，kube-apiserver 会成为瓶颈，尤其在内存方面，相信很多人也遇到过 kube-apiserver OOM 等问题（碰巧的是最近线上连续出现两次 kube-apiserver OOM 的问题）。本篇主要讲 kube-apiserver 中 Informer 需要用到的两个接口 list 和 watch 的实现。 网上搜索的话，可以找到大量相关的源码解析的文章，这里我并不会去过多涉及代码，主要还是以讲原理、流程为主，最后简单介绍下当前存在的问题，理论实践相结合。本篇主要讲当前实现，只有了解了当前实现，明白了为什么会有问题，才知道如何去解决问题，接下来的一篇会详细分析如何解决这些问题。 原理 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:0:0","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"Cacher 加载 在之前一篇 kubernetes 月光宝盒 - 时间倒流中我们已经介绍过 watch 的实现机制。 核心组件：Cacher，watchCache，cacheWatcher，reflector。其中 watchCache 作为 reflector 的 store，Etcd 作为 listerWatcher 的 storage，store 和 listerWatcher 作为参数用来构造 reflector。数据流大致如下： kube-apiserver 启动，针对每种资源类型，调用其对应 cacher 的 startCaching，进而调用 reflector.ListAndWatch，触发 listerWatcher 的 list 和 watch，对应 Etcd list 之后再 watch，watch 时会创建 watchChan，从 Etcd 读到的结果会先进入到 watchChan 的 incomingEventChan 中，经过 transform 处理后发送到 watchChan 的 resultChan 中，供 reflector 消费； reflector 会消费上述 resultChan 的数据，即 watch.Event 对象，并根据事件类型调用 store 的增删改方法，此处 store 即 watchCache，经过 watchCache.processEvent 处理，组装 watchCacheEvent 对象，更新 watchCache 的 cache（大小自适应的唤醒缓冲区，保留历史 event）和 store（全量数据），并最终通过 eventHandler 将其发送到 cacher 的 incoming chan 中； cacher.dispatchEvents 消费 incoming chan 的数据，经过处理后发送给每个 cacheWatcher 的 input chan； 外部调用 kube-apiserver watch 请求后会创建一个对应的 cacheWachter 对象，最终到 cacheWatcher 的 Watch 处理机中，消费 input chan，调用 watchCacheEvent 进行事件分发； ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:1:0","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"Cacher 数据流 用来缓存数据的核心结构是 watchCache，其内部又两个关键结构：cache（cyclic buffer），store（thread safe store），分别用来存储历史的 watchCacheEvent 和真实的资源对象，其中 store 里面存储的是全量对象，而 cache 虽然是自适应大小的，但还是有最大容量限制的，所以他存储的 watchCacheEvent 所代表的对象集合并不一定能覆盖 store 的全部数据。 历史问题 kube-apiserver 在优化自身内存使用方面做了很多优化了，不过至今仍然存在一些尚未完全解决的问题。 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:2:0","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"kube-apiserver OOM ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:3:0","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"内存消耗来源 kube-apiserver 的内存消耗，主要两个来源： 一部分来自于他缓存了集群所有数据（Event 除外，此 Event 为 k8s 的资源类型），并且为每种资源缓存了历史 watchCacheEvent，以及一些内部的数据结构和 chan 等，这部分是不可避免的，虽然可以适当优化，但作用并不大； 另一部分来自于客户端请求，尤其是 list 请求，kube-apiserver 需要在内存中进行数据深拷贝，序列化等操作，所需内存量和数据量、请求量正相关，随着数据量的增加，请求量的增加，所需要的内存也越大，而且这部分的内存通过 golang GC 是没有办法完全回收的，而 list 请求的主要来源就是 Informer； list 请求占用内存多的原因如下： 默认情况下（没有指定 resourceversion 的情况下），直接从etcd获取数据可能需要大量内存，超过数据存储的完整响应大小数倍； 请求明确指定 ResourceVersion 参数来从缓存中获取数据（例如，ResourceVersion=“0”），这实际上是大多数基于 client-go 的控制器因性能原因而使用的方法。内存使用量将比第一种情况低得多。但这并不是完美的，因为我们仍然需要空间来存储序列化对象并保存完整响应，直到发送。 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:3:1","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"常见场景 有两个常见的容易引起 kube-apiserver OOM 的场景： 一些 DaemonSet 之类的程序里面用到了 Informer，在进行变更，或者故障重启的时候，随着集群规模的增加，请求量也随之增加，对 kube-apiserver 内存的压力也增加，在没有任何防护措施（限流）的情况下，很容易造成 kube-apiserver 的 OOM，而且在 OOM 之后，异常连接转移到其他 master 节点，引起雪崩。理论上也属于一种容量问题，应对措施扩容（加 master 节点）、限流（服务端限流：APF、MaxInflightRequest等，及客户端限流）。 某种类型资源的数据量很大，kube-apiserver 配置的 timeout 参数太小，不足以支持完成一次 list 请求的情况下，Informer 会一直不断地尝试进行 list 操作，这种情况多发生在控制面组件，因为他们往往需要获取全量数据。 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:3:2","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"too old resource version ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:4:0","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"原理 严格说，这并不能算是一个问题，机制如此，理论上单机资源无限的情况下是可以避免这个现象的。为了方便描述，用 RV 代指 resourceversion。 其本质是客户端在调用 watch api 时携带非 0 的 RV，服务端在走到 cacher 的 watch 实现逻辑时需要根据传入的 RV 去 cyclic buffer 中二分查找大于 RV 的所有 watchCacheEvent 作为初始 event 返回给客户端。当 cyclic buffer 的最小 RV 还要比传入的 RV 大时，也就是说服务端缓存的事件的最小 RV 都要比客户端传过来的大，意味着缓存的历史事件不全，可能是因为事件较多，缓存大小有限，较老的 watchCacheEvent 已经被覆盖了。 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:4:1","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"常见场景 这种情况多发生在 kubelet 连接 apiserver 的场景下，或者说 watch 带了 labelselector 或 fieldselector 的情况下。因为每个 kubelet 只关心自己节点的 Pod，如果自身节点 Pod 一直没有变化，而其他节点上的 Pod 变化频繁，则可能 kubelet 本地 Informer 记录的 last RV 就会比 cyclic buffer 中的最小的 RV 还要小，这时如果发生重连（网络闪断，或者 Informer 自身 timeout 重连），则可以在 kube-apiserver 的日志中看到 “too old resoure version” 的字样。 kube-apiserver 重启的场景，如果集群中部分类型资源变更频繁，部分变更不频繁，则对于去 watch 变更不频繁的资源类型的 Informer 来说起本地的 last RV 是要比最新的 RV 小甚至小很多的，在 kube-apiserver 发生重启时，他以本地这个很小的 RV 去 watch，还是有可能会触发这个问题； 客户端 Informer 遇到这个报错的话会退出 ListAndWatch，重新开始执行 LIstAndWatch，进而造成 kube-apiserver 内存增加甚至 OOM。问题本质原因：RV 是全局的。场景的景本质区别在于场景 1 是在一种资源中做了筛选导致的，场景 2 是多种资源类型之间的 RV 差异较大导致的。 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:4:2","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"优化 经过上述分析，造成这个问题的原因有两个： cyclic buffer 长度有限； 客户端 Informer 持有的 last RV 过于陈旧； 社区也已经在多个版本之前进行了优化来降低这个问题出现的概率。 针对问题一，采用了自适应窗口大小，虽然还是会有问题，但相比之前写死一个值出现问题的概率要小，同时在不必要的时候缩小长度，避免内存资源的浪费。 针对问题二，有两个优化，引入了 BOOKMARK 机制来优化同一种资源不同筛选条件导致的问题，BOOKMARK 是一种 event 类型，定期将最新的 RV 返回客户端；引入 ProgressNotify 解决多种资源类型 RV 差异较大，在 kube-apiserver 重启后，Informer resume 时导致的问题，本质是利用了 Etcd 的 clientv3 ProgressNotify 的机制，kube-apiserver 在 Watch Etcd 的时候携带了特定的 Options 开启此功能。ProgressNotify 参考 Etcd 官方文档： WithProgressNotify makes watch server send periodic progress updates every 10 minutes when there is no incoming events. Progress updates have zero events in WatchResponse. 详情可以参考如下 KEP 956-watch-bookmark 和 1904-efficient-watch-resumption ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:4:3","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"stale read 这更是一个历史悠久的问题了，自从有了 watchCache 之后就有了这个问题，本质是将之前直接访问 Etcd 时的线性一致性读（Etcd 提供的能力），降级成了读 kube-apiserver cache 的顺序一致性。 ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:5:0","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":"场景 T1: StatefulSet controller creates pod-0 (uid 1) which is scheduled to node-1 T2: pod-0 is deleted as part of a rolling upgrade node-1 sees that pod-0 is deleted and cleans it up, then deletes the pod in the api The StatefulSet controller creates a second pod pod-0 (uid 2) which is assigned to node-2 node-2 sees that pod-0 has been scheduled to it and starts pod-0 The kubelet on node-1 crashes and restarts, then performs an initial list of pods scheduled to it against an API server in an HA setup (more than one API server) that is partitioned from the master (watch cache is arbitrarily delayed). The watch cache returns a list of pods from before T2 node-1 fills its local cache with a list of pods from before T2 node-1 starts pod-0 (uid 1) and node-2 is already running pod-0 (uid 2). 详情可以参考 issue 59848。 思考 我们经常看到各种源码分析，原理解析的文章，容易轻信其内容，但随着版本迭代，以及一些细节的处理，可能会导致我们理解不到位，或者并不能真正的掌握。例如是否在 list 请求时传 RV=0 就一定会走 kube-apiserver 的缓存？网上搜的话，应该都是说会走，但看代码你会发现并不是这样，例如当 kube-apiserver 重启后数据还没有完全加载好的时候，遇到 list 带了 RV=0 的请求会直接去访问 Etcd 获取数据。看似不起眼的细节，可能会影响我们处理问题的思路，比如 Etcd 负载较高要查原因，如果你知道这个细节的话，就会有意识的去看所有的 list 请求，而不只是那些 RV != 0 的请求。 最后留一个思考，kube-apiserver 的内存压力主要来自 list 请求，那么我们是否可以不使用 list 请求而是使用一种流式处理来实现 list 的功能呢？这样是不是就可以把内存消耗限制在一个常数的空间复杂度范围内了？下一篇将会专门分析使用流式 api 解决 list 导致的内存暴涨的问题，敬请期待~ ","date":"2023-10-24","objectID":"https://www.likakuli.com/posts/cacher/:5:1","tags":["kubernetes"],"title":"kube-apiserver 又 OOM 了？","uri":"https://www.likakuli.com/posts/cacher/"},{"categories":["源码分析"],"content":" 代码版本 v.126 由来 Informer 作为 client-go 的核心，网上有众多的源码分析，原理解析相关文章，可以教给大家如何\"正确\"的使用 Informer。当然其前提是在 Informer 本身逻辑没问题的前提下，本篇旨在为大家指出几个 Informer 里面长期存在甚至现在仍然存在的问题。 通过这些问题可以检查下自己是否真的对工作原理理解了，而不仅仅是停留在使用的阶段，也希望引起大家注意，遇到相关问题时可以及时定位原因，解决问题，最好是可以在最初就避免问题。 下文假设大家已经看过 Informer 源码，或者看过相关源码解析的文章，因此在本文中不会过多涉及代码。 测验 先尝试回答几个问题，来看看自己对 Informer 的理解和掌握情况。 Informer vs SharedInformer 的区别，shared 的内容是什么？ 什么场景下要开 Resync？Informer 同步的内容是什么？ HasSynced 返回 true，代表什么意思？ 使用 NamespacedName 作为 key，有没有问题？ 答案 区别：前者只能注册单个 ResourceEventHandler，后者可以注册多个，也就是说当你需要在一个事件来了之后去触发多个动作或者有多个观察者的话，那就用后者，否则可以用前者；shared 的是 Reflector； 需要与外部（非 k8s）的组件或者基础设施交互时使用 SharedInformer，SharedInformer 同步是将 Store 里面存储的对象重新放回 Queue 中，重新触发一次全量通知，Resync 并不会触发重新去 kube-apiserver 获取数据；Informer 的 Resync 不会执行任何操作，直接返回； 对于 Informer，HasSynced 返回 true 代表其所注册的 event handler 已经执行完了，针对 SharedInformer，则代表 Queue（Reflector Store）里面的数据已经全部 Pop 出来，经过处理将对应的 Indexer 里面的数据分发给所有的 processorListener 了，但是否注册的所有的 event handler 已经都处理完了并不一定； 一般索引都用 ID 表示，使用 NamespacedName 会有潜在的问题，后文会举例说明； 前两个问题出现频率比较高，应该没什么问题，主要分析后两个。 分析 一图胜千言，我们按这个图来依次解释（瞎比比)上面的几个问题。 ","date":"2023-10-14","objectID":"https://www.likakuli.com/posts/informer1/:0:0","tags":["kubernetes"],"title":"你真的搞懂 Informer 了吗？","uri":"https://www.likakuli.com/posts/informer1/"},{"categories":["源码分析"],"content":"区别 一个 Informer 只能通知一个观察者，而一个 SharedInformer 可以通知多个观察者。需要触发的行为通过 ResourceEventHandler 注册，对于 Informer，ResourceEventHandler 是通过参数在创建 Informer 的时候传递的，直接保存在自身结构内，而 SharedInformer 则是单独提供了方法来添加 ResourceEventHandler，保存在 SharedProcessor 内，而 SharedProcessor 保存在 SharedInformer 内。 ","date":"2023-10-14","objectID":"https://www.likakuli.com/posts/informer1/:1:0","tags":["kubernetes"],"title":"你真的搞懂 Informer 了吗？","uri":"https://www.likakuli.com/posts/informer1/"},{"categories":["源码分析"],"content":"Shared 内容 最终发送的通知事件内容是存储在 Reflector Store 中的，也就是说为了通知多个观察者，Informer 需要有多个实例，每个实例都维护一份全量数据，而 SharedInformer 只需要一个实例，维护全量数据，有利于降低内存占用。 ","date":"2023-10-14","objectID":"https://www.likakuli.com/posts/informer1/:2:0","tags":["kubernetes"],"title":"你真的搞懂 Informer 了吗？","uri":"https://www.likakuli.com/posts/informer1/"},{"categories":["源码分析"],"content":"Resync 本质是用 Reflector Store 也就是 DeltaFIFO 中 KnownObjects 数据同步 Queue，对应上图步骤 x。 ","date":"2023-10-14","objectID":"https://www.likakuli.com/posts/informer1/:3:0","tags":["kubernetes"],"title":"你真的搞懂 Informer 了吗？","uri":"https://www.likakuli.com/posts/informer1/"},{"categories":["源码分析"],"content":"HasSynced 还记得之前发过的一篇设计一个分发器的内容吗？k8s 在这里巧妙的借助了两个无缓冲区的 chan 和一个 RingBuffer 实现了事件异步分发给多个观察者的能力。这块的代码值得大家学些下，可以看 client-go 的实现，也可以看这个精简版的实现：dispatcher。 针对 Informer 来说，HasSynced 返回 true 就是代表其注册的 ResourceEventHandler 已经处理完一遍全部数据了，而针对 SharedInformer 而言，只代表了图 7 完成，从 7 开始，后面的执行过程和前面的是异步的，所以到底是否已经被 ResourceEventHandler 处理完一遍了，未可知。 这也就意味着你可能一直在毫不知情的错误的使用 SharedInformer。尤其针对一些在真正开始执行具体逻辑之前需要先同步处理完一遍全量数据的场景，例如调度器在真正调度 Pod 之前需要确保已经利用全量数据构建好了本地缓存，否则可能会导致错误的调度结果。 此问题在做模拟调度项目时碰到了，社区在 PR 113985 修复此问题，随着 v1.27 中发布。解决方案是为每个 ResourceEventHandler 添加 HasSynced 方法，针对上述需要确保全量数据需要被处理完成一遍的场景，判断对应 ResourceEventHandler 的 HasSynced 即可。 ","date":"2023-10-14","objectID":"https://www.likakuli.com/posts/informer1/:4:0","tags":["kubernetes"],"title":"你真的搞懂 Informer 了吗？","uri":"https://www.likakuli.com/posts/informer1/"},{"categories":["源码分析"],"content":"NamespacedName 考虑如下场景： SharedInformer 收到 Pod1 的 Add 事件，并将 Pod1 添加到 Store 中； SharedInformer 与 Kubernetes API 断开连接，并尝试使用 ResourceVersion1 重连； Pod1 被删除后立马又被创建出来，对应 ResourceVersion2； Etcd 执行了压缩，ResourceVersion2 之前的历史记录都没了； SharedInformer 重新连接成功； 由于 ResourceVersion1 和 ResourceVersion2 之间数据丢失，导致 SharedInformer 得不到 Pod1 Delete 事件，在重连后可以正常看到后创建的 Pod1，此时的处理逻辑是去看下能否通过 Pod1 NamespacedName 从本地 Store 获取到对应的记录，获取到的话则认为是 Update 事件并进行分发，而正常应该将这种特殊行为判定为 Delete、Add 两个事件，因为对象的 UID 发生了变化，虽然名字没变，但已经不是之前的对象了。 这也可能会触发一些问题，为了避免这些问题，对事件类型有依赖的场景还需要在接收到 Update 事件之后，再根据新旧对象的 UID 再次判断是真正的 Update 还是 Delete 与 Add 的合并。 总结 历史版本中出现过众多问题，甚至一些至今仍存在的问题，有一些是细节处理，有一些是设计问题，上面分析到的也只是一小部分，其他不再一一介绍。这里介绍 Informer 是为后续做铺垫，方便理解后续的文章。我们知道大规模集群下，APIServer 会成为瓶颈，尤其在内存方面，相信很多人也遇到过类似 APIServer OOM 等问题，后续会继续分析 Watch 在客户端，服务端的实现，以及 APIServer 内存到底来自哪些部分（请求），空间复杂度如何，如何根据数据量评估内存需求，以及如何优化内存使用。敬请期待… ","date":"2023-10-14","objectID":"https://www.likakuli.com/posts/informer1/:5:0","tags":["kubernetes"],"title":"你真的搞懂 Informer 了吗？","uri":"https://www.likakuli.com/posts/informer1/"},{"categories":["源码分析"],"content":"kubernetes 时间倒流","date":"2023-09-25","objectID":"https://www.likakuli.com/posts/backclock/","tags":["kubernetes"],"title":"kubernetes 月光宝盒 - 时间倒流","uri":"https://www.likakuli.com/posts/backclock/"},{"categories":["源码分析"],"content":"背景 碰到一个\"诡异\"的线上问题，已经定位到原因，虽然不是什么大问题，但感觉还是挺有意思的。在远古时期（k8s 1.7）中也有一个类似回溯的现象，不过从现象看远古那次是事件回溯，现在是时间回溯。接下来分别看下两个 case 和其背后的逻辑。 时间回溯 ","date":"2023-09-25","objectID":"https://www.likakuli.com/posts/backclock/:0:0","tags":["kubernetes"],"title":"kubernetes 月光宝盒 - 时间倒流","uri":"https://www.likakuli.com/posts/backclock/"},{"categories":["源码分析"],"content":"现象 - lastProbeTime: null lastTransitionTime: t1 status: \"True\" type: Ready 上面是 Pod spec.status.conditions 中的 PodReady Condition，表示在 t1 时刻，Pod 变成 Ready 状态。 t1 时刻 watch 到 Pod Ready，lastTransitionTime 为 t1’； t2 时刻 watch 到 Pod NotReady，lastTransitionTime 为 t2‘； t3 时刻 watch 到 Pod Ready，lastTransitionTime 为 t3’； 常理来说 t3‘ \u003e t2‘ \u003e t1’，但诡异的是 t3’ == t1‘，也就是说在 t3 时刻看到的效果就是 Pod 在 t1‘ 时刻就已经 Ready 了，这与实际（t2‘ 到 t3’ 中间这段时间 Pod NotReady）不符。 经过查看 kubelet 代码发现具体处理逻辑在如下 func (m *manager) updateStatusInternal(pod *v1.Pod, status v1.PodStatus, forceUpdate, podIsFinished bool) { var oldStatus v1.PodStatus cachedStatus, isCached := m.podStatuses[pod.UID] if isCached { oldStatus = cachedStatus.status // TODO(#116484): Also assign terminal phase to static pods. if !kubetypes.IsStaticPod(pod) { if cachedStatus.podIsFinished \u0026\u0026 !podIsFinished { klog.InfoS(\"Got unexpected podIsFinished=false, while podIsFinished=true in status cache, programmer error.\", \"pod\", klog.KObj(pod)) podIsFinished = true } } } else if mirrorPod, ok := m.podManager.GetMirrorPodByPod(pod); ok { oldStatus = mirrorPod.Status } else { oldStatus = pod.Status } ... // Set ReadyCondition.LastTransitionTime. updateLastTransitionTime(\u0026status, \u0026oldStatus, v1.PodReady) ... } // updateLastTransitionTime updates the LastTransitionTime of a pod condition. func updateLastTransitionTime(status, oldStatus *v1.PodStatus, conditionType v1.PodConditionType) { _, condition := podutil.GetPodCondition(status, conditionType) if condition == nil { return } // Need to set LastTransitionTime. lastTransitionTime := metav1.Now() _, oldCondition := podutil.GetPodCondition(oldStatus, conditionType) if oldCondition != nil \u0026\u0026 condition.Status == oldCondition.Status { lastTransitionTime = oldCondition.LastTransitionTime } condition.LastTransitionTime = lastTransitionTime } kubelet 内部维护了从 container runtime 获取到的 status 信息以及探活信息保存在 podStatuses 结构中，updateStatusInternal 每次执行到之后会先从此结构中获取，对应上面场景是会获取到的， oldStatus 就是缓存中的值。注意 podStatuses 的内容和 k8s pod spec 内容无关，也就是说上面场景即使在 t2’ 时刻 Pod NotReady 了，但在 podStatuses 中仍然是在 t1‘ Pod Ready 的内容。所以在 t3 时刻 Pod Ready 后，在调用 updateLastTransitionTime 时对比了 status 和 oldStatus 的 Status 值（都是 true）发现一致，则最终 condition.LastTransitionTime 会被设置为 oldCondition.LastTransitionTime 即 t1’。 ","date":"2023-09-25","objectID":"https://www.likakuli.com/posts/backclock/:1:0","tags":["kubernetes"],"title":"kubernetes 月光宝盒 - 时间倒流","uri":"https://www.likakuli.com/posts/backclock/"},{"categories":["源码分析"],"content":"Feature Or Bug? 不管是 feature 还是 bug，目前对于我们来说这个问题只是看起来时间回溯了，并没有实际的影响。就此问题也咨询了社区，参考 issue: 119514。在 kubelet 里面有一段注释： // needsReconcile compares the given status with the status in the pod manager (which // in fact comes from apiserver), returns whether the status needs to be reconciled with // the apiserver. Now when pod status is inconsistent between apiserver and kubelet, // kubelet should forcibly send an update to reconcile the inconsistence, because kubelet // should be the source of truth of pod status. // NOTE(random-liu): It's simpler to pass in mirror pod uid and get mirror pod by uid, but // now the pod manager only supports getting mirror pod by static pod, so we have to pass // static pod uid here. // TODO(random-liu): Simplify the logic when mirror pod manager is added. func (m *manager) needsReconcile(uid types.UID, status v1.PodStatus) bool { ... } 关键一句：kubelet should be the source of truth of pod status. 也可以理解，因为毕竟 kubelet 才是真正干活的，只有他知道 container 的真实状态，但 Pod 的状态就并不一定是只有 kubelet 会更新了，这就是一个比较 trick 的地方。 Pod Ready Condition 除了在 kubelet 中会设置外，在 kube-controller-manager 中也会设置，当 Node NotReady 一段时间后，其上所有 Pod 都会被 controller-manager 标记为 NotReady，同时更新 LastTransitionTime。 所以这个现象也是很容易的复现，步骤如下： start a http proxy in a worker node to proxy kube-apiserver request set kubelet to use this proxy instead of directly use kube-apiserver schedule some pod to this node and wait pod running, record the LastTransitionTime of PodReady true condition as time1. stop the proxy and wait node not ready and pod ready condition changed to false which set by kube-controller-manager, record the LastTransitionTime of PodReady false condition as time2. then start the proxy again and wait pod ready, record the LastTransitionTime of PodReady true condition as time3. you will see time3 == time1 and time3 is before time2 可以使用 socat 作为 http proxy 代理使用，轻松复现。 Kube-controller-manager 为什么要去操作 Pod Ready 状态呢，因为 endpoint controller 对这个状态有依赖，只有 Pod Ready 后，Pod IP 才会被当做可用，也就是说这个状态直接影响使用 endpoint 来进行通信的场景，而 endpoint 几乎所有云原生网络相关组件都会用到。也就是说如果你也使用了 endpoint，在出现上述问题时，可能某些 Pod 在一段时间内没有流量，但在最后你去查问题的时候，看 Pod 状态发现他很早之前就是 Ready 状态的，这就容易误导人，如果真如 Condition LastTransitionTime 所述那应该在 t1 时刻开始 Pod 一直有流量，但实际确实在 t2 到 t3 时刻，Pod 并未接流。所以这到底算是 bug 还是 feature 呢？ 不管是 feature 还是 bug，这个问题是需要解决的，而解决办法最好是再额外加一个 Condition，Pod Ready Condition 只留给 kubelet 去操作，其他组件都是读，同时每个组件按需增加自己的 Condition，结合 Pod Ready Condition 去做一些逻辑处理，比如 node controller 在发现 Node NotReady 后添加 Custom Condition，然后 endpoint controller 结合 Pod Ready Condition 与 Custom Condition 状态判断 Pod 是否可以接流。 时间回溯虽然看起来奇怪，好在并没有实际影响。但接下来事件回溯就厉害了，他是真正的让其他组件感觉到了时间回到了过去，并且历史发生的事情又重来一遍。 事件回溯 这个问题发生在 2017 年，某天早上起来，一篇名为 《Kubernetes 惊天地泣鬼神之大Bug》在知乎上，朋友圈，技术群里面疯狂转发，瞬间炸了锅。 ","date":"2023-09-25","objectID":"https://www.likakuli.com/posts/backclock/:2:0","tags":["kubernetes"],"title":"kubernetes 月光宝盒 - 时间倒流","uri":"https://www.likakuli.com/posts/backclock/"},{"categories":["源码分析"],"content":"现象 问题的现象是，在某种情况下，一个或者多个 Kubernetes service 对应的 Kubernetes endpoints 消失几分钟至几十分钟，然后重新出现，然后又消失。 这可就是实打实的影响集群里面正在运行的服务了，此问题在 v1.7 版本引入，在 v1.9 中修复，影响还是比较大的，当然我们当时并没有遇到这个问题，因为我们使用的还是 v1.6，所以使用新版本还是有风险的，哈哈。 ","date":"2023-09-25","objectID":"https://www.likakuli.com/posts/backclock/:3:0","tags":["kubernetes"],"title":"kubernetes 月光宝盒 - 时间倒流","uri":"https://www.likakuli.com/posts/backclock/"},{"categories":["源码分析"],"content":"原因 Fixes a bug in the cache watcher where we were returning the “current” object from a watch event, not the historic event. This means that we broke behavior when introducing the watch cache. This may have API implications for filtering watch consumers - but on the other hand, it prevents clients filtering from seeing objects outside of their watch correctly, which can lead to other subtle bugs. 问题是在 pr: 46223 引入的，当时是为了修复另外一个 bug 而引入的新的 bug。因为此问题和 cache watcher 的 watch 实现有关，所以先看下 kube-apiserver 有关 watch 的实现机制 图片来自：https://zhuanlan.zhihu.com/p/33335726 先讲一下 store 和 storage 的区别，store 一般用来作为一个 service 对外提供服务，对外屏蔽存储细节，而 storage 是 store 内的一个结构，用来与底层存储交互，这是一个很常见的分层设计。 以 Pod 为例，这里涉及到两层或者说两对 store 和 storage： 外层 store（上图并未给出） 用来和 restful api 交互，即通过 k8s api 访问 Pod 的处理请求在 kube-apiserver 内部都会走到这个 store 上，其内部对应的 storage 为上图中所示的 PodStorage； 内层的 store 为上图的 Pod.Store，最终对应 Cacher，而内层的 storage 才是直接与 Etcd 交互； 核心组件：Cacher，watchCache，cacheWatcher，reflector。其中 watchCache 作为 reflector 的 store，Etcd 作为 listerWatcher 的 storage，store 和 listerWatcher 作为参数用来构造 reflector。数据流大致如下： cacher.startCaching 调用 reflector.ListAndWatch，进而触发 listerWatcher 的 list 和 watch，对应 Etcd list 之后再 watch，watch 时会创建 watchChan，从 Etcd 读到的结果会先进入到 watchChan 的 incomingEventChan 中，经过 transform 处理，对于 delete 类型的事件，处理时会直接 prevValue 的值，而其他类型事件则直接使用其 value，处理后的对象的 ResourceVersion 为 Etcd event 的 ModRevision，之后进行数据过滤（例如带着 LabelSelector 的请求，也可以看到筛选是在获取全量数据到 Etcd 后在 kube-apiserver 里面进行的），再以符合条件的数据构建 watch.Event 对象，并后发送到 watchChan 的 resultChan 中，供 reflector 消费； reflector 会消费上述 resultChan 的数据，即 watch.Event 对象，并根据事件类型调用 store 的增删改方法，此处 store 即 watchCache，经过 watchCache.processEvent 处理，组装 watchCacheEvent 对象，并最终通过 eventHandler 将其发送到 cacher 的 incoming chan 中； cacher.dispatchEvents 消费 incoming chan 的数据，经过处理后发送给每个 cacheWatcher 的 input chan； 外部调用 kube-apiserver watch 请求后，最终到 cacheWatcher 的 Watch 处理机中，消费 input chan，调用 watchCacheEvent 进行事件分发； Event 转换示意：终于介绍完了一些基础，最终导致事件回溯的原因就在最后的 sendWatchCacheEvent 中，针对 delete 请求，有问题的代码如下 // NOTE: sendWatchCacheEvent is assumed to not modify \u003cevent\u003e !!! func (c *cacheWatcher) sendWatchCacheEvent(event *watchCacheEvent) { ... var watchEvent watch.Event switch { case curObjPasses \u0026\u0026 !oldObjPasses: ... case curObjPasses \u0026\u0026 oldObjPasses: ... case !curObjPasses \u0026\u0026 oldObjPasses: object, err := c.copier.Copy(event.PrevObject) if err != nil { utilruntime.HandleError(fmt.Errorf(\"unexpected copy error: %v\", err)) return } watchEvent = watch.Event{Type: watch.Deleted, Object: object} } ... } 其直接使用了 watchCacheEvent 的 PrevObject，而他的 ResourceVersion 是一个过去时的值，例如连续创建三个 Pod，再删除第一个 Pod，则此时返回的 watchEvent 的 Object 对象的 ResourceVersion 是第一个 Pod 创建后的 ResourceVersion，但实际情况是最新的 ResourceVersion 已经随着后两个 Pod 的创建而递增了。也就是此时客户端 watch 到一个 delete event 之后，客户端 informer 所使用的 reflector 内部维护的 resourceVersion 已经不对了，是一个历史值，如果此时发生一个问题（例如网络闪断）需要去重新 watch 时，会使用这个错误的 resourceVersion，也就是这个版本之后的所有符合条件的事件会再次接收并处理一遍，真正的回到过去，历史重演。 此问题在 v1.9 中修复，pr: 58547，修复后的逻辑如下 // NOTE: sendWatchCacheEvent is assumed to not modify \u003cevent\u003e !!! func (c *cacheWatcher) sendWatchCacheEvent(event *watchCacheEvent) { ... var watchEvent watch.Event switch { case curObjPasses \u0026\u0026 !oldObjPasses: ... case curObjPasses \u0026\u0026 oldObjPasses: ... case !curObjPasses \u0026\u0026 oldObjPasses: // return a delete event with the previous object content, but with the event's resource version oldObj := event.PrevObject.DeepCopyObject() if err := c.versioner.UpdateObject(oldObj, event.ResourceVersion); err != nil { utilruntime.HandleError(fmt.Errorf(\"failure to version api object (%d) %#v: %v\", event.ResourceVersion, oldObj, err)) } watchEvent = watch.Event{Type: watch.Deleted, Object: oldObj} } ... } 仍然使用 watchCacheEvent 的 PrevObject，但会使用 watchCacheEvent 的 ResourceVersion 覆盖前者的 ResourceVersion。 ","date":"2023-09-25","objectID":"https://www.likakuli.com/posts/backclock/:4:0","tags":["kubernetes"],"title":"kubernetes 月光宝盒 - 时间倒流","uri":"https://www.likakuli.com/posts/backclock/"},{"categories":["源码分析"],"content":"kubernetes 陈年老 bug 之绑核","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"背景 最近遇到一个线上问题，使用了 lxcfs 的容器，跑在 cgroup v2 的机器上时，在容器内使用 top 或者 htop 看到的核数和 cpu 使用率有问题。虽然根本问题在 lxcfs 的实现，但问题最终的触发与 cpuset 和 cpu.max 的设置有关。这里并不会去介绍这个问题本身，而是由问题引发的思考，我们真的了解 k8s 里面的绑核行为吗，你以为你以为的就是你以为的吗？ 测验 接下来你可以尝试回答如下几个问题，并且通过实际的在 k8s 集群中进行操作来验证。为了方便描述，我们假设有一台最新的 v1.28 的 worker 节点有 12c，其分别为 system 以及 kubelet 各预留了 1c，那么他实际可用的核数将会是 10c，也就是 allocatable cpu 是 10c，并且其 cpuManagerPilicy 为 staitic 模式，cpuManagerReconcilePeriod 使用默认值 10s。 问题一：在没有分配任何 Pod 时，cpu_manager_state 默认的 defaultCPU 是多少核？ 问题二：调度一个 Burstable Podb，其 cpu request 4c，cpu limit 8c，那么 Podb 的 cpuset 有几个核？ 问题三：基于问题二，再调度一个 Guaranteed Podg1，使用 6c，那么 Podb 的 cpuset 有几个核？ 问题四：删除 Podg1，那么 Podb 的 cpuset 有几个核？ 问题五：重复问题三的步骤，创建 Podg2，继续看 Podb 有几个核？ 答案 前两个问题答案都是 12c，后三个问题答案都是 6c。这个结果应该和绝大部分人以为的结果不一样，如果不相信的话，可以自行验证。涉及到一些考古信息，方便大家了解整个过程，当然最后总结部分也有按时间线提炼，可以直接跳到那里看。如果想都搞清楚搞明白的话，可能还真得需要一些时间才行。 分析 这里主要介绍两点，一个算是 feature：共享资源池，一个算是 bug：资源释放。估计大家会好奇问题四，而这也是这里要重点讲的。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:0:0","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"共享资源池 第一点在于 defaultCPU 的设置，官网文档很明确的写了 https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/cpu-management-policies/ 此策略管理一个 CPU 共享池，该共享池最初包含节点上所有的 CPU 资源。 可独占性 CPU 资源数量等于节点的 CPU 总量减去通过 kubelet --kube-reserved 或 --system-reserved 参数保留的 CPU 资源。 从 1.17 版本开始，可以通过 kubelet --reserved-cpus 参数显式地指定 CPU 预留列表。 由 --reserved-cpus 指定的显式 CPU 列表优先于由 --kube-reserved 和 --system-reserved 指定的 CPU 预留。 通过这些参数预留的 CPU 是以整数方式，按物理核心 ID 升序从初始共享池获取的。 共享池是 BestEffort 和 Burstable Pod 运行的 CPU 集合。 Guaranteed Pod 中的容器，如果声明了非整数值的 CPU requests，也将运行在共享池的 CPU 上。 只有 Guaranteed Pod 中，指定了整数型 CPU requests 的容器，才会被分配独占 CPU 资源。 也就是说预留资源属于共享资源池资源，可以被非 Guaranteed Pod 共享。这可以解释前两个问题。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:1:0","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"资源释放 对于问题四，为什么删除了 Podg1 后，Podb 的 cpuset 还是只有 6c？此时去看 cpu_state_manager 文件内容的话，可以看到 defaultCPU 只有 6c，而且 Podg1 的信息仍然还存在。既然删了之后没释放，可用的只有 6c 了，那继续问题五的步骤，岂不是 cpu 会被用完，导致没有剩余的 cpu 给 Podb 用了？但结果 Podb 仍然还有 6c 可用，此时再去看 cpu_manager_state 文件内容，defaultCPU 仍然是 6c，但已经分配的内容变了，换了个 PodID，但实际也是只有一个 Pod 分配了 6c，而不是两个。是不是很神奇，看起来泄露了一个，等新的 Podg2 调度上去之后，之前未删除的记录反而消失了。 之前曾经整理过一篇有关绑核的 bug，但那是针对较低版本的 k8s 的，可以看这篇：https://www.likakuli.com/posts/kubernetes-cpu-manager/。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:2:0","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"ISSUE 感兴趣的可以翻翻历史 ISSUE，此问题从 1.8 开始有大量相关的 ISSUE，下面列几个比较典型的 Internal PreStartContainer hook failed: not enough cpus available to satisfy request #63018 cpumanager: AddContainer error: not enough cpus available to satisfy request #79159 TopologyManager: Guarantee Aligned resources for Multiple Containers #83476 Container cpuset lost, apparently due to race between PostStopContainer() and new container creation #90303 The CPU manager does not work correctly for the guaranteed pod with multiple containers #103952 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:2:1","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"PR Make CPU manager release CPUs when Pod enters completed phase #52363 cpumanager: rollback state if updateContainerCPUSet failed #67430 clean containers in reconcileState of cpuManager #68619 Update CPUManager stored state semantics #84462 Fix exclusive CPU allocations being deleted at container restart #90377 Do not clear state of pods pending admission for CPU/Memory/Device manager #103979 灵魂拷问：就一个绑核，有那么难吗，这么多问题。接下来分析下问题的原因，怎么引入的，怎么修复。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:2:2","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"原因 先说结论，v1.22 对 kubelet 进行了比较大的重构，这个过程中引入了很多的问题，绑核这只是其中的一个问题。此问题在 2021 年 v1.22 引入，至今（2023.9 v1.28）仍然存在。根本原因在如下代码处： func (m *manager) removeStaleState() { // Only once all sources are ready do we attempt to remove any stale state. // This ensures that the call to `m.activePods()` below will succeed with // the actual active pods list. if !m.sourcesReady.AllReady() { return } // We grab the lock to ensure that no new containers will grab CPUs while // executing the code below. Without this lock, its possible that we end up // removing state that is newly added by an asynchronous call to // AddContainer() during the execution of this code. m.Lock() defer m.Unlock() // Get the list of active pods. activeAndAdmittedPods := m.activePods() if m.pendingAdmissionPod != nil { activeAndAdmittedPods = append(activeAndAdmittedPods, m.pendingAdmissionPod) } # 垃圾清理 ... } 首先调用 activePods 获取尚未 Completed 的 Pod，然后在把处于 admission 阶段的 Pod 也当做 active 的，那么剩下的就是需要清楚的了，pendingAdmissionPod 在执行到 admit 时会去将其设置为当前的 Pod，但是并没有一个把他置空的地方，即使 Pod 被删除了，这个资源还是有值的，是最后一个执行的 Pod，直到下一个 Pod 去 admit 时，才会更新为新 Pod 的值。这个过程就对应了上述问题三四五的结论，Podg1 虽然删了，但是 pendingAdmissionPod 还是 Podg1，kubelet 并不去清理之前为他分配的绑核记录，直到问题五再次调度一个 Podg2 过来的时候，这时候 pendingAdmissionPod 被设置为 Podg2，在执行到 removeStaleState 时会认为 Podg1 是需要删除的，进而释放其之前所占用的资源。 也比较好理解，总结就是有个变量记录了处于 admission 阶段的 Pod，但是在 Pod 被删除后并没有去重置，导致在 kubelet 看来这个已经被删除的 Pod 还在运行中，还需要为其保留之前的 cpuset 的信息。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:2:3","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"引入过程 直接引入 看起来好理解，好修复，但为什么需要一个单独的字段去保留处于 admission 阶段的 Pod？还是得考古一番。 pendingAdmissionPod 是 Do not clear state of pods pending admission for CPU/Memory/Device manager #103979 引入的，为了修复 The CPU manager does not work correctly for the guaranteed pod with multiple containers #103952 问题，简单描述下问题现象就是一个 Guaranteed Pod 里面有两个 Container，都需要在 admit 时分配 cpuset，第一个已经分配好，而等到第二个 Container 分配的时候，第一个已分配的绑核信息被回收了居然…，导致最终的异常分配（两个容器只有一条绑核信息）。 这个问题在之前的版本中是不存在，也就是说是在 v1.22 重构后才有的，于是大家首先想到的就是这是重构导致的问题，但重构的作者并不这么认为，而且他最终还说服了这个 pr 的作者。 简单总结就是，重构之前的 activePods 是会返回处于 admission 状态的 Pod 的，但是重构之后的不再返回这个阶段的 Pod，重构的作者认为处于 admission 阶段的 Pod 尚未为其分配 podworker，不算 active 的，而且给出了详细的解释，言辞凿凿。 https://github.com/kubernetes/kubernetes/issues/103952#issuecomment-887859214 https://github.com/kubernetes/kubernetes/issues/103952#issuecomment-888387811 他还给出了应对方案就是引入额外字段记录处于 admission 阶段的 Pod，然后就有了这个 pr，确实这个 pr 解决了对应 issue 的问题，但同时也引入了当前的问题。 间接引入 当前实现里面，触发回收绑核资源的只有一个地方，即 reconcileState，一个定时执行的 worker，每次执行时都会尝试清理废弃的绑核信息。那为什么不直接在 Pod 删除时进行回收或者 Container 删除时进行回收呢？ 这也是因为 Fix exclusive CPU allocations being deleted at container restart #90377 这个 pr，用来修复 Container cpuset lost, apparently due to race between PostStopContainer() and new container creation #90303 这个问题。 简单总结就是，在很早之前，确实是会在 container 删除时去清理其绑核信息的，但这个操作是在 PostStopContainer 中做的，这就会有个并发的问题，即老的 Container 删除，新的 Container 要去创建，同时还要去清理老的 Container 的绑核信息，创建新的 Container 和清理老的 Container 的绑核信息是并行执行的，如果先执行了清理，再执行创建，那么是没问题的，但反过来的话，由于新的 Container 已经创建并且分配了绑核信息，结果又被清理逻辑给清理了，导致丢失绑核信息。 那为什么清理老的 Container 的绑核信息的是会把新的 Container 的绑核信息清理掉呢，这是一个更早的问题。涉及到 cpu_manager_state 文件格式的改变。在最初，其保存的结构为 CPUManagerCheckpointV1 当前为 CPUManagerCheckpointV2，这是通过 Update CPUManager stored state semantics #84462 引入的，是为了修复 Internal PreStartContainer hook failed: not enough cpus available to satisfy request #63018，cpumanager: AddContainer error: not enough cpus available to satisfy request #79159 这两个问题的。 简单总结就是在 v1 版本中 checkpoint 单纯以 Container 为维度以 ContainerID 为 key 记录绑核信息，没有 Pod 的概念，这会导致一个，在容器创建成功，但是启动失败（例如节点 io 高，load 高等问题）时，记录中会存在多条信息，但其中只有一条是有用的，这就到导致资源泄露，虽然外部看起来还有空闲的 cpu，但无法被其他 Pod 使用。于是在 v2 中修改了结构，改成了一个两层的 map，外层以 PodID 为 key，内层以 ContainerName 为 key，这样就可以规避同一个 Pod 同一个 Container Name 有多个 Container 的问题。但也正是因为最终以 ContainerName 为 Key，导致在清理时，遇到先创建后清理的情况，由于是同一个 Pod 同一个 ContainerName，刚分配的绑核信息就会被清理掉。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:2:4","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["源码分析"],"content":"修复 问题四在 2021 年就已经被确认是一个 bug: Exclusive CPUs not removed from deleted Pod and put back in the defaultCPUSet. #107074，也有一个 pr: kubelet: fix exclusive CPUs not removed from deleted Pod #107732 来修复这个问题，但最后却 close 了，并没有合入 master。 为什么没有合呢，一个原因是被要求添加单元测试，但是提 pr 的作者迟迟没有添加，最后他自己关了这个 pr。看实现，确实是可以解决这个 bug 的，但是不是最优的方式呢？还得继续考古去。 上面提到问题的直接引入是为了解决重构引起的 activePods 返回结果的差异导致的问题，而就在修复问题的 pr 被合并之后，重构 kubelet 的作者在此 pr 后面回复如下 Note that I have a PR I’ll open soon that correctly accounts for all admitted and still running pods from the pod worker (I.e. any force deleted pod may still be terminating, so GetActivePods is incorrect today). When that lands it will fix the problem mentioned in the review threads where GetActivePods() is currently not broad enough. 也就是说他有一个新的 pr 来纠正 activePods 返回结果和重构前不一致的问题。在此之前还言辞凿凿的说这不是问题，返回结果中就不应该包含处于 admission 阶段的 Pod，啪啪打脸。 他后来提的这个 pr: kubelet: Admission must exclude completed pods and avoid races #104577 也已经合了，确实是解决了 activePods 返回值和重构前不一致的问题，进而解决了单个 Guaranteed Pod 多个 Container 时，已分的绑核信息随着后续 Container 准入时被回收的问题，也就是说有了这个 pr，那么之前那个修复了老问题引入了当前问题的 pr 就不再需要了，但是社区并没有去处理…，造成的结果就是直到现在了问题依然存在。 综合上述考古和分析，最直接的方案是直接 revert 引入此问题的 pr 即可。https://github.com/kubernetes/kubernetes/pull/120661 如果你心细的话，可能在介绍间接引入那一节会注意到我们只解释了为什么没有在删除 Container 时去释放资源，但并没有解释为什么不在删除 Pod 去清理资源。这里其实我个人认为是可以实现的，只需要在 PostStopContainer 时去传入 Pod 信息，并根据 Pod 状态决定是否要进行绑核资源释放即可。这样就可以既实现 Pod 删除时的资源释放，也避免新创建的 Container 绑核信息被误删，同样可以解决这个问题。 总结 按照时间线来做个总结： cpu_manager 实现早期其 checkpoint 以 ContainerID 为 key，导致在 Container 创建成功，但启动失败时，由于 kubelet 的重试，导致同一个 ContainerName （pod yaml）在宿主上存在多个实际的 Container，checkpoint 会存在同一个 Container 的多条记录，造成 cpu 资源泄露。Update CPUManager stored state semantics #84462 升级 checkpoint v1 到 v2 ，数据结构改成两个 map，外层 PodUID，内层 ContainerName（yaml 内）解决了上述问题，同时也引入了新的问题：在删除 Container 后由于清理绑核信息和创建新的 Container 是并行的，会导致刚创建的 Container 的绑核信息被误删除； Fix exclusive CPU allocations being deleted at container restart #90377 修复了 1 引入的问题，删除了 PostStopContainer 中对已删除的 Container 绑核信息的回收的逻辑，导致 Pod 删除和 Container 删除时不再进行绑核信息的回收，而是完全通过另外一个单独的 worker 定时的触发 reconcileState 进行清理； v1.22 对 kubelet 进行了重构，重构完之后 activePods 方法返回的结果不再包含处于 admission 阶段的 Pod，导致在 reconcileState 时会认为处于 admission 阶段的 Pod 的绑核信息是需要回收的，从而引入一个 Guaranteed Pod 存在多个 Container 时，在后续 Container admit 分配绑核信息时有可能会因为 2 中提到的 worker 的清理机制把这个 Pod 已分配的绑核信息给清理了； 重构 kubelet 的作者认为 activePods 就是不应该返回处于 admission 阶段的 Pod，于是另外一个作者提了这个 pr Do not clear state of pods pending admission for CPU/Memory/Device manager #103979 修复了 3 引入的问题，添加了 pendingAdmissionPod 字段，在每次 admit 时为其复制，但并没有清理这个属性的地方，导致了最开始问题四的出现； v1.22 重构 kubelet 的作者修改了实现 kubelet: Admission must exclude completed pods and avoid races #104577，让 activePods 返回了和重构之前一样的结果，即包含处于 admission 阶段的 Pod，同样修复了 3 中的问题，而且是没有副作用的，但并没有对 3 进行回滚； 中间也有人提了 pr 去修复这个问题，但因为 pr 里面没有对应的单测，一段时间后没有合就直接关闭了。最终结果就是直到现在 v1.28，问题四依然存在于 v1.22 之后的所有版本中… 引申 按目前的实现看，绑核的服务是不是就一定不会受到其他服务的影响呢？ 提示：按照最开始问题一，二的顺序思考，首先已经给 Podb 分配了全部的 12c，又为 Podg 分配了 6c，这中间是否存在一段时间，Podb 和 Podg 的 cpuset 中存在相同的核呢？是先回收一部分 Podb 使用的核之后再分配给 Podg 使用？还是直接分配给 Podg 一些核再从 Podb 的 cpuset 里面扣除？关键在搞清楚非 Guaranteed Pod 的 cpuset 信息是何时，又是如何更新的。 ","date":"2023-09-14","objectID":"https://www.likakuli.com/posts/cpusetbug1/:2:5","tags":["kubernetes"],"title":"Kubernetes 陈年老 bug - 绑核","uri":"https://www.likakuli.com/posts/cpusetbug1/"},{"categories":["问题排查"],"content":"high QPS for configmap GET requests in kube-apiserver - 3","date":"2023-08-26","objectID":"https://www.likakuli.com/posts/cachebasedmanager3/","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 3","uri":"https://www.likakuli.com/posts/cachebasedmanager3/"},{"categories":["问题排查"],"content":"背景 线上 k8s 集群 kube-apiserver 的 ConfigMap Get 操作 QPS 较高，且同时间段 Etcd 中 ConfigMap 资源的 Get 操作 QPS 也较高，看日志多数请求的发起方是 kubelet。对应 k8s v1.22.13 版本代码，同时在 v1.28.0 测试现象相同。kube-apiserver 日志大致如下： 2023-08-23T08:55:54.331196195Z stderr F I0823 08:55:54.330840 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.926865ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"36cfcbe3-d76a-4a4d-b251-47cc2df060cb\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.269527ms\" resp=200 2023-08-23T08:57:09.333913507Z stderr F I0823 08:57:09.333470 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.810334ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"563bd337-df29-4342-afd0-9ca6e0632f0f\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.177012ms\" resp=200 2023-08-23T08:58:14.338971779Z stderr F I0823 08:58:14.338630 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.563356ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"45350dc7-7a4b-43f1-8972-3b8053578234\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"929.214µs\" resp=200 由来 定位此问题的过程中花了一定的时间，同时也纠正了一些有关 kubelet 内 Pod 处理的错误理解。本篇旨在描述上述现象产生的原因及潜在问题，同时也希望能帮助大家更进一步的理解 kubelet 对 Pod 的处理逻辑。 由于涉及到的逻辑较多，因此将拆分成三篇来写： ConfigMap Get 请求的来源？ 为什么 QPS 高？为什么没有走 kube-apiserver 缓存？ 问题如何解决？ 本篇主要介绍问题如何解决。 回顾 前两篇内容总结对应这个图，分三块：syncPod、dswp（disiredStateOfWorldPolulator）、volumemanager。 接下来描述一下整个流程： kubelet 主 goroutine 启动定时器（图左上）每 1s 尝试从 queue 里面获取需要同步的 pod，启动一个专用 goroutine （图左下1）负责 dswp populatorLoop 每 100ms 一次的定时执行，启动一个专用 goroutine （图左下2）负责 volumemanager reconciler 每 100ms 一次的定时执行，所述三者完全并行； 新 pod 创建后 kubelet 为其分配自己专用的 podworker goroutine，podworker 启动后进入无限循环直到 pod 被删后 kubelet 才会去清理这个 worker，worker 收到有新 pod 需要创建的请求后会去执行 syncPod （图右上）操作，这里主要关注三个动作： RegisterPod：他会最终标记本地 ConfigMap 缓存无效，记为 t1 时刻； WaitForAttachAndMount：他会把 pod 从 dsp 的 processedPods 数据中踢出，记为 t2 时刻； syncPod 执行完之后会执行 completeWork （图深黄色部分），会重新把 pod 入队列，并基于 --sync-frequncy 设置一个有效时间，时间到了之后才能被 1 从 queue 里面获取到； dswp populatorLoop 触发后会判断 pod 是否处理过，处理过直接 return，没有处理过则把 pod 和 volume 信息添加到 dsw（disiredStateOfWorld） 的 volumesToMount 中，并设置 remountRequired 为 true； volumemanager reconcile 触发后先判断是否未挂载或者需要重新挂载，需要的话会获取要挂载的 ConfigMap 信息，缓存无效则直接去 apiserver 最终请求 etcd 最新数据，有效但超时则去 apiserver 获取但不走 etcd 直接返回 apiserver 缓存结果，未超时则直接返回自己本地缓存的数据不再请求 apiserver，最后用获取到的信息进行 mount 操作，成功后设置 remountRequired 为 false； 1 中的定时器在一段时间（3 中 enqueue 入队列计算的时间）后再次走到 pod worker 走一遍 2，3 的逻辑，同时也会触发 4，5 的再次执行； 分析 有一个问题是比较明显的：从日志看最终都是穿透到 etcd 去了。为什么这里要区分开针对第一次请求要去 etcd 获取，而真多后续因缓存过期导致的请求就可以直接从 apiserver cache 返回了？注释里面写了特意这么设计，也即是说设计如此。本人最初想法是不要区别对待两种请求，都直接从 apiserver cache 返回即可，因为另一种 watchBasedManager 是基于 watch 实现的，reflector 在第一次执行时就是直接从 apiserver 缓存获取数据而不会走到 etcd。这样的话就可以保持两者一致的行为：本地缓存失效后始终从 apiserver 返回，不再穿透到 etcd。但经过与社区讨论，他们觉得还是要保持当前实现，即第一次还是要从 apiserver 获取并穿透到 etcd，目的是获取最新的数据。 虽然统一不走 etcd 的方案被驳回，但确实方案基本只是降低了 etcd 请求，并没有缓解 apiserver 的请求，因为每次都还是会标记缓存失效再去请求 aspiserver 的。 社区也给出了另外一种解法，即不要在 AddReference 时每次都标记缓存失效，而是只有在第一次时标记缓存无效。这明显是要比之前的方案好，因为这样不止能降低 etcd 的请求，还能降低 apiserver 的请求，更大限度的利用本地缓存。 针对规模较小的集群，效果 apiserver qps 优化并不明显，针对大规模集群的话则比较明显。因为 kube-controller-manager 会根据节点数量通过为每个节点设置 annotation：node.alpha.kubernetes.io/ttl 来控制每个节点本地缓存有效期，规则如下 ttlBoundaries = []ttlBoundary{ {sizeMin: 0, sizeMax: 100, ttlSeconds: 0}, {sizeMin: 90, sizeMax: 500, ttlSeconds: 15}, {sizeMin: 450, sizeMax: 1000, ttlSeconds: 30}, {sizeMin: 900, sizeMax: 2000, ttlSeconds: 60}, {sizeMin: 1800, sizeMax: 10000, ttlSeconds: 300}, {sizeMin: 9000, sizeMax: math.MaxInt32, ttlSeconds: 600}, } size 对应节点数量，ttlSeconds 代表 ttl 时长，0 代表使用默认值，默认值在 kubelet 里面配置的 1min。 效果 ","date":"2023-08-26","objectID":"https://www.likakuli.com/posts/cachebasedmanager3/:0:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 3","uri":"https://www.likakuli.com/posts/cachebasedmanager3/"},{"categories":["问题排查"],"content":"high QPS for configmap GET requests in kube-apiserver - 2","date":"2023-08-25","objectID":"https://www.likakuli.com/posts/cachebasedmanager2/","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 2","uri":"https://www.likakuli.com/posts/cachebasedmanager2/"},{"categories":["问题排查"],"content":"背景 线上 k8s 集群 kube-apiserver 的 ConfigMap Get 操作 QPS 较高，且同时间段 Etcd 中 ConfigMap 资源的 Get 操作 QPS 也较高，看日志多数请求的发起方是 kubelet。对应 k8s v1.22.13 版本代码，同时在 v1.28.0 测试现象相同。kube-apiserver 日志大致如下： 2023-08-23T08:55:54.331196195Z stderr F I0823 08:55:54.330840 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.926865ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"36cfcbe3-d76a-4a4d-b251-47cc2df060cb\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.269527ms\" resp=200 2023-08-23T08:57:09.333913507Z stderr F I0823 08:57:09.333470 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.810334ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"563bd337-df29-4342-afd0-9ca6e0632f0f\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.177012ms\" resp=200 2023-08-23T08:58:14.338971779Z stderr F I0823 08:58:14.338630 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.563356ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"45350dc7-7a4b-43f1-8972-3b8053578234\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"929.214µs\" resp=200 由来 定位此问题的过程中花了一定的时间，同时也纠正了一些有关 kubelet 内 Pod 处理的错误理解。本篇旨在描述上述现象产生的原因及潜在问题，同时也希望能帮助大家更进一步的理解 kubelet 对 Pod 的处理逻辑。 由于涉及到的逻辑较多，因此将拆分成三篇来写： ConfigMap Get 请求的来源？ 为什么 QPS 高？为什么没有走 kube-apiserver 缓存？ 问题如何解决？ 本篇主要介绍 QPS 为什么高，为什么请求会穿透到 Etcd。 回顾 在第一篇中已经介绍了 ConfigMap Get 请求的逻辑： 每有 Pod 需要 Sync 时，会触发 syncPod，在 syncPod 时会调用 configMapManager.RegisterPod 标记缓存无效，reconciler goroutine 每 100ms 执行一次 mountAttachVolumes 去挂载 Pod 所有的 Volume，发现被标记失效就会重新去 apiserver 获取。 至于具体哪些 Pod 需要去挂载哪些 Volume，有另外两个数据结构存储相关信息：DesiredStateOfWorld，ActualStateOfWorld。 为什么 QPS 高？ ","date":"2023-08-25","objectID":"https://www.likakuli.com/posts/cachebasedmanager2/:0:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 2","uri":"https://www.likakuli.com/posts/cachebasedmanager2/"},{"categories":["问题排查"],"content":"QPS 估算 可以粗略计算每台机器上的 kubelet 对 ConfigMap Get 的贡献，我们可以先统一前提，就是没有外部 Pod 的增删改请求，也没有 Container Runtime 侧的变化，单纯是靠 syncCh 来触发的 syncPod，有如下数据： sync tiker 的周期是 1s； reconciler 的周期为 100ms； --sync--frequency 默认配置是 1m； // pkg/kubelet/pod_workers.go // completeWork requeues on error or the next sync interval and then immediately executes any pending // work. func (p *podWorkers) completeWork(pod *v1.Pod, phaseTransition bool, syncErr error) { // Requeue the last update if the last sync returned error. switch { ... case syncErr == nil: // No error; requeue at the regular resync interval. p.workQueue.Enqueue(pod.UID, wait.Jitter(p.resyncInterval, workerResyncIntervalJitterFactor)) ... } p.completeWorkQueueNext(pod.UID) } 虽然 reconciler 周期 100ms，但只有在缓存标记为失效或者过期时才会去 apiserver 拿，并且我们看到的日志都是直接穿透到 Etcd 的，也就是说针对我们的情况，都是由缓存被标记为失效引起的。 resyncInterval 对应的是参数 --sync-frequency。k8s 里面经常是在配置里面设置一个同步的间隔，而实际的间隔往往是一个介于配置的间隔和此间隔额外加上一个随机值之间的随机值，这里也是同样的，实际每次 Enqueue 的时间是当前时间加上介于 1m 与 1m30s 之间的一个随机值。虽然 sync ticker 周期是 1s，也只是每次尝试去看队列里面有没有需要同步的 Pod，他的判断依据就是拿当前时间与 Pod Enqueue 时计算的时间作比较，如果已近过了 Enqueue 带的时间，就会触发 syncPod。 如果节点上只有一个 Pod 挂载了一个 ConfigMap，那么理论上他贡献的 QPS 介于 1/90 到 1/60 之间。这么一看的话，QPS 也不算高，假如一个有 90 个实例的 deployment 挂载一个 ConfigMap，如果每个 deployment 的所有实例都分布在不同的机器上，那么他对 QPS 的贡献介于 1 和 90 之间，如果有 100 个这样的 deployment，并且他们挂载的 ConfigMap 各不相同，那么整体的 QPS 的贡献介于 100 和 9000 之间。也就是说 9000 个 Pod 可能的贡献介于 100 和 9000之间，这个上下限差距很大。 为什么没有走 apiserver 缓存？ 分别就本地缓存和 apiserver 缓存来分析 ","date":"2023-08-25","objectID":"https://www.likakuli.com/posts/cachebasedmanager2/:1:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 2","uri":"https://www.likakuli.com/posts/cachebasedmanager2/"},{"categories":["问题排查"],"content":"有没有走本地缓存？ 不知道你有没有这个疑问，既然每次 syncPod 都会设置缓存失效，那本地缓存还有什么用？ 确实从当前的实现结果看，本地貌似缓存没有用，但实际还是有一些作用，某些情况下是可以降低 apiserver QPS 的。考虑如下情况 同一个 deployment 的不同实例在同一台宿主上； 不同的 deployment 挂载了相同的 ConfigMap； 会导致出现挂载相同 ConfigMap 的 Pod 出现在同一台机器上，而我们知道 syncPod 是有多个 goroutine 并行执行的，以 Pod 为粒度，而 ConfigMap 的缓存的 Key 的是其 namespace，name 组成的结构，也就以是 ConfigMap 为粒度，且最终 reconciler 执行 Get 的操作是串行的，就可能会出现相同的 CM 被标记失效，但获取的时候由于串行执行，前面一个已经获取过了，后面的就不会再走到去 apiserver 获取 CM 的逻辑，而是直接利用缓存中的 CM 返回。 因此，在当前的机制和实现结果下，QPS 高的主要原因在于使用 CM 作为 Pod Volume 的数量过多导致，且 Pod 调度的越分散，CM 重合度越低，QPS 会越高。 ","date":"2023-08-25","objectID":"https://www.likakuli.com/posts/cachebasedmanager2/:2:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 2","uri":"https://www.likakuli.com/posts/cachebasedmanager2/"},{"categories":["问题排查"],"content":"为什么没有走 apiserver 缓存？ 按上面的分析，默认每 1m 到 1m30s s 触发一次 syncPod，而缓存过期时间为 1m（来自 node annotation node.alpha.kubernetes.io/ttl，会随着集群规模变化，控制逻辑在 kube-controller-manager 里面，node 上如果没有这个设置的话，会采用默认的最小值，即 1m），理论上就应该出现由于缓存过期导致的再次去 apiserver 获取时直接从 apiserver cache 里面拿的请求，即会出现带着 resourceversion=0 的请求，但实际没有，哪里的问题呢？ 这里就涉及到 remountRequired 了，可以看到在 MountVolume 成功后会设置这个参数为 false，代表不再需要挂载了已经，如果没有其他地方改这个参数的话，理论上挂载成功后就不会再请求 apiserver 了，与实际情况不符，那就是还会有其他地方去改这个值，设置为 true，代码如下 // pkg/kubelet/volumemanager/populator/desired_state_of_world_populator.go func (asw *actualStateOfWorld) MarkRemountRequired( podName volumetypes.UniquePodName) { asw.Lock() defer asw.Unlock() for volumeName, volumeObj := range asw.attachedVolumes { if podObj, podExists := volumeObj.mountedPods[podName]; podExists { volumePlugin, err := asw.volumePluginMgr.FindPluginBySpec(podObj.volumeSpec) if err != nil || volumePlugin == nil { // Log and continue processing klog.ErrorS(nil, \"MarkRemountRequired failed to FindPluginBySpec for volume\", \"uniquePodName\", podObj.podName, \"podUID\", podObj.podUID, \"volumeName\", volumeName, \"volumeSpecName\", podObj.volumeSpec.Name()) continue } if volumePlugin.RequiresRemount(podObj.volumeSpec) { podObj.remountRequired = true asw.attachedVolumes[volumeName].mountedPods[podName] = podObj } } } } 继续沿着其调用链路往上找，会发现另外一个类似上篇中 reconciler 的一个 goroutine 在定时的执行以下方法，就在这个方法里面会触发最终 MarkRemountRequired 的调用。 // pkg/kubelet/volumemanager/populator/desired_state_of_world_populator.go func (dswp *desiredStateOfWorldPopulator) Run(sourcesReady config.SourcesReady, stopCh \u003c-chan struct{}) { // Wait for the completion of a loop that started after sources are all ready, then set hasAddedPods accordingly klog.InfoS(\"Desired state populator starts to run\") wait.PollUntil(dswp.loopSleepDuration, func() (bool, error) { done := sourcesReady.AllReady() dswp.populatorLoop() return done, nil }, stopCh) dswp.hasAddedPodsLock.Lock() dswp.hasAddedPods = true dswp.hasAddedPodsLock.Unlock() wait.Until(dswp.populatorLoop, dswp.loopSleepDuration, stopCh) } func (dswp *desiredStateOfWorldPopulator) populatorLoop() { dswp.findAndAddNewPods() ... } // Iterate through all pods and add to desired state of world if they don't // exist but should func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() { ... for _, pod := range dswp.podManager.GetPods() { if dswp.podStateProvider.ShouldPodContainersBeTerminating(pod.UID) { // Do not (re)add volumes for pods that can't also be starting containers continue } dswp.processPodVolumes(pod, mountedVolumesForPod, processedVolumesForFSResize) } } // processPodVolumes processes the volumes in the given pod and adds them to the // desired state of the world. func (dswp *desiredStateOfWorldPopulator) processPodVolumes( pod *v1.Pod, mountedVolumesForPod map[volumetypes.UniquePodName]map[string]cache.MountedVolume, processedVolumesForFSResize sets.String) { ... uniquePodName := util.GetUniquePodName(pod) if dswp.podPreviouslyProcessed(uniquePodName) { return } allVolumesAdded := true ... // some of the volume additions may have failed, should not mark this pod as fully processed if allVolumesAdded { dswp.markPodProcessed(uniquePodName) // New pod has been synced. Re-mount all volumes that need it // (e.g. DownwardAPI) dswp.actualStateOfWorld.MarkRemountRequired(uniquePodName) ... } else if dswp.podHasBeenSeenOnce(uniquePodName) { ... } } func (dswp *desiredStateOfWorldPopulator) podPreviouslyProcessed( podName volumetypes.UniquePodName) bool { dswp.pods.RLock() defer dswp.pods.RUnlock() return dswp.pods.processedPods[podName] } loopSleepDuration 写死的 100ms，即 100ms 执行一次 populatorLoop。简化为如下图 是否需要设置 remountRequired 关键还是要看 dswp.podPreviouslyProcessed，继续看在哪里会对此进行操作 func (dswp *desiredStateOfWorldPopulator) markPodProcessingFailed( podName volumetypes.UniquePodName) { dswp.pods.Lock() dswp.pods.processedPods[podName] = false dswp.pods.Unlock() } func (dswp *desiredStateOfWorldPopulator) ReprocessPod( podName volumetypes.UniquePodName) { dswp.markPodProcessingFail","date":"2023-08-25","objectID":"https://www.likakuli.com/posts/cachebasedmanager2/:3:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 2","uri":"https://www.likakuli.com/posts/cachebasedmanager2/"},{"categories":["问题排查"],"content":"high QPS for configmap GET requests in kube-apiserver - 1","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"背景 线上 k8s 集群 kube-apiserver 的 ConfigMap Get 操作 QPS 较高，且同时间段 Etcd 中 ConfigMap 资源的 Get 操作 QPS 也较高，看日志多数请求的发起方是 kubelet。对应 k8s v1.22.13 版本代码，同时在 v1.28.0 测试现象相同。kube-apiserver 日志大致如下： 2023-08-23T08:55:54.331196195Z stderr F I0823 08:55:54.330840 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.926865ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"36cfcbe3-d76a-4a4d-b251-47cc2df060cb\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.269527ms\" resp=200 2023-08-23T08:57:09.333913507Z stderr F I0823 08:57:09.333470 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.810334ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"563bd337-df29-4342-afd0-9ca6e0632f0f\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"1.177012ms\" resp=200 2023-08-23T08:58:14.338971779Z stderr F I0823 08:58:14.338630 1 httplog.go:132] \"HTTP\" verb=\"GET\" URI=\"/api/v1/namespaces/default/configmaps/nginx-cfgmap\" latency=\"1.563356ms\" userAgent=\"kubelet/v1.28.0 (linux/amd64) kubernetes/855e7c4\" audit-ID=\"45350dc7-7a4b-43f1-8972-3b8053578234\" srcIP=\"192.168.228.2:59052\" apf_pl=\"system\" apf_fs=\"system-nodes\" apf_iseats=1 apf_fseats=0 apf_additionalLatency=\"0s\" apf_execution_time=\"929.214µs\" resp=200 由来 定位此问题的过程中花了一定的时间，同时也纠正了一些有关 kubelet 内 Pod 处理的错误理解。本篇旨在描述上述现象产生的原因及潜在问题，同时也希望能帮助大家更进一步的理解 kubelet 对 Pod 的处理逻辑。 由于涉及到的逻辑较多，因此将拆分成三篇来写： ConfigMap Get 请求的来源？ 为什么 QPS 高？为什么没有走 kube-apiserver 缓存？ 问题如何解决？ 本篇主要介绍 ConfigMap Get 请求的整个调用链路 追踪溯源 在找到具体原因之前，大概有个排查方向，因为日志中显示的 ConfigMap 都是挂载到 Pod 中作为 Volume 使用的，问题的根源大概率与此有关，因此先从这个方向来。 采取自下而上方式沿着函数调用链路看代码，找源头，首先需要找到最后调用 ConfigMap Get 的地方，如果熟悉 client-go 的话，很容易在 /pkg/kubelet 目录下根据关键字 ConfigMaps( 搜索到结果，在 /pkg/kubelet/configmap/configmap_manager.go 文件中，里面有三种类型的 manager，分别对用不同类型的参数设置，因为 kubelet 配置的 configMapAndSecretChangeDetectionStrategy: Cache，因此看 NewCachingConfigMapManager 即可，最终返回 cacheBasedManager 就是管理 ConfigMap 的。 // NewCachingConfigMapManager creates a manager that keeps a cache of all configmaps // necessary for registered pods. // It implement the following logic: // - whenever a pod is create or updated, the cached versions of all configmaps // are invalidated // - every GetObject() call tries to fetch the value from local cache; if it is // not there, invalidated or too old, we fetch it from apiserver and refresh the // value in cache; otherwise it is just fetched from cache func NewCachingConfigMapManager(kubeClient clientset.Interface, getTTL manager.GetObjectTTLFunc) Manager { getConfigMap := func(namespace, name string, opts metav1.GetOptions) (runtime.Object, error) { return kubeClient.CoreV1().ConfigMaps(namespace).Get(context.TODO(), name, opts) } configMapStore := manager.NewObjectStore(getConfigMap, clock.RealClock{}, getTTL, defaultTTL) return \u0026configMapManager{ manager: manager.NewCacheBasedManager(configMapStore, getConfigMapNames), } } NewCachingConfigMapManager 创建一个管理器，用于管理缓存的 ConfigMap 数据。 当创建或更新 Pod 时，缓存中的所有 ConfigMap 数据都会被标记为无效。 在每次调用 GetObject() 方法时，首先尝试从本地缓存中获取数据。如果数据不存在、已被标记为无效或已过期，会从 apiserver 获取最新数据，并刷新缓存中的数据。如果数据存在于缓存中且有效，则直接从缓存中获取。 注释写的很清楚，每次创建或者更新 Pod 时，缓存中 Pod 对应的 ConfigMap 会被标记为无效，等 GetObject 被调用时，发现本地缓存中对应的 ConfigMap 已被标记为无效，就去 apiserver 获取。一种典型的协作式逻辑，接下来围绕两个调用链来看分析： GetObject 的调用链，缓存无效标记链。 ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:0:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"GetObject 调用链 ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:1:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"投石问路 首先看下 GetObject 干了什么，核心逻辑还是在 Get 中，下面代码中的 klog 日志输出是为了方便调试自己加上去的，在官方代码中没有的。 // pkg/kubelet/util/manager/cache_based_manager.go func (c *cacheBasedManager) GetObject(namespace, name string) (runtime.Object, error) { return c.objectStore.Get(namespace, name) } func (s *objectStore) Get(namespace, name string) (runtime.Object, error) { key := objectKey{namespace: namespace, name: name} data := func() *objectData { s.lock.Lock() defer s.lock.Unlock() item, exists := s.items[key] if !exists { return nil } if item.data == nil { klog.V(5).InfoS(\"empty item data\", \"key\", key.namespace+\"/\"+key.name) item.data = \u0026objectData{} } return item.data }() if data == nil { return nil, fmt.Errorf(\"object %q/%q not registered\", namespace, name) } // After updating data in objectStore, lock the data, fetch object if // needed and return data. data.Lock() defer data.Unlock() if data.err != nil || !s.isObjectFresh(data) { opts := metav1.GetOptions{} if data.object != nil \u0026\u0026 data.err == nil { // This is just a periodic refresh of an object we successfully fetched previously. // In this case, server data from apiserver cache to reduce the load on both // etcd and apiserver (the cache is eventually consistent). util.FromApiserverCache(\u0026opts) klog.V(5).InfoS(\"set resourceversion 0\", \"key\", key.namespace+\"/\"+key.name) } object, err := s.getObject(namespace, name, opts) if err != nil \u0026\u0026 !apierrors.IsNotFound(err) \u0026\u0026 data.object == nil \u0026\u0026 data.err == nil { // Couldn't fetch the latest object, but there is no cached data to return. // Return the fetch result instead. return object, err } if (err == nil \u0026\u0026 !isObjectOlder(object, data.object)) || apierrors.IsNotFound(err) { // If the fetch succeeded with a newer version of the object, or if the // object could not be found in the apiserver, update the cached data to // reflect the current status. data.object = object data.err = err data.lastUpdateTime = s.clock.Now() } } else { klog.V(5).InfoS(\"return from cache directly\", \"key\", key.namespace+\"/\"+key.name) } return data.object, data.err } objectStore 用来存储所用到的所有 ConfigMap，key 是一个由 ConfigMap 的 namespace 和 name 组成的结构体，value 是 objectStoreItem 结构体，其内维护了对应 ConfigMap 的引用计数、ConfigMap 自身、最后更新时间等属性。 // pkg/kubelet/util/manager/cache_based_manager.go type objectKey struct { namespace string name string uid types.UID } // objectStoreItems is a single item stored in objectStore. type objectStoreItem struct { refCount int data *objectData } type objectData struct { sync.Mutex object runtime.Object err error lastUpdateTime time.Time } // objectStore is a local cache of objects. type objectStore struct { getObject GetObjectFunc clock clock.Clock lock sync.Mutex items map[objectKey]*objectStoreItem defaultTTL time.Duration getTTL GetObjectTTLFunc } ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:1:1","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"顺藤摸瓜 沿着 GetObject 调用链路向上找，直到 configMapVolumeMounter 结构，在他的 SetUpAt 方法中有一行代码来获取 ConfigMap。 configMap, err := b.getConfigMap(b.pod.Namespace, b.source.Name) 很明显，结构体名字就是和挂载有关的。继续往上找看又是谁在调用 SetUpAt ，最终在 reconiler 结构的 mountAttachVolumes 方法中找到了最终的触发者 operationExecutor.MountVolume，忽略了部分不相关代码，如下 // pkg/kubelet/volumemanager/reconciler/reconciler.go func (rc *reconciler) mountAttachVolumes() { // Ensure volumes that should be attached/mounted are attached/mounted. volumesToMount := rc.desiredStateOfWorld.GetVolumesToMount() for _, volumeToMount := range volumesToMount { volMounted, devicePath, err := rc.actualStateOfWorld.PodExistsInVolume(volumeToMount.PodName, volumeToMount.VolumeName) volumeToMount.DevicePath = devicePath if cache.IsVolumeNotAttachedError(err) { ... } else if !volMounted || cache.IsRemountRequiredError(err) { // Volume is not mounted, or is already mounted, but requires remounting remountingLogStr := \"\" isRemount := cache.IsRemountRequiredError(err) if isRemount { remountingLogStr = \"Volume is already mounted to pod, but remount was requested.\" } klog.V(4).InfoS(volumeToMount.GenerateMsgDetailed(\"Starting operationExecutor.MountVolume\", remountingLogStr)) err := rc.operationExecutor.MountVolume( rc.waitForAttachTimeout, volumeToMount.VolumeToMount, rc.actualStateOfWorld, isRemount) if err != nil \u0026\u0026 !isExpectedError(err) { // Ignore nestedpendingoperations.IsAlreadyExists and exponentialbackoff.IsExponentialBackoff errors, they are expected. // Log all other errors. klog.ErrorS(err, volumeToMount.GenerateErrorDetailed(fmt.Sprintf(\"operationExecutor.MountVolume failed (controllerAttachDetachEnabled %v)\", rc.controllerAttachDetachEnabled), err).Error()) } if err == nil { if remountingLogStr == \"\" { klog.V(1).InfoS(volumeToMount.GenerateMsgDetailed(\"operationExecutor.MountVolume started\", remountingLogStr)) } else { klog.V(5).InfoS(volumeToMount.GenerateMsgDetailed(\"operationExecutor.MountVolume started\", remountingLogStr)) } } } else if cache.IsFSResizeRequiredError(err) \u0026\u0026 utilfeature.DefaultFeatureGate.Enabled(features.ExpandInUsePersistentVolumes) { ... } } } 逻辑大致如下： 首先从 desiredStateOfWorld 获取需要挂载的所有的卷（ConfigMap） 遍历每个卷，看 Pod 是否存在于 actualStateOfWorld 中保存的已经挂载了卷的 Pod 列表中 如果 2 返回不存在（尚未挂载），或者需要重新挂载，则会调用 operationExecutor.MountVolume 进行挂载 之后就会一步步触发最终 GetObject 去获取要挂载的 ConfigMap，继续往上找 mountAttachVolumes 调用方，如下 // pkg/kubelet/volumemanager/reconciler/reconciler.go func (rc *reconciler) Run(stopCh \u003c-chan struct{}) { wait.Until(rc.reconciliationLoopFunc(), rc.loopSleepDuration, stopCh) } func (rc *reconciler) reconciliationLoopFunc() func() { return func() { rc.reconcile() ... } } func (rc *reconciler) reconcile() { ... // Next we mount required volumes. This function could also trigger // attach if kubelet is responsible for attaching volumes. // If underlying PVC was resized while in-use then this function also handles volume // resizing. rc.mountAttachVolumes() ... } 至此找到了最上层触发 GetObject 调用的调用者 reconciler，他在 kubelet 启动时启动，每间隔 loopSleepDuration （代码中写死的 100ms ）执行一次，每次执行时都会去调用 mountAttachVolumes 最终调用 GetObject。 上述流程总结如下图 ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:1:2","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"缓存无效标记链 ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:2:0","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"蛛丝马迹 思路同上，需要先找到标记是什么，还是得回到 GetObject 中（开头代码注释中写了）。因为从 kube-apiserver 日志中看到的现象是请求直接穿透到了 Etcd，再结合上面 Get 方法，可以知道是命中了 item.data == nil 之后执行 item.data = \u0026objectData{}，因此才没有命中 data.object != nil 的逻辑，也就没有给请求设置 ResourceVersion: 0 的参数，最终导致请求穿透到了 Etcd。在每次请求结束后，会将 data.object 赋值为刚获取到的 ConfigMap。又因为日志中已知出现请求，基本可以得到标记无效就是靠设置 item.data = nil 实现的。 代码中搜索 .data = nil 会发现只有一条匹配的 func (s *objectStore) AddReference(namespace, name string) { key := objectKey{namespace: namespace, name: name} // AddReference is called from RegisterPod, thus it needs to be efficient. // Thus Add() is only increasing refCount and generation of a given object. // Then Get() is responsible for fetching if needed. s.lock.Lock() defer s.lock.Unlock() item, exists := s.items[key] if !exists { item = \u0026objectStoreItem{ refCount: 0, data: \u0026objectData{}, } s.items[key] = item } item.refCount++ // This will trigger fetch on the next Get() operation. item.data = nil } 注释写的也很清晰，设置 item.data = nil 会触发下次执行 Get 操作时去访问 apiserver。标记找到了，接下来就是去看下整个调用链了。 ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:2:1","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["问题排查"],"content":"抽丝剥茧 沿着 AddReference 调用链路往上找，发现最终调用方在 syncPod 中，如下 func (kl *Kubelet) syncPod(ctx context.Context, updateType kubetypes.SyncPodType, pod, mirrorPod *v1.Pod, podStatus *kubecontainer.PodStatus) (isTerminal bool, err error) { ... // ensure the kubelet knows about referenced secrets or configmaps used by the pod if !kl.podWorkers.IsPodTerminationRequested(pod.UID) { if kl.secretManager != nil { kl.secretManager.RegisterPod(pod) } if kl.configMapManager != nil { kl.configMapManager.RegisterPod(pod) } } ... } syncPod 调用 configMapManager.RegisterPod 最终触发 AddReference 设置 item.data 为 nil，相当于标记缓存无效。到这里熟悉 kubelet 主流程的话应该就比较清楚是怎么回事了，Kubelet 会为每个 Pod 启动一个单独的 goroutine PodWorker 负责 Pod 生命周期的管理，这里盗用一个网图 有四个 chan： configCh 对应从外界获取到的 Pod 信息，有三种来源 File，Http，Apiserver； syncCh 对应一个 ticker，每秒触发一次，时间写死在代码中； housekeepingCh 也对应一个 ticker，每 2 秒触发一次，时间同样写死在代码中； plegCh 对应从 container runtime 获取到的 container 的真实信息，有个对应的结构，每秒从 container runtime 获取所有 container 信息，经过处理之后发到 plegCh； 任意 chan 内有数据了之后都会最终触发 syncPod。 总结 至此整个流程已经清楚了，一句话概括就是每有 Pod 需要 Sync 时，会触发 syncPod，在 syncPod 时会调用 configMapManager.RegisterPod 标记缓存无效，reconciler goroutine 每 100ms 执行一次 mountAttachVolumes 去挂载 Pod 所有的 Volume，发现被标记失效就会重新去 apiserver 获取。 至于具体哪些 Pod 需要去挂载哪些 Volume，有另外两个数据结构存储相关信息：DesiredStateOfWorld，ActualStateOfWorld。上文在执行 mountAttachVolumes 的时候出现过，将会在下一篇中分析为什么 QPS 会高，为什么没有走 apiserver cache 的时候进行更详细的分析。 ","date":"2023-08-23","objectID":"https://www.likakuli.com/posts/cachebasedmanager1/:2:2","tags":["kubernetes"],"title":"high QPS for configmap GET requests in kube-apiserver - 1","uri":"https://www.likakuli.com/posts/cachebasedmanager1/"},{"categories":["开源项目介绍"],"content":"背景 容器平台的三个价值：稳定性、效率、成本，都离不开容量管理。容量管理是 Kubernetes 集群管理中非常重要的一部分。它可以确保系统中的资源得到合理的分配和使用，避免了因资源不足或浪费导致的系统运行异常或效率低下的问题。通过容量管理，可以更好地控制和优化系统资源的利用，保证 Kubernetes 集群的稳定性和可靠性。容量管理还可以帮助管理员对系统进行更好的规划和预测，避免因资源不足而需要紧急扩容的情况出现，从而提高了系统的可维护性和可靠性。 面临的问题 K8s 集群管理员或多或少会被如下问题所困扰： 不清楚当前集群资源使用或者剩余容量处于什么水位； 不清楚当前集群资源浪费情况如何； 不清楚当前集群资源碎片程度如何； 不清楚如何设置调度策略配置值以提升资源利用效率； … 资源是一个典型的可量化的指标，上述问题均可以做到可量化，我们缺少的只是一个好用的工具。 项目介绍 kluster-capacity 旨在通过模拟线上真实调度器的能力来解决上述问题，目前已经支持三个能力：容量评估，调度模拟，集群压缩。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:0:0","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"容量评估 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:1:0","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"介绍 随着集群中节点上新的 Pod 被调度，消耗的资源越来越多。监控集群中可用的资源非常重要，因为运维人员可以及时增加当前的资源，以免所有资源都耗尽。或者，采取不同的步骤来增加可用资源。 集群容量包括单个集群节点的容量。容量涵盖了 CPU、内存、磁盘空间和其他资源。 整体剩余可分配容量是一个估计值。目标是分析剩余可分配的资源并估计可用容量，即可以在集群中安排给定资源需求的 Pod 实例数量。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:1:1","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"增强 以下是对原集群容量的一些增强功能： 支持直接从集群中使用现有的 Pod 作为 Pod 模板。 支持针对不同的 Pod 模板进行批量模拟。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:1:2","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"运行 # 直接使用指定的 pod 模板 $ ./kluster-capacity ce --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --pods-from-template \u003cpath to pod templates\u003e # 使用集群中指定的 pod 作为模板 $ ./kluster-capacity ce --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --pods-from-cluster \u003cnamespace/name key of the pod\u003e 更多运行参数及功能，请执行如下命令： $ ./kluster-capacity ce --help ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:1:3","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"演示 假设集群运行有 4 个节点和 1 个主节点，每个节点有 2 个 CPU 和 4GB 内存。而每个 Pod 所需的资源为 150m CPU 和 100Mi 内存。 $ ./kluster-capacity ce --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --pods-from-template \u003cpath to pod templates\u003e --verbose Pod requirements: - cpu: 150m - memory: 100Mi The cluster can schedule 52 instance(s) of the pod. Termination reason: FailedScheduling: pod (small-pod-52) failed to fit in any node fit failure on node (kube-node-1): Insufficient cpu fit failure on node (kube-node-4): Insufficient cpu fit failure on node (kube-node-2): Insufficient cpu fit failure on node (kube-node-3): Insufficient cpu Pod distribution among nodes: - kube-node-1: 13 instance(s) - kube-node-4: 13 instance(s) - kube-node-2: 13 instance(s) - kube-node-3: 13 instance(s) 随着集群中运行的 pod 数量增加，再次运行分析时，可调度的 pod 数量也会减少。 $ ./kluster-capacity ce --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --pods-from-template \u003cpath to pod templates\u003e --verbose Pod requirements: - cpu: 150m - memory: 100Mi The cluster can schedule 46 instance(s) of the pod. Termination reason: FailedScheduling: pod (small-pod-46) failed to fit in any node fit failure on node (kube-node-1): Insufficient cpu fit failure on node (kube-node-4): Insufficient cpu fit failure on node (kube-node-2): Insufficient cpu fit failure on node (kube-node-3): Insufficient cpu Pod distribution among nodes: - kube-node-1: 11 instance(s) - kube-node-4: 12 instance(s) - kube-node-2: 11 instance(s) - kube-node-3: 12 instance(s) ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:1:4","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"输出格式 ce 命令有一个 --output (-o) 标志，可以将其输出格式化为 json 或 yaml。 $ ./kluster-capacity ce --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --pods-from-template \u003cpath to pod templates\u003e -o json|yaml ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:1:5","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"调度模拟 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:2:0","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"介绍 调度器模拟以当前集群中的所有 node、pod 等相关资源为输入，模拟从没有 pod 到创建并调度所有 pod 的过程。这可以用来计算集群压缩率比，以评估调度效果或衡量调度算法的质量。 与集群压缩相比，其结果更加激进和理想化。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:2:1","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"运行 ./kluster-capacity ss --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e 更多运行参数及功能，请执行如下命令： $ ./kluster-capacity ss --help 它支持两种终止条件：AllSucceed 和 AllScheduled。前者是指所有pod调度成功后程序结束，后者是指所有 pod 至少被调度一次后程序退出。默认值为 AllSucceed。可以使用 --exit-condition 标志设置退出条件。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:2:2","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"演示 假设集群运行有 4 个节点和 1 个主节点，每个节点有 2 个 CPU 和 4GB 内存。有 40 个资源需求是 100m CPU 和 200Mi 内存的 Pod 需要被调度。 如果调度器使用 LeastAllocated 策略，调度结果可能如下所示： $ ./kluster-capacity ss --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e Termination reason: AllSucceed: 40 pod(s) have been scheduled successfully. Pod distribution among nodes: - kube-node-1: 10 instance(s) - kube-node-2: 10 instance(s) - kube-node-3: 10 instance(s) - kube-node-4: 10 instance(s) 如果调整调度器使用 MostAllocated 策略，调度结果可能如下所示： $ ./kluster-capacity ss --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e Termination reason: AllSucceed: 40 pod(s) have been scheduled successfully. Pod distribution among nodes: - kube-node-1: 20 instance(s) - kube-node-2: 20 instance(s) 可以分析上面的调度结果来评估调度策略的有效性和集群容量压缩比。例如，上面的结果表示集群压缩比为2，这意味着在理想情况下有50%的资源浪费。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:2:3","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"集群压缩 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:3:0","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"介绍 集群压缩以集群的当前状态，包括所有 node、pod 和其他相关资源作为输入，模拟通过移除节点来压缩集群的过程。它可用于计算集群的压缩比，这是衡量资源利用效率的指标。 与模拟调度相比，集群压缩的结果通常更显示，可操作性更强。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:3:1","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"运行 ./kluster-capacity cc --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --verbose 更多运行参数及功能，请执行如下命令： $ ./kluster-capacity cc --help ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:3:2","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"演示 假设集群运行有 4 个节点和 1 个主节点，每个节点有 2 个 CPU 和 4GB 内存。运行有 40 个资源需求是 100m CPU 和 200Mi 内存的 Pod。 ./kluster-capacity cc --kubeconfig \u003cpath to kubeconfig\u003e --schedulerconfig= \u003cpath to schedulerconfig\u003e --verbose 2 node(s) in the cluster can be scaled down. Termination reason: FailedSelectNode: could not find a node that satisfies the condition, 1 master node(s); 2 node(s) can't be scale down because of insufficient resource in other nodes; nodes selected to be scaled down: - kube-node-1 - kube-node-3 上面的结果表明，给定 40 个 pod 的资源需求，在保证所有 pod 都能被调度的情况下，集群可以去掉 2 个节点，压缩比为 2，也就是有 50% 的资源浪费。 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:3:3","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"演进 当前已经支持上述三种能力，后续会继续完善其他容量、资源管理相关能力，如 基于 snapshot 的模拟 资源碎片分析 帮助我们基于集群历史某一时刻的状态来进行相关模拟操作，以及分析资源碎片情况等，欢迎体验并提出您的宝贵意见，谢谢！ 我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=ue85q0jbdgn2 ","date":"2023-02-20","objectID":"https://www.likakuli.com/posts/kluster-capacity/:4:0","tags":["kubernetes"],"title":"k8s 集群容量分析工具 - kluster-capacity","uri":"https://www.likakuli.com/posts/kluster-capacity/"},{"categories":["开源项目介绍"],"content":"背景 源于一次线上 P0 故障，一个生产集群被误操作删除（不只是业务被删，是集群也被删了），集群规模较大，在集群恢复后 Pod 进行了重新、调度的过程，整个过程（从开始恢复集群到业务服务就绪）耗时略长，其中涉及到调度环节耗时的计算，由于当时监控服务也部署在集群中，导致故障时的调度器监控数据丢失，最后的最后，又回到了原点：故障驱动，自证清白。于是就有了 scheduler-stress-test 项目，就有了本篇关于此项目的介绍，希望可以帮助到有类似需求（调度器压测）的同志们。 实现 ","date":"2023-02-12","objectID":"https://www.likakuli.com/posts/scheduler-stress-test/:0:0","tags":["kubernetes"],"title":"调度器压测工具介绍","uri":"https://www.likakuli.com/posts/scheduler-stress-test/"},{"categories":["开源项目介绍"],"content":"Metrics 能想到的最简单直观的办法，就是通过调度器暴露出来的 metrics 来计算调度性能，调度器指标定义文件： k8s.io/kubernetes/pkg/scheduler/metrics/metrics.go 有如下几个关键指标： 指标 type query example scheduler_e2e_scheduling_duration_seconds_count count sum(rate(scheduler_e2e_scheduling_duration_seconds_count{job=“advanced-scheduler”,profile=“default-scheduler”,result=“scheduled”}[5m])) by (instance) scheduler_pending_pods gauge scheduler_pending_pods{queue=‘active’, job=“default-scheduler”} scheduler_e2e_scheduling_duration_seconds_bucket histogram histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=“default-scheduler”}[5m])) by (le)) scheduler_scheduling_algorithm_duration_seconds_bucket histogram histogram_quantile(0.99, sum by(le) (rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=“default-scheduler”}[5m]))) scheduler_binding_duration_seconds_bucket histogram histogram_quantile(0.99, sum by(le) (rate(scheduler_binding_duration_seconds_bucket{job=“default-scheduler”}[5m]))) 从名字可以很清晰的看出来指标点的含义，这里不再赘述。 ","date":"2023-02-12","objectID":"https://www.likakuli.com/posts/scheduler-stress-test/:1:0","tags":["kubernetes"],"title":"调度器压测工具介绍","uri":"https://www.likakuli.com/posts/scheduler-stress-test/"},{"categories":["开源项目介绍"],"content":"Condition 第二种方式是通过获取 Pod PodScheduled Condition 信息，通过计算其 LastTransitionTime 与 CreationTimestamp 时间差作为调度耗时。 ","date":"2023-02-12","objectID":"https://www.likakuli.com/posts/scheduler-stress-test/:2:0","tags":["kubernetes"],"title":"调度器压测工具介绍","uri":"https://www.likakuli.com/posts/scheduler-stress-test/"},{"categories":["开源项目介绍"],"content":"分析 两种方式都可以得到调度耗时相关性能数据，但有一些差异，具体表现为： 前者的耗时比较精确，是调度器内存中保存的耗时，但缺少每个 Pod 的耗时，暴露的是所有 Pod 耗时分布，而 histogram 本身就会存在一定的误差。 后者的耗时则包含更多阶段的耗时： LastTransitionTime 是调度器发起异步 Bind 请求且 kube-apiserver 收到请求后在实际保存数据到 Etcd 前设置的； CreationTimestamp 是 kube-apiserver 收到创建请求后在保存到 Etcd 之前设置的； 所以 LastTransitionTime - CreationTimestamp 的结果会包含 Create 请求写 Etcd 的耗时（网络传输、写磁盘）、调度器 watch 到 Pod 的耗时（网络传输）、调度器请求 apiserver 到 apiserver 收到请求进行绑定的耗时（网络传输）等。由于 metav1.Time 结构在传输时采用 RFC3339 进行编码，只能精确到秒，因此会损失部分精度。 综上，无论采用那种方式进行统计，结果都会有一些误差，重要的是要理解误差来源，以及每种统计方式的结果代表的含义。在实际测试时，可以同时使用两种方式。 项目介绍 scheduler-stress-test 即通过 Condition 方式进行统计，使用方式参考 README.md。 ","date":"2023-02-12","objectID":"https://www.likakuli.com/posts/scheduler-stress-test/:3:0","tags":["kubernetes"],"title":"调度器压测工具介绍","uri":"https://www.likakuli.com/posts/scheduler-stress-test/"},{"categories":["开源项目介绍"],"content":"环境准备 为了模拟大规模调度场景，您可以使用 kwok 创建所需数量的节点。创建的节点可能处于 NotReady 状态。为了能够将这些节点用于调度 Pod，必须为待调度的 Pod 添加一个 toleration，以容忍所有 NoSchedule 的污点。 为此，您应该执行以下步骤： 在您的 k8s 集群上安装 kwok，请参考 https://kwok.sigs.k8s.io/docs/user/kwok-in-cluster/； 在您的 k8s 集群上创建虚拟节点，可以参考如下命令 cat \u003c\u003c EOF \u003e node.yaml apiVersion: v1 kind: Node metadata: annotations: node.alpha.kubernetes.io/ttl: \"0\" kwok.x-k8s.io/node: fake labels: beta.kubernetes.io/arch: amd64 beta.kubernetes.io/os: linux kubernetes.io/arch: amd64 kubernetes.io/hostname: {NODE_NAME} kubernetes.io/os: linux kubernetes.io/role: agent node-role.kubernetes.io/agent: \"\" type: kwok name: {NODE_NAME} spec: taints: # Avoid scheduling actual running pods to fake Node - effect: NoSchedule key: kwok.x-k8s.io/node value: fake status: allocatable: cpu: \"64\" ephemeral-storage: 1Ti hugepages-1Gi: \"0\" hugepages-2Mi: \"0\" memory: 250Gi pods: \"110\" capacity: cpu: \"64\" ephemeral-storage: 1Ti hugepages-1Gi: \"0\" hugepages-2Mi: \"0\" memory: 250Gi pods: \"128\" nodeInfo: architecture: amd64 bootID: \"\" containerRuntimeVersion: \"\" kernelVersion: \"\" kubeProxyVersion: fake kubeletVersion: fake machineID: \"\" operatingSystem: linux osImage: \"\" systemUUID: \"\" phase: Running EOF # create nodes as you needed for i in {0..99}; do sed \"s/{NODE_NAME}/kwok-node-$i/g\" node.yaml | kubectl apply -f -; done ","date":"2023-02-12","objectID":"https://www.likakuli.com/posts/scheduler-stress-test/:4:0","tags":["kubernetes"],"title":"调度器压测工具介绍","uri":"https://www.likakuli.com/posts/scheduler-stress-test/"},{"categories":["开源项目介绍"],"content":"压测 下载代码并构建： git clone https://github.com/k-cloud-labs/scheduler-stress-test.git make build 该工具支持两个命令：create 和 wait。 create 命令使用指定的模板文件，在 k8s 集群中以指定的并发级别创建指定数量的 pod。 wait 命令等待所有上述创建的 pod 被调度并连续打印结果。 示例： # 创建 1000 个 pod，使用 1000 的并发级别（namespace: scheduler-stress-test） sst create --kubeconfig=/root/.kube/config --count 1000 --concurrency 1000 --pod-template=pod.yaml # 等待结果 sst wait --kubeconfig=/root/.kube/config --namespace=scheduler-stress-test 上述示例使用项目中的 pod.yaml 作为模板，在 k8s 集群的 scheduler-stress-test 命名空间中创建了 1000 个 pod。然后等待并连续打印结果，您可以根据需要修改 pod.yaml 文件。 Enjoy it!!! ","date":"2023-02-12","objectID":"https://www.likakuli.com/posts/scheduler-stress-test/:5:0","tags":["kubernetes"],"title":"调度器压测工具介绍","uri":"https://www.likakuli.com/posts/scheduler-stress-test/"},{"categories":["开源项目介绍"],"content":"1.背景 Definition 准入 Webhook 是一种用于接收准入请求并对其进行处理的 HTTP 回调机制。 可以定义两种类型的准入 webhook，即 验证性质的准入 Webhook 和 修改性质的准入 Webhook。 修改性质的准入 Webhook 会先被调用。它们可以更改发送到 API 服务器的对象以执行自定义的设置默认值操作。 在完成了所有对象修改并且 API 服务器也验证了所传入的对象之后， 验证性质的 Webhook 会被调用，并通过拒绝请求的方式来强制实施自定义的策略。 以上是 kubernetes 给出的定义，简单来说，k8s 的 webhook 分为两类 validating 和 mutating 分别用于校验和修改 k8s 资源的修改操作。当用户创建 or 修改资源时，apiserver 会调用(http)已注册的 validating 和 mutating webhook，在这些 webhook 响应后才真正处理这次修改操作。 执行流程\" 执行流程 对于熟悉云原生开发或熟悉 k8s 的开发来说，webhook 应该是非常熟悉的一块，也应该写过多多少少的相关代码。作为 k8s 原生提供的能力，目前的 webhook 开发/使用方式可以说是非常不友好且很原始，k8s 很多能力都可以通过修改已有的对象或 CRD 的信息达到目的（如更新镜像，滚动升级等），唯有 webhook 需要写代码开发且注册到 apiserver 并运行程序来达到目的。 然而在日常开发中，大多数对于 webhook 的需求仅限于很简单的一些能力，比如校验某些对象的字段和合法性、修改特定的对象的信息（打 label、annotation or 修改资源规格等），这就导致了每次新增需求或者修改需求时都需要开发改代码打包发版。这使得整个过程效率很低，同时会存在一个集群内注册了过多的 webhook 使 apiserver 的响应时间被拉长，可靠性降低。 为了解决以上的提到的这些问题，我们开发了一套全新的基于规则的可编程的 webhook – kinitiras，希望能通过该组件来代替集群内所有的 webhook 并且所有的需求可以通过该组件来解决无需自己开发新的 webhook，提升效率的同时减少因多个 webhook 带来的安全隐患。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:1:0","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"2.能力 特性\" 特性 在讲述其设计与实现之前，这里先讲述一下 kinitiras 能干什么，具备哪些能力。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:2:0","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"2.1 校验资源 策略例子 我们可以对不同的资源配置不同的策略，从而减少出现一些不可控情况或者限制一些特殊操作，比如： 对于创建更新操作，可以对资源的一些字段进行限制（不可空 或者 其值等于不等于指定的值等等） 限制更新或删除操作。可以对一些特定的资源（ns or deployment）进行禁止删除或者二次确认机制（只有存在指定的 annotation 才允许删除） 字段校验 这些校验的字段和值可以为当前的 object 的值 也可以跟其他 object 的值（比如与 cm 或者 secret 等其他 object 对比）也为第三方 http 服务获取的数据进行对比校验。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:2:1","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"2.2 修改资源 策略例子 对于修改资源，我们的策略可以配置很多不同场景，满足不同的需求，比如： 给资源统一打标签打 annotation 修改资源规格 修改资源的 affinity toleration 等 而这个修改的值，均可以为动态的，可以从别的 object 获取（比如把 owner 的属性写到 pod 上）也可以从第三方 http 服务获取（我自己有类似的需求）。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:2:2","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"3.设计 Kinitiras 是来自希腊语 κινητήρας (kini̱tí̱ras)，意思为发动机（engine/motor），该项目的核心能力也是一个基于策略/规则的引擎，提供高效强大的能力。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:3:0","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"3.1 基础概念 概念 意义 说明 validating 校验 验证资源对象的合法性 mutating 修改 修改资源对象的字段 policy 策略/规则 一条可执行的策略/规则 override policy 修改策略 表示用于修改资源的策略 validate policy 校验策略 表示用于校验资源的策略 cue cue 语言 是一个开源的可编程的 json 超集，https://cuelang.org ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:3:1","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"3.2 核心逻辑 kinitiras 核心逻辑如下： 分别定义 validating 和 mutating 对应的 crd 表示一条策略（policy），记录策略生效的范围（指定资源名称或 label）和执行规则（校验或修改内容） 注册统一的 webhook configuration，默认订阅所有带有特定 label 的资源的修改删除事件（安装时可自定义该配置） 在收到 apiserver 的回调时，当前被修改的资源和已有的策略匹配筛选命中的策略列表 按循序执行策略 策略引擎核心逻辑(流程中步骤 3 和步骤 4 标反了)\" 策略引擎核心逻辑(流程中步骤 3 和步骤 4 标反了) ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:3:2","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"3.3 Api definition 本项目定义了三个 CRD： OverridePolicy: 用来修改资源信息（namespace 级别） ClusterOverridePolicy: 用来修改资源信息（cluster 级别） ClusterValidatePolicy: 用来校验资源信息（cluster 级别） 下面将定义的 crd 的核心部分简单讲解一下。 3.3.1 Resource selector ResourceSelector the resources will be selected. Field type required Description apiVersion string Y APIVersion represents the API version of the target resources. kind string Y Kind represents the Kind of the target resources. namespace string N Namespace of the target resource. Default is empty, which means inherit from the parent object scope. name string N Name of the target resource. Default is empty, which means selecting all resources. labelSelector Kubernetes meta/v1.LabelSelector N A label query over a set of resources. If name is not empty, labelSelector will be ignored. fieldSelector FieldSelector N A field query over a set of resources. If name is not empty, fieldSelector wil be ignored. 该结构是用于选择策略匹配资源，可以指定特定的某个资源，也可以选择指定 label 或 field 的方式对一组资源都生效。 for example: # match with all the pod which contains label webhook:enabledresourceSelectors:- apiVersion:v1kind:PodlabelSelector:matchLabels:webhook:enabled 3.3.2 Validate rule Defines validate rules on operations. Field type required Description targetOperations []Kubernetes admission/v1.Operation Y Operations is the operations the admission hook cares about - CREATE, UPDATE, DELETE, CONNECT or * for all of those operations and any future admission operations that are added. If * is present, the length of the slice must be one. cue string N Cue represents validate rules defined with cue code. template ValidateRuleTemplate N Template of condition which defines validate cond, and it will be rendered to CUE and store in RenderedCue field, so if there are any data added manually will be erased. renderedCue string N RenderedCue represents validate rule defined by Template. Don’t modify the value of this field, modify Rules instead of. 这里是定义 validate 策略的执行逻辑相关信息。 targetOperations：表示生效的操作类型，即可以定义只对创建 or delete 事件生效 cue：该字段可填写一段 cue 代码，会在命中策略后执行该代码，请看下面 Example template：定义了一个简单的模板，将一些常见的校验常见模板化（无需写 cue 了） renderedCue：模板最终会自动渲染成 cue 代码并存储到该字段上。 cue example： validateRules:# 这段代码表示，检查当前资源的 label 如果有不可删除的标识，则拒绝这次删除操作- cue:|-object: _ @tag(object) reject: object.metadata.labels != null \u0026\u0026 object.metadata.labels[\"xxx.io/no-delete\"] == \"true\" validate: { if reject{ reason: \"operation rejected\" } if !reject{ reason: \"\" } valid: !reject }targetOperations:- DELETE template example: # 表示 存在 no-delete annotation 时 拒绝本次删除操作validateRules:- targetOperations:- DELETEtemplate:type:condition# 当前只有 condition 一个类型，后续扩展condition:affectMode:reject# 表示命中该规则是拒绝，可以设置为 allow 表示只有命中时才准入，默认是 rejectcond:Exist# 支持存在 不存在 大于 小于 等操作message:\"cannot delete this ns\"# 命中时返回的信息dataRef:# 校验的数据来源from:current# current 表示当前 object 中获取，支持从当前集群的其他资源或者通过 http 请求获取数据path:\"/metadata/annotations/no-delete\"# 数据 path 这种模板化的方式，可以减少学习 cue 的成本，能满足大部分判断字段是否存在，与其他字段做大小对比等场景。 3.3.3 Override rule Overriders offers various alternatives to represent the override rules. If more than one alternative exist, they will be applied with following order: RenderCue Cue Plaintext Field type required Description plaintext []PlaintextOverrider N Plaintext represents override rules defined with plaintext overriders. cue string N Cue represents override rules defined with cue code. template OverrideRuleTemplate N Template of rule which defines override rule, and it will be rendered to CUE and store in RenderedCue field, so if there are any data added manually will be erased. renderedCue string N RenderedCue represents override rule defined by Template. Don’t modify the value of this field, modify Rules instead of. 这里定义 Override policy 的核心部分，即修改资源信息策略，其含义如下： plaintext：为简单的修改方式，填写操作的字段和值即可 cue：该字段可填写一段 cue 代码，会在命中策略后执行该代码，请看下面 Example template：定义了一个简单的模板，将一些常见的修改模板化（无需写 cue 了） renderedCue：模板最终会自动渲染成 cue 代码并存储到该字段上。 plaintext example： overrideRules:- targetOperations:- CREATEoverriders:plaintext:# 为数组，可同时修改多个字段值，会直接 apply 到对象上- path:/metadata/annot","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:3:3","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"4.实现 上面把 kinitiras 的工作原理和核心概念进行了讲述，从这里开始将其核心能力的实现镜像简单的描述，方便使用时debug 或了解底层实现。 项目结构 项目整体来说分三个 repo，分别是 kinitiras, pkg, pidalio。每个 repo 的定位不同，其中 kinitiras 为比较常规的 webhook 项目，负责将自己注册到 apiserver，处理回调，pkg则为核心模块的实现，比如 api 定义，执行策略等。而 pidalio 为 client-go 的 transport 中间件，可以在客户端拦截请求执行策略的应用。本篇重点讲述前两个 repo 的核心实现。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:4:0","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"4.1 kinitiras 项目地址：https://github.com/k-cloud-labs/kinitiras 本项目作为一个 webhook，核心逻辑就是，初始化各个参数和将自己注册到 apiserver，然后在回调函数里调用 pkg 提供的统一处理方法即可。 4.1.2 初始化 // Run runs the webhook server with options. This should never exit. func Run(ctx context.Context, opts *options.Options) error { klog.InfoS(\"kinitiras webhook starting.\", \"version\", version.Get()) config, err := controllerruntime.GetConfig() if err != nil { panic(err) } config.QPS, config.Burst = opts.KubeAPIQPS, opts.KubeAPIBurst hookManager, err := controllerruntime.NewManager(config, controllerruntime.Options{ // ... set options }) if err != nil { klog.ErrorS(err, \"failed to build webhook server.\") return err } // init clients, informer, lister sm := \u0026setupManager{} if err := sm.init(hookManager, ctx.Done()); err != nil { klog.ErrorS(err, \"init setup manager failed\") return err } if err := sm.waitForCacheSync(ctx); err != nil { klog.ErrorS(err, \"wait for cache sync failed\") return err } if err := sm.setupInterrupter(); err != nil { klog.ErrorS(err, \"setup interrupter failed\") return err } setupCh, err := cert.SetupCertRotator(hookManager, cert.Options{ // ... set options }) if err != nil { klog.ErrorS(err, \"failed to setup cert rotator controller.\") return err } go func() { \u003c-setupCh // register handler here hookServer := hookManager.GetWebhookServer() hookServer.Register(\"/mutate\", \u0026webhook.Admission{Handler: pkgwebhook.NewMutatingAdmissionHandler(sm.overrideManager, sm.policyInterrupterManager)}) hookServer.Register(\"/validate\", \u0026webhook.Admission{Handler: pkgwebhook.NewValidatingAdmissionHandler(sm.validateManager, sm.policyInterrupterManager)}) hookServer.WebhookMux.Handle(\"/readyz\", http.StripPrefix(\"/readyz\", \u0026healthz.Handler{})) }() // blocks until the context is done. if err := hookManager.Start(ctx); err != nil { klog.ErrorS(err, \"webhook server exits unexpectedly.\") return err } // never reach here return nil } 上述代码比较常规，只有 sm.setupInterrupter() 这块单独说一下。本 webhook 自定义了几个 CRD 作为策略的载体，而策略本身也需要进行校验和修改，尤其是提供了模板化(template)后，模板需要渲染成 cue 脚本，为了能够在策略创建时进行校验和渲染，引进了 interrupter的概念。Interrupter 顾名思义 – 拦截器，用来拦截策略并对策略的特定字段进行校验和对模版进行渲染，这些逻辑与常规的对象的校验和修改不太一样，因此不走普通的逻辑，只经过 interrupter的逻辑部分。而上述的的 sm.setupInterrupter() 是用来初始化这些 interrupter 的，代码如下： func (s *setupManager) setupInterrupter() error { // 初始化模板 -- 模板渲染是基于 go.tmpl 实现的，因此这里初始化 tmpl otm, err := templatemanager.NewOverrideTemplateManager(\u0026templatemanager.TemplateSource{ Content: templates.OverrideTemplate, TemplateName: \"BaseTemplate\", }) if err != nil { klog.ErrorS(err, \"failed to setup mutating template manager.\") return err } // 初始化模板 -- 模板渲染是基于 go.tmpl 实现的，因此这里初始化 tmpl vtm, err := templatemanager.NewValidateTemplateManager(\u0026templatemanager.TemplateSource{ Content: templates.ValidateTemplate, TemplateName: \"BaseTemplate\", }) if err != nil { klog.ErrorS(err, \"failed to setup validate template manager.\") return err } // base baseInterrupter := interrupter.NewBaseInterrupter(otm, vtm, templatemanager.NewCueManager()) // op overridePolicyInterrupter := interrupter.NewOverridePolicyInterrupter(baseInterrupter, s.tokenManager, s.client, s.opLister) // register interrupter to manager s.policyInterrupterManager.AddInterrupter(schema.GroupVersionKind{ Group: policyv1alpha1.SchemeGroupVersion.Group, Version: policyv1alpha1.SchemeGroupVersion.Version, Kind: \"OverridePolicy\", }, overridePolicyInterrupter) // cop s.policyInterrupterManager.AddInterrupter(schema.GroupVersionKind{ Group: policyv1alpha1.SchemeGroupVersion.Group, Version: policyv1alpha1.SchemeGroupVersion.Version, Kind: \"ClusterOverridePolicy\", }, interrupter.NewClusterOverridePolicyInterrupter(overridePolicyInterrupter, s.copLister)) // cvp s.policyInterrupterManager.AddInterrupter(schema.GroupVersionKind{ Group: policyv1alpha1.SchemeGroupVersion.Group, Version: policyv1alpha1.SchemeGroupVersion.Version, Kind: \"ClusterValidatePolicy\", }, interrupter.NewClusterValidatePolicyInterrupter(baseInterrupter, s.tokenManager, s.client, s.cvpLister)) return s.policyInterrupterManager.OnStar","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:4:1","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"4.2 pkg 项目地址： https://github.com/k-cloud-labs/pkg pkg 包含了大部分逻辑的实现，同时也包含了 crd 的定义和生成的 client 代码。接上面的提到的内容，这里主要讲述interrupter的实现部分和策略的命中和执行部分。 4.2.1 interrupter 先看定义: // PolicyInterrupterManager manage multi PolicyInterrupter and decide which one to use by gvk. type PolicyInterrupterManager interface { PolicyInterrupter // AddInterrupter add a PolicyInterrupter to manager, // it will replace interrupter if already add with same gvk.s AddInterrupter(gvk schema.GroupVersionKind, pi PolicyInterrupter) } // PolicyInterrupter defines interrupt process for policy change // It validate and mutate policy. type PolicyInterrupter interface { // OnMutating called on \"/mutating\" api to complete policy // return nil means obj is not defined policy OnMutating(obj, oldObj *unstructured.Unstructured, operation admissionv1.Operation) ([]jsonpatchv2.JsonPatchOperation, error) // OnValidating called on \"/validating\" api to validate policy // return nil means obj is not defined policy or no invalid field OnValidating(obj, oldObj *unstructured.Unstructured, operation admissionv1.Operation) error // OnStartUp called when webhook process initialize // return error if initial phase get any error OnStartUp() error } PolicyInterrupterManager 继承了 PolicyInterrupter 并新增一个添加 interrupter 的方法，用来管理多个 interrupter。而每一个 interrupter 都会实现下面的三个方法： OnMutating: 在 apiserver 回调 /mutating 接口时调用，主要用来渲染和补充策略信息 OnValidating: 在 apiserver 回调 /validating 接口时调用，主要用来校验策略信息 OnStartUp: 在 webhook 启动阶段调用，可做一些初始化工作（拉取缓存等） 而 manager 的实现与实际 interrupter 不同，它首先识别当前的资源是不是我们定义的策略 crd，然后从内存找有没有对应的注册的 interrupter 再去调用该 interrupter 的对应方法。代码如下： type policyInterrupterManagerImpl struct { interrupters sync.Map } func (p *policyInterrupterManagerImpl) OnValidating(obj, oldObj *unstructured.Unstructured, operation admissionv1.Operation) error { if interrupter := p.getInterrupter(obj); interrupter != nil { return interrupter.OnValidating(obj, oldObj, operation) } return nil } func (p *policyInterrupterManagerImpl) getInterrupter(obj *unstructured.Unstructured) PolicyInterrupter { if !p.isKnownPolicy(obj) { klog.V(5).InfoS(\"unknown policy\", \"gvk\", obj.GroupVersionKind()) return nil } i, ok := p.interrupters.Load(obj.GroupVersionKind()) if ok { klog.V(4).InfoS(\"sub interrupter found\", \"gvk\", obj.GroupVersionKind()) return i.(PolicyInterrupter) } return nil } func (p *policyInterrupterManagerImpl) isKnownPolicy(obj *unstructured.Unstructured) bool { group := strings.Split(obj.GetAPIVersion(), \"/\")[0] return group == policyv1alpha1.SchemeGroupVersion.Group } 4.2.2 渲染 渲染这个事儿前面已经提了无数遍，这里将一次性将渲染相关的设计和实现都讲清楚。 先说背景。 本项目在早期就支持了用户手写 cue 的方式在策略中执行复杂逻辑，从而满足不同的需求。但是写 cue 需要对这个语言的语法和特性有一定了解加上没有比较好的验证 cue 脚本合法性的机制，导致上手难度比较高，因此想到了把一些常见的情况抽象出来一个结构化的模板，使用者只需要在模板填写必要的参数，由 webhook 本身把这个模板翻译成 cue 脚本。 为了能够将结构化数据翻译成 cue 脚本，我们写了一个比较复杂的 go/tmpl (template link)，然后继续翻译。流程如下： interrupter 检查是否填写模板信息 根据模板类型进行渲染(tmpl.Execute) 生成 cue 脚本 对结果进行 format 和 lint 检查 这个过程被称之为渲染。 再说实现。 由于相关模板和代码比较多，这里不进行展示，只把核心实现进行说明： 为不同的 policy 写了不同的 tmpl。由于 validate 和 override 策略的 cue 执行结果的结构要求不同，因此写了两份 tmpl 根据 policy 去执行不同的渲染。code link 使用 cue 官方提供的 go package 进行 format 和 lint。cue 底层是 go 语言实现的，因此对 go 的支持比较友好，提供了相关 package，可以在代码中直接 format 和 lint cue 脚本，确保渲染后后的 cue 脚本时合法可运行的。code link 4.2.3 策略命中 当前 object 和策略的匹配过程如下： 列出当前所有的策略。这块从 informer 内存读取，且根据当前是 validating 还是 mutating 的情况读取相对应的 policy 列表。 对于没有设置 resource selector 的策略，默认认为命中。 对于设置 resource selector 的策略，进行策略匹配（代码下面会展示。） 再对命中的策略中设置操作类型与当前 object 的操作类型进行匹配。 匹配完成。 resource selector 匹配规则： any means no matter if it’s empty or not name label selector field selector result not empty any any match name only empty empty empty match all empty not empty empty match labels only empty empty not empty match fields only empty not empty not empty match both labels and fields 相关代码： // ResourceMatchSelectors tells if the specific resource matches the selectors. func ResourceMatchSelectors(resource *unstructured.Unstructured, selectors ...policyv1alpha1.ResourceSelector) bool { for _, rs := range selectors { // 一个策略可以配置多个","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:4:2","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"5.总结 本篇介绍了 k-cloud-labs 推出的 webhook 产品，其功能和实用性方面都非常优秀，我现在作为该项目的其中一个维护者 对项目进行了一定的特性增加和优化，后期将持续更新新的能力，解决更多的问题。 主要内容： 介绍了开发该 webhook 的背景和其解决的问题 介绍了核心设计思路和 api 定义 介绍了其核心逻辑的实现 关于更详细的设计细节和使用案例以及安装方法，请点击这里跳转官网去了解。 ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:5:0","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["开源项目介绍"],"content":"6.链接🔗 官方：https://k-cloud-labs.github.io/kinitiras-doc/ Github：https://github.com/k-cloud-labs ","date":"2023-02-03","objectID":"https://www.likakuli.com/posts/kinitiras-all/:6:0","tags":["kubernetes"],"title":"Kinitiras - 可编程 webhook 规则引擎详解","uri":"https://www.likakuli.com/posts/kinitiras-all/"},{"categories":["随笔"],"content":"第一卷 - 关心群众生活，注意工作方法 这是毛泽东在一九三四年一月二十二日至二月一日在江西瑞金召开的第二次全国工农兵代表大会上所作的结论的一部分。 有两个问题，同志们在讨论中没有着重注意，我觉得应该提出来说一说。 ","date":"2022-11-26","objectID":"https://www.likakuli.com/posts/thoughtfulessay2/:0:0","tags":["思考","随笔"],"title":"思想随笔2 - 一起读毛选","uri":"https://www.likakuli.com/posts/thoughtfulessay2/"},{"categories":["随笔"],"content":"第一个问题是关于群众生活的问题。 我们现在的中心任务是动员广大群众参加革命战争，以革命战争打倒帝国主义和国民党，把革命发展到全国去，把帝国主义赶出中国去。谁要是看轻了这个中心任务，谁就不是一个很好的革命工作人员。我们的同志如果把这个中心任务真正看清楚了，懂得无论如何要把革命发展到全国去，那末，我们对于广大群众的切身利益问题，群众的生活问题，就一点也不能疏忽，一点也不能看轻。因为革命战争是群众的战争，只有动员群众才能进行战争，只有依靠群众才能进行战争。 如果我们单单动员人民进行战争，一点别的工作也不做，能不能达到战胜敌人的目的呢？当然不能。我们要胜利，一定还要做很多的工作。领导农民的土地斗争，分土地给农民；提高农民的劳动热情，增加农业生产；保障工人的利益；建立合作社；发展对外贸易；解决群众的穿衣问题，吃饭问题，住房问题，柴米油盐问题，疾病卫生问题，婚姻问题。总之，一切群众的实际生活问题，都是我们应当注意的问题。假如我们对这些问题注意了，解决了，满足了群众的需要，我们就真正成了群众生活的组织者，群众就会真正围绕在我们的周围，热烈地拥护我们。同志们，那时候，我们号召群众参加革命战争，能够不能够呢？能够的，完全能够的。 ","date":"2022-11-26","objectID":"https://www.likakuli.com/posts/thoughtfulessay2/:1:0","tags":["思考","随笔"],"title":"思想随笔2 - 一起读毛选","uri":"https://www.likakuli.com/posts/thoughtfulessay2/"},{"categories":["随笔"],"content":"第二个问题是关于工作方法的问题。 我们是革命战争的领导者、组织者，我们又是群众生活的领导者、组织者。组织革命战争，改良群众生活，这是我们的两大任务。在这里，工作方法的问题，就严重地摆在我们的面前。我们不但要提出任务，而且要解决完成任务的方法问题。我们的任务是过河，但是没有桥或没有船就不能过。不解决桥或船的问题，过河就是一句空话。不解决方法问题，任务也只是瞎说一顿。不注意扩大红军的领导，不讲究扩大红军的方法，尽管把扩大红军念一千遍，结果还是不能成功。其他如查田工作⑹、经济建设工作、文化教育工作、新区边区的工作，一切工作，如果仅仅提出任务而不注意实行时候的工作方法，不反对官僚主义的工作方法而采取实际的具体的工作方法，不抛弃命令主义的工作方法而采取耐心说服的工作方法，那末，什么任务也是不能实现的。 第一卷 - 矛盾论 事物发展过程的根本矛盾及为此根本矛盾所规定的过程的本质，非到过程完结之日，是不会消灭的；但是事物发展的长过程中的各个发展的阶段，情形又往往互相区别。这是因为事物发展过程的根本矛盾的性质和过程的本质虽然没有变化，但是根本矛盾在长过程中的各个发展阶段上采取了逐渐激化的形式。并且，被根本矛盾所规定或影响的许多大小矛盾中，有些是激化了，有些是暂时地或局部地解决了，或者缓和了，又有些是发生了，因此，过程就显出阶段性来。如果人们不去注意事物发展过程中的阶段性，人们就不能适当地处理事物的矛盾。 第三卷 - 关于领导方法的若干问题 （一九四三年六月一日） 这是毛泽东为中共中央所写的决定。 （一）我们共产党人无论进行何项工作，有两个方法是必须采用的，一是一般和个别相结合，二是领导和群众相结合。 （四）在我党的一切实际工作中，凡属正确的领导，必须是从群众中来，到群众中去。这就是说，将群众的意见（分散的无系统的意见）集中起来（经过研究，化为集中的系统的意见），又到群众中去作宣传解释，化为群众的意见，使群众坚持下去，见之于行动，并在群众行动中考验这些意见是否正确。然后再从群众中集中起来，再到群众中坚持下去。如此无限循环，一次比一次地更正确、更生动、更丰富。这就是马克思主义的认识论。 （五）领导骨干和广大群众在组织中在斗争行动中发生正确关系的思想，正确的领导意见只能从群众中集中起来又到群众中坚持下去的思想，在领导意见见之实行时要将一般号召和个别指导互相结合的思想，都必须在这次整风中普遍地加以宣传，借以纠正干部中在这个问题上的错误观点。许多同志，不注重和不善于团结积极分子组成领导核心，不注重和不善于使这种领导核心同广大群众密切地结合起来，因而使自己的领导变成脱离群众的官僚主义的领导。许多同志，不注重和不善于总结群众斗争的经验，而欢喜主观主义地自作聪明地发表许多意见，因而使自己的意见变成不切实际的空论。许多同志，满足于工作任务的一般号召，不注重和不善于在作了一般号召之后，紧紧地接着从事于个别的具体的指导，因而使自己的号召停止在嘴上、纸上或会议上，而变为官僚主义的领导。这次整风，必须纠正这些缺点，在整风学习、检查工作、审查干部中学会领导和群众相结合、一般和个别相结合的方法，并在以后应用此种方法于一切工作。 （六）从群众中集中起来又到群众中坚持下去，以形成正确的领导意见，这是基本的领导方法。在集中和坚持过程中，必须采取一般号召和个别指导相结合的方法，这是前一个方法的组成部分。从许多个别指导中形成一般意见（一般号召），又拿这一般意见到许多个别单位中去考验（不但自己这样做，而且告诉别人也这样做），然后集中新的经验（总结经验），做成新的指示去普遍地指导群众。同志们在这次整风中应该这样去做，在任何工作中也应该这样去做。比较好的领导，就是从比较善于这样去做而得到的。 ","date":"2022-11-26","objectID":"https://www.likakuli.com/posts/thoughtfulessay2/:2:0","tags":["思考","随笔"],"title":"思想随笔2 - 一起读毛选","uri":"https://www.likakuli.com/posts/thoughtfulessay2/"},{"categories":["开源项目介绍"],"content":"本篇由来 在使用 Admission Webhook 的时候，很可能会涉及到发送 http 请求以获取某些数据。在 v0.1.1 版本中对此进行了支持，本文主要来介绍如何在 kinitiras 中发送 http 请求，如果对 kinitiras 还不熟悉，请参考前篇。 ","date":"2022-07-18","objectID":"https://www.likakuli.com/posts/kinitiras-cue-http/:1:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras 发送 http(s) 请求","uri":"https://www.likakuli.com/posts/kinitiras-cue-http/"},{"categories":["开源项目介绍"],"content":"功能介绍 当前仅支持在使用 CUE 时可以发送 http(s) 请求，相关结构定义在 processing 下，其中 output 用来定义输出结果，需要与 http response 的结构一致，按需定义即可。http 部分和 CUE http 结构一致。 示例： object: _ @tag(object) processing: { http: { method: *\"GET\" | string url: \"http://127.0.0.1:8090/api/v1/token?val=test-token\" request: { body ?: bytes header: { \"Accept-Language\": \"en,nl\" } trailer: { \"Accept-Language\": \"en,nl\" User: \"foo\" } } } output: { token?: string } } validate:{ reason: \"hello cue\" valid: object.metadata.name == \"ut-cue-success-with-parameter\" \u0026\u0026 processing.output.token == \"test-token\" } 上面这段 CUE 配置的含义是传入一个 k8s 资源对象，访问 http://127.0.0.1:8090/api/v1/token?val=test-token 获取 token 信息，最终返回 validate 结果，如果传入的 k8s 对象的名字是 ut-cue-success-with-parameter 并且 http 返回结果中的 token 是 test-token 的话，校验成功。 CUE http 相关处理参考 kubevela 相关能力的实现，涉及到的部分包在当前 CUE 中属于 internal，无法在外部直接引用，故部分内容直接在 builtin 目录下重新写了一遍。 ","date":"2022-07-18","objectID":"https://www.likakuli.com/posts/kinitiras-cue-http/:2:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras 发送 http(s) 请求","uri":"https://www.likakuli.com/posts/kinitiras-cue-http/"},{"categories":["开源项目介绍"],"content":"项目由来 使用 kubernetes 的同学可能或多或少会有以下的实际业务或者需求场景： 为确保安全性，需要对某些资源进行删除保护，例如不允许删除 namespace、crd 定义等； 根据服务画像为不同的服务设置不同的属性，这些属性基本是业务无感的，例如设置不同的超售比、设置不同的 Label 以及调度特性从而实现 io 敏感型与 io 密集型服务的反亲和调度等； 针对同一个 workload 生成 per pod per config 的配置，例如为特定的业务容器设置 debug 开关、特权模式、日志路径等； 集群里面每引入一个组件，可能就会同时引入一个甚至多个 admission webhook，久而久之，集群里面会存在众多的 admission webhook，实现方式、关注的资源都不同，管理起来比较复杂； 虽然使用一些工具例如 kube-builder、controller-runtime 等可以快速的创建 admission webhook 的框架，但开发整个功能也需要一定的开发工作量，往往需要开发的业务逻辑比较简单，基本是根据一些规则进行一些决策； admission webhook 里的业务逻辑的改动需要升级 wehook 来实现，变更就有可能引入线上稳定性风险； 可以大致归为三类：集群资源管理、admission webhok 自身管理、业务资源定制。基于以上的需求场景，一个通用的可编程的 webhook 规则引擎的想法诞生了。 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:1:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"项目介绍 Kinitiras 为希腊语，意思为发动机、引擎，贴合 rule engine 的概念。项目通过抽象出来三种策略来实现集群资源的 mutate 和 validate 的逻辑，支持通过 CUE 配置业务逻辑，从而支持了动态编程能力，可以在不变更程序的前提下通过对策略的操作实现所需的能力。 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:2:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"能力介绍 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:3:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"Mutate 内置了 OverridePolicy 和 ClusterOverridePolicy 来支持 Mutate 能力，前者是 Namespace 级别生效，即只能修改同 Namespace 下的资源对象，后者是 Cluster 级别生效，可修改资源对象不受 Namespace 限制。策略内置一些筛选条件来对目标资源对象进行筛选，对于同一个资源对象，可以有多条策略与之匹配，其生效顺序针对不同类型策略优先使用 ClusterOverridePolicy，同一种类型的策略则按照字母顺序依次生效。 例： kind:ClusterOverridePolicyapiVersion:policy.kcloudlabs.io/v1alpha1metadata:name:add-anno-cop-cuespec:resourceSelectors:- apiVersion:v1kind:PodlabelSelector:matchLabels:kinitiras.kcloudlabs.io/webhook:enabledoverrideRules:- targetOperations:- CREATEoverriders:cue:|-object: _ @tag(object) patches: [ if object.metadata.annotations == _|_ { { op: \"add\" path: \"/metadata/annotations\" value: {} } }, { op: \"add\" path: \"/metadata/annotations/added-by\" value: \"cue\" } ]---kind:OverridePolicyapiVersion:policy.kcloudlabs.io/v1alpha1metadata:name:add-anno-op-plaintextnamespace:defaultspec:resourceSelectors:- apiVersion:v1kind:PodlabelSelector:matchLabels:kinitiras.kcloudlabs.io/webhook:enabledoverrideRules:- targetOperations:- CREATEoverriders:plaintext:- path:/metadata/annotations/added-byop:addvalue:op 上述例子中定义了两种类型的策略，通过 resourceSelector 进行目标资源对象的筛选，对于筛选出来的资源对象，将会继续使用 overrideRules 里面定义的规则进行判断，如果规则的 targetOperations 中包含本次操作类型，则将使用规则内的 ``overriders生成最终对象，对比原始对象和最终对象生成 json-patch 所需的 patches 数组返回给kube-apiserver. 支持两种类的 overriders`：plaintext、cue，前者适用于一些简单场景，后者适用于需要根据传入数据进行额外逻辑处理才能得到预期结果的场景，能力相对前者会更强。 cue 脚本约定了输入输出参数，必须包含这些参数脚本才能成功执行。输入参数只有一个：object，即要操作的资源对象，输出参数为 patches 数组，定义如下： object: _ @tag(object) patch: { op: string path: string value: string } // for mutating result patches: [...patch] 具体数据结构如下： // ResourceSelector the resources will be selected. type ResourceSelector struct { // APIVersion represents the API version of the target resources. // +required APIVersion string `json:\"apiVersion\"` // Kind represents the Kind of the target resources. // +required Kind string `json:\"kind\"` // Namespace of the target resource. // Default is empty, which means inherit from the parent object scope. // +optional Namespace string `json:\"namespace,omitempty\"` // Name of the target resource. // Default is empty, which means selecting all resources. // +optional Name string `json:\"name,omitempty\"` // A label query over a set of resources. // If name is not empty, labelSelector will be ignored. // +optional LabelSelector *metav1.LabelSelector `json:\"labelSelector,omitempty\"` } // Overriders offers various alternatives to represent the override rules. // // If more than one alternatives exist, they will be applied with following order: // - Cue // - Plaintext type Overriders struct { // Plaintext represents override rules defined with plaintext overriders. // +optional Plaintext []PlaintextOverrider `json:\"plaintext,omitempty\"` // Cue represents override rules defined with cue code. // +optional Cue string `json:\"cue,omitempty\"` } 上述例子的效果是对于任意 Namespace 的 Pod，只要其携带 kinitiras.kcloudlabs.io/webhook: enabled Label，则会修改其 annotation 为 added-by: cue，针对 default 下的 Pod，只要其携带 kinitiras.kcloudlabs.io/webhook: enabled Label，则会修改其 annotation 为 added-by: op。如果两个策略同时 apply 到集群中，同时在 default 下创建一个 Pod 并携带上述 Label，则最终创建的 Pod annotation 将会是 added-by: op，因为 OverridePolicy 后执行，覆盖了 ClusterOverridePolicy 的修改。 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:3:1","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"Validate 内置了 ClusterValidatePolicy， 集群级别生效。针对同一个资源对象，同样可能存在多条匹配的策略，顺序在这里不重要，需要全部策略都通过之后才算整体校验通过，任意一个策略校验失败后将会拒绝请求。 例： apiVersion:policy.kcloudlabs.io/v1alpha1kind:ClusterValidatePolicymetadata:name:test-delete-nsspec:validateRules:- cue:|-object: _ @tag(object) reject: object.metadata.labels != null \u0026\u0026 object.metadata.labels[\"kinitiras.kcloudlabs.io/webhook\"] == \"enabled\" validate: { if reject{ reason: \"operation rejected\" } if !reject{ reason: \"\" } valid: !reject }targetOperations:- DELETEresourceSelectors:- apiVersion:v1kind:Namespace 通过 resourceSelector 进行目标资源对象的筛选，对于筛选出来的资源对象，将会继续使用 validateRules 里面定义的规则进行判断，如果规则的 targetOperations 中包含本次操作类型，则将使用 cue 脚本进行校验，检验结果返回给 kube-apiserver。与 Mutate 不同的是，Validate 只支持 cue 脚本校验。cue 脚本约定了输入输出参数，必须包含这些参数脚本才能成功执行。输入参数有两个：object、oldObject，其中后者只有在校验 UPDATE 操作时才需要，输出参数为 validate 结果，定义如下： object: _ @tag(object) oldObject: _ @tag(oldObject) // for validating result validate: { reason?: string valid: bool } 上述例子的效果为阻止删除带有 kinitiras.kcloudlabs.io/webhook: enabled 标签的 namespace。 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:3:2","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"部署 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:4:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"部署 CRD 资源 kubectl apply -f https://raw.githubusercontent.com/k-cloud-labs/pkg/main/charts/_crds/bases/policy.kcloudlabs.io_overridepolicies.yaml kubectl apply -f https://raw.githubusercontent.com/k-cloud-labs/pkg/main/charts/_crds/bases/policy.kcloudlabs.io_clusteroverridepolicies.yaml kubectl apply -f https://raw.githubusercontent.com/k-cloud-labs/pkg/main/charts/_crds/bases/policy.kcloudlabs.io_clustervalidatepolicies.yaml ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:4:1","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"部署应用程序 在部署之前，需要先根据实际需求修改 webhook-configuration.yaml 文件，尽量缩小目标资源对象的范围，减少不必要的请求。调整完所有 yaml 文件之后，只需要执行 apply 即可，如下 kubectl apply -f deploy/ ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:4:2","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"例子 在 examples 文件夹下内置了上述出现过的三个策略，可以 apply 到集群进行尝试。 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:4:3","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["开源项目介绍"],"content":"小结 上面对项目由来，能力及使用方式进行了简单介绍，核心还是利用了 admission webhook 来实现。可能有的小伙伴对 admission webhook 的稳定性、性能比较谨慎，鉴于此，这里提供了另外一个项目 pidalio，通过扩展 client-go Transport 来实现，在客户端生效，使用简单，代码层面只需要引用对用的包，并为 rest.Config 设置自定义的 Transport 即可。如下 import( \"github.com/k-cloud-labs/pidalio\" ) config.Wrap(pidalio.NewPolicyTransport(config, stopCh).Wrap) 但 pidalio 存在一个限制，即只支持 Mutate 操作，且必须使用 client-go 访问 kube-apiserver。 ","date":"2022-05-12","objectID":"https://www.likakuli.com/posts/kinitiras/:5:0","tags":["kubernetes"],"title":"admission webhook 花式玩法 - kinitiras","uri":"https://www.likakuli.com/posts/kinitiras/"},{"categories":["有用的知识"],"content":"本篇由来 分享在使用 Docker for Mac 过程中遇到的问题，希望可以帮助到遇到问题的人。 ","date":"2021-11-03","objectID":"https://www.likakuli.com/posts/docker-for-mac-1/:1:0","tags":["docker"],"title":"docker for mac - 1","uri":"https://www.likakuli.com/posts/docker-for-mac-1/"},{"categories":["有用的知识"],"content":"网络不通 Docker for Mac 没有提供从宿主的macOS通过容器IP访问容器的方式。参考 Known limitations, use cases, and workarounds。遇到此问题可以参照此文档操作：mac docker connector。 ","date":"2021-11-03","objectID":"https://www.likakuli.com/posts/docker-for-mac-1/:2:0","tags":["docker"],"title":"docker for mac - 1","uri":"https://www.likakuli.com/posts/docker-for-mac-1/"},{"categories":["有用的知识"],"content":"查看容器文件 Docker for Mac 容器是运行在虚拟机中的，在 MacOS 上运行了一个虚机。所以通过 docker inspect 看到的命令，尤其和文件系统相关的信息，直接在宿主机上是无法找到的，需要找到目录对应的在 MacOS 上的目录，网上搜索可以得到两种方案，都是通过 screen 命令，后接具体文件，版本不同，文件不同，但最新版都已经不可用。 可以通过一个万能的指令来实现，原理还是利用 linux 的各种 namespace 机制，命令如下 docker run -it --privileged --pid=host debian nsenter -t 1 -m -u -n -i sh 此命令是以特权模式启动一个新容器，共享虚机的 pid namespace，启动之后通过 nsenter 命令进去到进程1 的 mount、uts、net namespaec 中。因为启动的新容器利用了虚机的 pid namespace，所以看到的1号进程就是虚机的1号进程。 多么巧妙的实现~ ","date":"2021-11-03","objectID":"https://www.likakuli.com/posts/docker-for-mac-1/:3:0","tags":["docker"],"title":"docker for mac - 1","uri":"https://www.likakuli.com/posts/docker-for-mac-1/"},{"categories":["有用的知识"],"content":"Exec Permission denied docker run报错，报如上的错。出现这种情况大概率是在 linux 上打了镜像，在 mac 上运行时出的错。利用上一步中提到的方法进入容器找到对应的文件， 可以参考这里。 chmod 755 filename ","date":"2021-11-03","objectID":"https://www.likakuli.com/posts/docker-for-mac-1/:4:0","tags":["docker"],"title":"docker for mac - 1","uri":"https://www.likakuli.com/posts/docker-for-mac-1/"},{"categories":["源码分析","问题排查"],"content":"背景 上半年遇到了一些绑核相关的 bug，分析了其原因，但没有总结整理下来，现在又碰到了，补一下作业，同时也希望可以帮助大家快速从坑里爬出来。本篇会总结绑核相关的 bug，部分官网已修复，部分尚未修复，与 k8s 版本有关，感兴趣的可以对 k8s 进行一些考古，翻一翻从 1.8 到现在 CPU Manager 的发展过程，当然下面也会做简单介绍。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:1:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"发展历程 CPU Manager 作为 alpha 特性引入 Kubernetes 1.8 版本，1.12 开始转换为 beta 状态。如何使用，参数配置不是这里的重点，可以参考官网。在此基础上还有 NUMA Topology Aware 的能力，可以参考前篇。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:2:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"问题表现 绑核的功能实现是在 kubelet 当中的，在容器启动之前会经过 admit 机制进行校验宿主上是否有足够资源供绑核使用。如果资源不足，则容器准入失败，会报错提示 cpu 资源不足，not enough cpus available to satisfy request。 此问题存在于1.8之后的所有版本中，所以如果在线上遇到的话不要惊讶，一直在修复，从未被彻底修复，这可能也是为什么直到现在仍然处于 beta 状态的原因。针对此现象的所有的 PR 都只是对已知原因的修复，且当前仍存在原因已知但尚未修复的问题。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:3:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"解决方案 根据我的经验，遇到此类问题最直接有效的解决方案分三步，就如同把大象装冰箱一样简单： 停止 Kubelet 进程 删除本地 cpu_manager_state 文件 重启 Kubelet 进程 可以解决99%的此类问题，如果还是无法解决且 k8s 版本 \u003c 1.18，那就需要祭出更厉害的武功秘籍了： docker ps 查找同 Pod 同 Conatiner Name（.spec.containers 中同一个 name 的 container）的记录，大概率会存在多条记录，例如存在 Created 状态的 container，这时需要删除这些个多余的 container 按照上面的1，2，3再来一次即可 如果还是没有解决，那么恭喜你，你可能遇到未知原因的问题了，希望你可以深入排查并反馈给社区，帮助更多受害者。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:4:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"原理介绍 上图是最新版本 k8s 中的实现，在不同 k8s 版本中实现方式不同，上图中的虚线框中的部分在低版本中是不存在的。在低版本中，计算绑核信息以及设置绑核信息到容器是在 Reconsile 和 PreStartContaier 中实现的，而现版本是在 Reconsile 和 Admit、PreCreateContainer 实现的，即在 Admit 时会计算出容器所需资源并保存在内存中，在真正调用 Docker 之前，从内存中获取到容器绑核信息并设置到其 Config 中，然后传递给 Docker，而老版本中是先创建出来容器，然后再调用 Docker API 去更新其绑核信息。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:5:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"相关ISSUE \u0026 PR ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:6:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"ISSUE 感兴趣的可以翻翻历史 ISSUE，此问题从 1.8 开始有大量相关的 ISSUE，下面列几个比较典型的 Internal PreStartContainer hook failed: not enough cpus available to satisfy request #63018 [cpumanager] AddContainer error: not enough cpus available to satisfy request #79159 TopologyManager: Guarantee Aligned resources for Multiple Containers #83476 Container cpuset lost, apparently due to race between PostStopContainer() and new container creation #90303 The CPU manager does not work correctly for the guaranteed pod with multiple containers #103952 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:6:1","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"PR Make CPU manager release CPUs when Pod enters completed phase #52363 cpumanager: rollback state if updateContainerCPUSet failed #67430 clean containers in reconcileState of cpuManager #68619 Update CPUManager stored state semantics #84462 Fix exclusive CPU allocations being deleted at container restart #90377 Do not clear state of pods pending admission for CPU/Memory/Device manager #103979 Slack 上有一段专门针对 CPU Manager 问题的讨论，可以加深对问题的理解，见这里。 其中针对1.22及以上版本绑核相关的 bug 和 kubelet 重构有关，参考这里。在 1.22 开始 kubelet 进行了很大的重构，参考 Commit，在此 PR 合入之后出现了一些相关的 Bug 以及 Failing Test 的错误，针对其想要实现的功能也存在一个文档。这里不再多说，针对上述 Commit 和 设计文档会再另开一篇来介绍，因为其本质和 CPU Manager 无关，但是是非常重大的一个改变。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:6:2","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"已知问题 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:7:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"1.18 之前的版本 需要根据具体使用的 k8s 版本确定有哪些修复的 PR 还没有合入。 即使所有 PR 都已经合入了，也还是可能遇到问题的。原因如下：当遇到机器异常，如 docker 异常、load 高、io 使用率高时问题出现概率会增加，根本原因就在于 1.18 之前的版本 cpu_manager_state 文件对应的数据结构为 map[ContainerID]resource，也就是说他是以 container ID 作为 Key 的，这也就是为什么会有上面解决方案中提到更厉害的的秘籍，因为同 Pod，同 Container Name 存在多个 Container，每个都有自己的 ID，但逻辑上应该只记录一个到 cpu_manager_state 中，但实际上记录了多个。 在 1.18 之后，cpu_manager_state 文件数据结构发生改变，变为map[PODUID]map[ContainerName]resource，就可以避免出现同 Pod 同 Container Name 的容器占用多份资源的问题。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:7:1","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"所有版本 对于强制删除的Pod，如果在其删除过程中遇到某些原因导致 Container 无法删除导致其内存和 cpu_manager_state 中记录的信息与实际使用不符时，也可能会遇到此问题。这种问题理论上通过简单的3步解决方案即可解决。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:7:2","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["源码分析","问题排查"],"content":"总结 如果一定要使用绑核功能，请尽量使用 1.18 及以上版本，同时当前最新版 1.23 尚未正式 release，且 1.22 开始对 Kubulet 进行了部分重构，存在大量已知问题，建议采用 1.18 ~ 1.21 中的版本。如果还是遇到此问题，参考上述解决方案。 ","date":"2021-10-16","objectID":"https://www.likakuli.com/posts/kubernetes-cpu-manager/:8:0","tags":["kubernetes"],"title":"那些年，我们一起追的Bug","uri":"https://www.likakuli.com/posts/kubernetes-cpu-manager/"},{"categories":["随笔"],"content":"机缘巧合 去年一个偶然的机会在抖音上刷到了\"姜胡说\"，里面不乏一些有趣且发人深思的内容，在其中一期中胡子提到了几本书，其中包含《思考 快与慢》，又名《慢思快行》，作者丹尼尔 · 卡尼曼，诺贝尔经济学奖获得者，行为经济学的创始人之一。本书吸引我的地方在于其中涉及到了大量的心理学理论、心理学实验等内容，分析了人思考的过程，这是最吸引我的地方。之前也尝试阅读过一些关于沟通技巧的书，例如《非暴力沟通》，国内外有大量相关的书，但基本都属于在介绍沟通技巧，比较好的里面会设置一些实际案例或者介绍一下相关的心理学效应，但是读完始终都感觉缺少点什么。这种感觉有点类似虽然我解决了问题，但是我并不知道问题发生的根因是什么，如何避免问题的发生，如何做到举一反三，以不变应万变等。 ","date":"2021-07-10","objectID":"https://www.likakuli.com/posts/thoughtfulessay/:1:0","tags":["思考","随笔"],"title":"思想随笔1 - 思考思考的过程","uri":"https://www.likakuli.com/posts/thoughtfulessay/"},{"categories":["随笔"],"content":"迷之自信 书中把人类的思考模式拆分成快思考和慢思考（系统1和系统2）两个系统，系统1和系统2并不是真实存在的实体，也不是大脑中某个固定的部位，它们是两种不同的思考模式。之所以叫系统1和系统2，只是为了理解起来更方便。 系统1是依赖直觉的、无意识的思考系统，系统2是需要主动控制的、有意识进行的思考系统。系统1是直觉系统，运行起来速度快，不怎么消耗脑力，不用意识控制，可以称为快思考。系统2是非直觉系统，有意识进行，需要保持足够的专注，主动控制，可以称为慢思考。 在主观上，我们往往觉得自己是理性的。但实际上，我们的大多数行为都是在系统1的指引下，在无意识之间完成的。因为系统2需要集中注意力，但人类大脑又是天生有惰性的，不愿意多付出这些注意力（最省力原则）。在这种惰性之下，系统2往往会对系统1的直观判断无条件接受。尤其是在人精力不足的情况下，系统2会更加弱势。系统1的直觉式快思考，也就成为了人们决策和行为的真正主宰。而系统1自身存在的缺陷，容易导致人在思考过程中出现偏见和错误。而且我们不仅会出现偏见与错误，还容易直接忽视自己会出现偏见与错误这件事本身，产生迷之自信。 书中提到了很多造成系统1出现偏见和错误的现象，如典型性偏好、可得性偏好、因果性偏好、光环效应、锚定效应、框架效应、禀赋效应等，得出这些结论所涉及到的一些实验也是特别的有意思。 ","date":"2021-07-10","objectID":"https://www.likakuli.com/posts/thoughtfulessay/:2:0","tags":["思考","随笔"],"title":"思想随笔1 - 思考思考的过程","uri":"https://www.likakuli.com/posts/thoughtfulessay/"},{"categories":["随笔"],"content":"本篇由来 本篇并不是读后感，在这里是想记录分享一些自己这段时间遇到的一些比较有意思的事情，印证书中的部分观点：人类思考天然存在缺陷，不可避免的会出现偏见与错误。意识到这些问题对于我们在做决策、沟通、做事等方面会有很大的帮助。接下来聊几个有意思的事。 ","date":"2021-07-10","objectID":"https://www.likakuli.com/posts/thoughtfulessay/:3:0","tags":["思考","随笔"],"title":"思想随笔1 - 思考思考的过程","uri":"https://www.likakuli.com/posts/thoughtfulessay/"},{"categories":["随笔"],"content":"迪加泰罗 有一首比较火的歌曲，名字叫《踏山河》，可能大家对名字比较陌生，但是应该很多人都听过，其高潮部分里面有几句歌词： 而我枪出如龙 乾坤撼动 一啸破苍穹 长枪刺破云霞 放下一生牵挂 望着寒月如牙 孤身纵马 生死无话 我儿子也特别喜欢这一段，但是从他嘴里唱出来的调是这个调，但词就完全听不出来是啥了，我问他啥歌词，他说啥奥特大英雄，迪加泰罗，叽里呱啦啥啥啥的，很多听不懂的话。还给我解释迪迦是啥，泰罗是啥，我虽然活了30岁了，但还真没看过奥特曼。我对他唱的歌词一直的印象就是还挺能编的，挺顺口的还，直到我听到奥特曼版本的踏山河才知道，还真有这个歌词，而且存在多个版本。这个时候才意识到他是\"对\"的，我之前对他的印象 - 觉得他自己瞎编的是\"错\"的。他口中那些个叽里呱啦的可能对应的就是其中的日语的一部分，搞得我这个好歹日语也过了N2的都听不懂，汗颜啊。 回过头来想一想，为啥我的反应会是歌词是他瞎编的呢？ 因为儿子经常用平板看奥尔曼片段，在他的歌词中涉及到了奥特曼，加之在他这个年纪（4岁多），在我得出他是在瞎编的结论时，几乎不需要思考，可以说是就像条件反射一样。这可以用可得性偏好、因果性偏好解释。 可得性偏好：如果一件事情更容易出现在大脑里，人们就认为这件事情更容易发生。 因果性偏好：喜欢对事物进行因果关系解释，而不管是不是解释是不是合理。 因为他四岁，我就觉得他是编的，因为他经常看奥特曼，我就觉得他是编的。在我对他做出判断时，我的认知里根本不存在奥特曼版踏山河，更不会有人叫他这个歌词，“这是他编歌词\"的这个事情直接出现在我的大脑里，因此我毫不费力的对他的这种行为做出了一个无意识的错误的判断。 ","date":"2021-07-10","objectID":"https://www.likakuli.com/posts/thoughtfulessay/:3:1","tags":["思考","随笔"],"title":"思想随笔1 - 思考思考的过程","uri":"https://www.likakuli.com/posts/thoughtfulessay/"},{"categories":["随笔"],"content":"他山之石 前一段时间换了份工作，在新公司的一些经历”促使“我对在上一家的公司的一些工作重新做了总结，对某些事有了重新的认识，虽然在换工作时自己也曾总结每份工作中自己的成长收获以及需要改进的地方，但总感觉效果没有那么好，没有那么深刻，或者会忽视很多问题与收获，所谓”当局者迷“。 前几天朋友圈看到聪哥分享的知乎上的一篇短文：那时候我还不明白。摘取一段其内容： 念过太多太多的诗词，读过太多太多的故事，哪怕已经深深地刻在脑上，可依旧是不明白。 不明白同叔为何要望尽天涯路，不明白稼轩为何会道天凉好个秋。 不明白幽梦里相对无言唯有泪千行的思忆之痛，不明白僧庐下夜听细雨点滴到天明的黍离之悲。 不明白项脊轩中亭亭如盖的枇杷树，不明白姑苏城外独对愁眠的渔火钟声。 课本的注释写得再清晰，老师的讲解说得再透彻，也只勉强读得懂词中意，奈何参不透句中情。 是啊，年少之时，又怎么会懂丧子亡妻，怎么会懂国破家亡，怎么会懂人鬼殊途，怎么会懂名落孙山。　直到一天，书里的故事变成了人生，莫名悲痛涌上心头，不能自已，才大彻大悟，醍醐灌顶，仿似大梦初醒。如梦幻泡影，如露亦如电。 卡尼曼认为，系统2是懒惰的，要让这个懒惰的系统变得勤快起来，就需要刻意的提示，可以是自我提示，也可以是来自于外部的提示。这两种方式相比较，外部提示往往是更加有效的。 世界很奇妙，不同领域的一些结论总有惊人的相似之处。例如 电影中也经常可以看到类似桥段，被蜘蛛咬了才有了蜘蛛侠；《功夫中》的周星驰虽然是百年不遇的武学奇才，但也是在被火云邪神打的连他妈都不认识之后，无意中打通任督二脉才破茧成蝶。 高中物理中学到的\"力是改变物体运动状态的原因，而不是维持物体运动的原因”。 热力学中提到的\"熵\"，即根据热力学第一定律与第二定律，能量是守恒的，可以互相转化，而不会消失，但是无法100%利用。在转化过程中，总是有一部分能量会被浪费掉，公式如下 能量的总和 = 有效能量 + 无效能量 “熵\"就是无效能量，是\"无序化\"的度量。“无序化\"代表着混乱，可以得到三个重要结论： 如果没有外部能量输入，封闭系统趋向越来越混乱（熵越来越大）。 如果要让一个系统变得更有序，必须有外部能量的输入 当一个系统（或部分）变得更加有序，必然有另一个系统（或部分）变得更加无序，而且\"无序\"的增加程度将超过\"有序\"的增加程度。 等等等等… ","date":"2021-07-10","objectID":"https://www.likakuli.com/posts/thoughtfulessay/:3:2","tags":["思考","随笔"],"title":"思想随笔1 - 思考思考的过程","uri":"https://www.likakuli.com/posts/thoughtfulessay/"},{"categories":["随笔"],"content":"饮水机旁的闲谈 我们经常会面临选择与决策，现在我们已经开始逐渐意识到我们的思考是存在缺陷的，但是可以借助外部力量来帮我们纠正。当我们身处一个机构之中，身边就会有很多旁观者。这些旁观者会用来自于他们系统2的慢思考，来帮你纠正自己系统1的快思考可能导致的错误。所以，集体讨论决策虽然会有效率不高的问题，但在很多情况下却还是有意义的，因为它可以启动很多人的慢思考，减少快思考可能带来的偏见与失误。 书中提到了饮水机闲谈，我们可以到比较轻松的环境，听听大家的闲谈和批评。去面对这些批评虽然不容易，但是比起自己鼓起勇气自我批评，还是容易得多。而无论是听别人的闲言碎语，还是自我反思自我批评，目的都是一个，让思考慢下来，让系统2运转起来，尽量避免系统1的直觉思维和快思考，可能给我们带来的偏见和失误。 先让思考慢下来 曾子说：吾日三省吾身，为人谋而不忠乎？与朋友交而不信乎？传不习乎？ 季文子三思而后行。子闻之，曰：“再，斯可矣。” 赤壁之战中孙权面对众文官的极力劝降说出了：此事容我三思。 再借助他山之石 三人行，必有我师焉。择其善者而从之，其不善者而改之 他山之石，可以为错；他山之石，可以攻玉 以铜为镜，可以正衣冠，以古为镜，可以见兴替，以人为镜，可以知得失 共勉 ","date":"2021-07-10","objectID":"https://www.likakuli.com/posts/thoughtfulessay/:4:0","tags":["思考","随笔"],"title":"思想随笔1 - 思考思考的过程","uri":"https://www.likakuli.com/posts/thoughtfulessay/"},{"categories":["原理介绍"],"content":"由来 最近的工作内容中涉及到了 NUMA 感知相关的功能，之前没有特意去看过 kubelet 相关部分的实现，也是趁此机会把落下的补补。在看代码的过程中，NUMA 感知部分的逻辑尤其涉及到一些位操作的部分，看的让人头疼，于是从网上搜了搜有关原理的介绍，恰好在官网找到一篇 blog，看完之后再去看代码就会豁然开朗。此篇是对原文的翻译，想阅读原文的可以直接到这里。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:1:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"TopologyManager TopologyManager 在1.18版本中处于 beta 状态，该功能支持 CPU 和外围设备（例如 SR-IOV VF 和 GPU）的 NUMA 对齐，使工作负载能够在针对低延迟优化的环境中运行。 在引入 TopologyManager 之前，CPU 和设备管理器会做出相互独立的资源分配决策。这可能会导致 Multi-Socket 系统上出现不希望的分配，降低延迟敏感应用的性能。随着 TopologyManager 的引入，我们现在有办法避免这种情况。这篇blog包括如下内容： NUMA 及其重要性简介 用户可使用的用于保证 CPU 和外围设备 NUMA 对齐的策略 TopologyManager 内部工作原理 TopologyManager 当前限制 TopologyManager 未来发展规划 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:2:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"什么是NUMA \u0026 为什么需要关心？ NUMA 是 Non-Uniform Memory Access 的简称。它是一种在多 CPU 系统上可用的技术，允许不同的 CPU 以不同的速度访问内存的不同部分。 任何直接连接到 CPU 的内存都被认为是该 CPU 的本地内存，并且可以非常快速地访问。 任何未直接连接到 CPU 的内存都被认为是非本地的。在现代系统上，本地与非本地内存的概念也可以扩展到外围设备，例如 NIC 或 GPU。 为了获得高性能，应该分配 CPU 和设备，以便它们可以访问相同的本地内存。 NUMA 系统上的所有内存都分为一组NUMA 节点，每个节点代表一组 CPU 或设备的本地内存。 如果单个 CPU 的本地内存与该 NUMA 节点相关联，则我们将其称为 NUMA 节点的一部分。 基于访问外围设备时所必须通过的最短互连数量，我们将外围设备视为 NUMA 节点的一部分。 例如，在图 1 中，CPU 0-3 被称为 NUMA node 0 的一部分，而 CPU 4-7 是 NUMA node 1 的一部分。同样，GPU 0 和 NIC 0 被称为 NUMA node 0 的一部分，因为它们 连接到 Socket 0，其 CPU 都是 NUMA node 0 的一部分。 NUMA node 1 上的 GPU 1 和 NIC 1 也是如此。 尽管上面的示例显示了 NUMA 节点到 Socket 的 1-1 映射，但在一般情况下不一定如此。 单个 NUMA 节点上可能有多个 Socket，或者单个 Socket 的单个 CPU 可能连接到不同的 NUMA 节点。 此外，Sub-NUMA Clustering（在最近的英特尔 CPU 上可用）等新兴技术允许单个 CPU 与多个 NUMA 节点相关联，只要它们对两个节点的内存访问时间相同（或差异可以忽略不计）。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:3:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"NUMA 对齐 TopologyManager 提供了如下几种对齐策略： none：此政策不会尝试进行任何资源调整。 它的行为就像 TopologyManager 根本不存在一样。 这是默认策略。 best-effort：使用此策略，TopologyManager 将尝试尽可能地对齐 NUMA 节点上的分配，但即使某些分配的资源未在同一 NUMA 节点上对齐，也会始终允许 pod 启动。 restricted：此策略与尽力而为策略相同，但如果分配的资源无法正确对齐，它将导致 pod 准入失败。 与 single-numa-node 策略不同，如果不可能在单个 NUMA 节点上满足分配请求，则某些分配可能来自多个 NUMA 节点。 single-numa-node：这个策略是最严格的，只有当所有请求的 CPU 和设备都可以从一个 NUMA 节点分配时，pod 准入才会通过。 需要注意的是，所选策略单独应用于 pod 规范中的每个容器，而不是将所有容器中的资源对齐在一起。 此外，单个策略通过全局 kubelet 标志应用于节点上的所有 pod，而不是允许用户逐个 pod（或逐个容器）选择不同的策略。 我们希望在未来放宽这一限制。 kubelet可供设置的策略如下： --topology-manager-policy= [none | best-effort | restricted | single-numa-node] 通过feature gate来控制功能的开启，于1.16引入，自1.18开始默认开启，形式如下： --feature-gates=\"...,TopologyManager=\u003ctrue|false\u003e\" 为了根据所选策略触发对齐，用户必须根据特定要求在其 pod.spec 中设置 CPU 和 device 的request值。 对于外围设备，这意味着从设备插件（例如 intel.com/sriov、nvidia.com/gpu 等）提供的可用资源中请求设备。 这仅在设备插件与 TopologyManager 正确集成时才有效。 目前，已知具有此扩展的唯一插件是 Nvidia GPU 设备插件和英特尔 SRIOV 网络设备插件。 可以在此处找到有关如何扩展设备插件以与 TopologyManager 集成的详细信息。 对于 CPU，这要求 CPUManager 已配置为启用了其 –static 策略，并且 pod 在保证 QoS 类中运行（即所有 CPU 和内存限制都等于它们各自的 CPU 和内存请求）。 还必须以整数值（例如 1、2、1000m 等）请求 CPU。 可以在此处找到有关如何设置 CPUManager 策略的详细信息。 例如，假设 CPUManager 在其 –static 策略启用的情况下运行，并且 gpu-vendor.com 和 nic-vendor.com 的设备插件已扩展为与 TopologyManager 正确集成，则下面的 pod 规范足以触发 TopologyManager 运行其选定的策略： spec: containers: - name: numa-aligned-container image: alpine resources: limits: cpu: 2 memory: 200Mi gpu-vendor.com/gpu: 1 nic-vendor.com/nic: 1 遵循上一节中的图 1，这将导致以下对齐分配之一： {cpu: {0, 1}, gpu: 0, nic: 0} {cpu: {0, 2}, gpu: 0, nic: 0} {cpu: {0, 3}, gpu: 0, nic: 0} {cpu: {1, 2}, gpu: 0, nic: 0} {cpu: {1, 3}, gpu: 0, nic: 0} {cpu: {2, 3}, gpu: 0, nic: 0} {cpu: {4, 5}, gpu: 1, nic: 1} {cpu: {4, 6}, gpu: 1, nic: 1} {cpu: {4, 7}, gpu: 1, nic: 1} {cpu: {5, 6}, gpu: 1, nic: 1} {cpu: {5, 7}, gpu: 1, nic: 1} {cpu: {6, 7}, gpu: 1, nic: 1} 只需遵循此模式，即可让 TopologyManager 确保请求拓扑感知设备和独占 CPU 的容器之间的 NUMA 对齐。 注意： 如果 Pod 被 TopologyManager 策略之一拒绝，它将被置于 Terminated 状态，并出现 Pod 准入错误和 TopologyAffinityError 的原因。 一旦 pod 处于此状态，Kubernetes 调度程序将不会尝试重新调度它。 因此，建议使用带 replicas 的 deployment 来应对在遇到此类故障时触发 pod 的重新部署。 还可以实施外部控制循环来触发具有 TopologyAffinityError 的 pod 的重新部署。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:4:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"实现机制 TopologyManager 执行的主要逻辑的伪代码如下所示： for container := range append(InitContainers, Containers...) { for provider := range HintProviders { hints += provider.GetTopologyHints(container) } bestHint := policy.Merge(hints) for provider := range HintProviders { provider.Allocate(container, bestHint) } } 下图总结了此循环期间采取的步骤： 步骤说明如下： 循环遍历 pod 中的所有容器。 对于每个容器，从容器请求的每种拓扑感知资源类型（例如 gpu-vendor.com/gpu、nic-vendor.com/nic、cpu 等）的一组 HintProviders 中收集 TopologyHints。 使用选定的策略，合并收集到的 TopologyHints 以找到在所有资源类型之间对齐资源分配的最佳提示。 再次遍历 HintProviders，利用上一步中返回的最佳提示为容器分配资源。 此循环在 pod 准入时间运行，如果这些步骤中的任何一个失败或根据所选策略无法满足对齐，则Pod注入失败。 相应地清除之前分配的任何资源。 以下部分更详细地介绍了 TopologyHints 和 HintProviders 的确切结构，以及每个策略使用的合并策略的一些细节。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:5:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"TopologyHints TopologyHint 对一组约束进行编码，从中可以满足给定的资源请求。 目前，我们考虑的唯一约束是 NUMA 对齐。 它的定义如下： type TopologyHint struct { NUMANodeAffinity bitmask.BitMask Preferred bool } NUMANodeAffinity 字段包含可以满足资源请求的 NUMA 节点的位掩码。 例如，具有 2 个 NUMA 节点的系统上可能的掩码包括： {00}, {01}, {10}, {11} Preferred 字段包含一个布尔值，用于指示指定的提示是否优先选择。 使用 best-effort 策略，在生成最佳提示时，Preferred 提示将优先于非 Preferred 提示。 使用 restricted 和单 numa-node 策略，非 Preferred 的提示将被拒绝。 通常，HintProviders 通过查看可以满足资源请求的当前可用资源集来生成 TopologyHints。 更具体地说，它们为可以满足资源请求的 NUMA 节点的每个可能掩码生成一个 TopologyHint。 如果掩码不能满足请求，则将其省略。 例如，当被要求分配 2 个资源时，HintProvider 可能会在具有 2 个 NUMA 节点的系统上提供以下提示。 这些提示显示两种资源可以来自单个 NUMA 节点（0 或 1），也可以分别来自不同的 NUMA 节点（但我们更希望它们仅来自一个）。 {01: True}, {10: True}, {11: False} 目前，当且仅当 NUMANodeAffinity 编码可以满足资源请求的最小 NUMA 节点集时，所有 HintProvider 才将 Preferred 字段设置为 True 。 通常，这仅对于在其位掩码中设置了单个 NUMA 节点的 TopologyHint 为 True。 但是，如果满足资源请求的唯一方法是跨越多个 NUMA 节点（例如，请求 2 个设备并且系统上仅有的 2 个设备位于不同的 NUMA 节点上），则它也可能为 True： {0011: True}, {0111: False}, {1011: False}, {1111: False} 注意：以这种方式设置 Preferred 字段不是基于当前可用资源的集合。 它基于在一些最小的 NUMA 节点集上物理分配请求资源数量的能力。 通过这种方式，如果在其他容器释放其资源之前无法满足实际的首选分配，则 HintProvider 可以返回所有 Preferred 字段设置为 False 的提示列表。 例如，请考虑图 1 中系统的以下场景： 目前除分配给容器的CPU之外只剩2 CPU空闲 剩余的 2 CPU 位于不同的 NUMA 节点上 一个新的容器出现，要求 2 CPU 在这种情况下，唯一生成的提示将是 {11: False} 而不是 {11: True}。 发生这种情况是因为可以从该系统上的同一个 NUMA 节点分配 2 个 CPU（鉴于当前的分配状态，现在不是这样）。 这个想法是，当可以满足最小对齐时，最好让 pod 准入失败并重试部署，而不是允许以次优对齐来调度 pod。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:5:1","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"HintProviders HintProvider 是 kubelet 内部的一个组件，用于协调与 TopologyManager 对齐的资源分配。 目前 Kubernetes 中唯一的 HintProvider 是 CPUManager 和 DeviceManager。 我们计划很快添加对 HugePages 的支持。 如前所述，TopologyManager 从 HintProviders 收集 TopologyHints，使用合并后的最佳提示触发对齐的资源分配。 因此，HintProviders 实现了以下接口： type HintProvider interface { GetTopologyHints(*v1.Pod, *v1.Container) map[string][]TopologyHint Allocate(*v1.Pod, *v1.Container) error } 请注意，对 GetTopologyHints() 的调用返回一个 map\\[string][]TopologyHint。 这允许单个 HintProvider 为多种资源类型提供提示，而不仅仅是一种。 例如，DeviceManager 需要这样做，以便为其插件注册的每个资源类型传回提示。 当 HintProviders 生成他们的提示时，他们只考虑如何满足系统上当前可用资源的对齐，不考虑已分配给其他容器的任何资源。 例如，考虑图 1 中的系统，其中有以下两个容器从中请求资源： 如果 Container0 是在系统上分配的第一个容器，则将为规范中的三种拓扑感知资源类型生成以下提示集： cpu: [{01: True}, {10: True}, {11: False}] gpu-vendor.com/gpu: [{01: True}, {10: True}] nic-vendor.com/nic: [{01: True}, {10: True}] 对应的对齐结果： {cpu: {0, 1}, gpu: 0, nic: 0} 当考虑 Container1 时，这些资源被假定为不可用，因此只会生成以下提示集： cpu: [{01: True}, {10: True}, {11: False}] gpu-vendor.com/gpu: [{10: True}] nic-vendor.com/nic: [{10: True}] 对其结果如下： {cpu: {4, 5}, gpu: 1, nic: 1} 注意：与本节开头提供的伪代码不同，对 Allocate() 的调用实际上并没有直接采用一个合并后的最佳提示的参数。 相反，TopologyManager 实现了以下 Store 接口，HintProviders 可以查询该接口以检索为特定容器生成的提示： type Store interface { GetAffinity(podUID string, containerName string) TopologyHint } 将其分离到其自己的 API 调用中，允许人们在 pod 准入循环之外访问此提示。 这对于调试以及通过 kubectl 等工具获取生成的提示很有用。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:5:2","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"Policy.Merge 由给定策略定义的合并策略决定了它如何将所有 HintProvider 生成的一组 TopologyHint 组合成单个 TopologyHint。 所有受支持策略的合并策略都以相同的方式开始： 取为每种资源类型生成的 TopologyHints 的叉积 对于叉积中的每个条目，将每个 TopologyHint 的 NUMA Affinity进行按位与计算，并将结果设置为到合并提示的 NUMA Affinity。 如果条目中的所有提示都将 Preferred 设置为 True ，则在生成的合并提示中将 Preferred 设置为 True。 如果即使条目中的一个提示已将 Preferred 设置为 False ，生成的合并提示中 Preferred为 False 。 如果其 NUMA Affinity 包含全 0，则合并提示中 Preferred 也为 False。 按照上一节的示例，Container0 生成的提示为： cpu: [{01: True}, {10: True}, {11: False}]F gpu-vendor.com/gpu: [{01: True}, {10: True}] nic-vendor.com/nic: [{01: True}, {10: True}] 上述算法产生以下叉积和合并后的提示： {cpu, gpu-vendor.com/gpu, nic-vendor.com/nic} “merged” hint [{01: True}, {01: True}, {01: True}] {01: True} [{01: True}, {01: True}, {10: True}] {00: False} [{01: True}, {10: True}, {01: True}] {00: False} [{01: True}, {10: True}, {10: True}] {00: False} [{10: True}, {01: True}, {01: True}] {00: False} [{10: True}, {01: True}, {10: True}] {00: False} [{10: True}, {10: True}, {01: True}] {00: False} [{10: True}, {10: True}, {10: True}] {01: True} [{11: False}, {01: True}, {01: True}] {01: False} [{11: False}, {01: True}, {10: True}] {00: False} [{11: False}, {10: True}, {01: True}] {00: False} [{11: False}, {10: True}, {10: True}] {10: False} 一旦合并提示列表生成，剩下的工作就由特定的 TopologyManager 策略来决定将哪个提示视为最佳提示。 一般来说，这包括： 按“狭窄程度”对合并的提示进行排序。 窄度定义为在提示的 NUMA Affinity 掩码中设置的位数。 设置的位越少，提示越窄。 对于在其 NUMA Affinity 掩码中设置相同位数的提示，具有最低位设置的提示被认为更窄。 按 Preferred 字段对合并的提示进行排序。 与将 Preferred 设置为 False 的提示相比，将 Preferred 设置为 True 的提示更可能被视为候选对象。 选择具有最佳设置的最窄提示。 在 best-effort 策略的情况下，此算法将始终导致某些提示被选为最佳提示，最终Pod准入通过。 HintProviders 根据这个最佳提示进行资源分配。 但是，在 restricted 和 single-numa-node 策略的情况下，任何选择的 Preferred 设置为False 的提示将被立即拒绝，导致pod 准入失败并且无法资源分配。 此外 single-numa-node 还将拒绝在其 Affinity 掩码中设置了多个 NUMA 节点的提示。 在上面的示例中，所有策略都会使用 {01: True} 提示，Pod准入成功。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:5:3","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"Upcoming enhancements 虽然 1.18 版本和 Beta 版升级带来了一些重要的增强和修复，但仍然存在许多限制，如下所述。 我们已经在努力解决这些限制以及更多问题。 本节将介绍我们计划在不久的将来为 TopologyManager 实施的一组增强功能。 这个列表并不详尽，但它很好地说明了我们前进的方向。它按照我们期望看到每个增强完成的时间范围进行排序。 如果您想参与其中的任何增强功能，请参加每周一次的 Kubernetes SIG-node 会议以了解更多信息并成为社区工作的一部分！ ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:6:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"Supporting device-specific constraints 目前，NUMA 亲和性是 TopologyManager 为资源对齐考虑的唯一约束。 此外，可以对 TopologyHint 进行的唯一扩展涉及 node-level 约束，例如跨设备类型的 PCIe 总线对齐。 尝试向该结构添加任何特定于设备的约束（例如，一组 GPU 设备之间的内部 NVLINK 拓扑）将是棘手的。 因此，我们建议对设备插件接口进行扩展，允许插件声明其拓扑感知分配首选项，而无需向 kubelet 公开任何特定于设备的拓扑信息。 通过这种方式，TopologyManager 可以被限制为仅处理常见的节点级拓扑约束，同时仍然可以将特定于设备的拓扑约束合并到其分配决策中。 可以在此处找到此提案的详细信息，并且将很快在 Kubernetes 1.19 中提供。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:6:1","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"NUMA alignment for hugepages 如前所述，TopologyManager 当前仅有的两个 HintProvider : CPUManager 和 DeviceManager。 但是，目前正在增加巨页的支持。 随着这项工作的完成，TopologyManager 最终将能够在同一个 NUMA 节点上分配内存、hugepages、CPU 和 PCI 设备。 这项工作的 KEP 目前正在审查中，并且正在制作原型以尽快实现此功能。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:6:2","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"Scheduler awareness 目前，TopologyManager 充当 Pod 准入控制器，它不直接参与 pod 的调度决策。相反，当 kubernetes 调度程序（或在部署中运行的任何调度程序）将 pod 放置在节点上运行时，TopologyManager 将决定是“接纳”还是“拒绝”该 pod。如果 Pod 由于缺乏可用的 NUMA 对齐资源而被拒绝，事情会变得有点有趣。这个 kubernetes issue 很好地突出并讨论了这种情况。 那么我们如何着手解决这个限制呢？我们可以利用 Kubernetes 调度框架来实现！该框架提供了一组新的插件 API，这些 API 与现有的 Kubernetes 调度程序集成，并允许实现调度功能，例如 NUMA 对齐。 如何实现这些扩展与 TopologyManager 集成的细节尚未制定。我们仍然需要回答以下问题： 我们是否需要重复的逻辑在 TopologyManager 和调度程序中决定设备的亲和性？ 我们是否需要一个新的 API 来从 TopologyManager 获取 TopologyHints 到调度程序插件？ 此功能的工作应在接下来的几个月内开始，敬请期待！ ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:6:3","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"Per-pod alignment policy 如前所述，单个策略通过全局 kubelet 标志应用于节点上的所有 pod，而不是允许用户逐个 pod（或逐个容器）选择不同的策略。 虽然我们同意这将是一个很棒的功能，但在实现之前还有很多障碍需要克服。 最大的障碍是此增强功能需要更改 API 才能在 Pod spce 或其关联的 RuntimeClass 中表达所需的对齐策略。 我们现在才开始认真讨论这个功能，它离可用还有几个版本。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:6:4","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["原理介绍"],"content":"Conclusion 随着 TopologyManager 在 1.18 中升级为 Beta，我们鼓励大家尝试并期待您的任何反馈。 在过去的几个版本中进行了许多修复和增强，大大提高了 TopologyManager 及其 HintProviders 的功能和可靠性。 虽然仍然存在许多限制，但我们计划进行一系列增强来解决这些问题，并期待在即将发布的版本中为您提供许多新功能。 如果您有其他增强功能的想法或对某些功能的渴望，请随时告诉我们。 团队始终对增强和改进 TopologyManager 的建议持开放态度。 ","date":"2021-06-18","objectID":"https://www.likakuli.com/posts/kubernetes-topology-manager/:7:0","tags":["kubernetes"],"title":"Kubenetes NUMA拓扑感知功能介绍","uri":"https://www.likakuli.com/posts/kubernetes-topology-manager/"},{"categories":["性能优化"],"content":"从事Kubernetes相关工作的同学对Kube-scheduler一定不会感到陌生，有的甚至还可能遇到过里面的一些问题，本篇主要介绍其中的一个优选策略：InterPodAffinity的性能优化过程，希望可以帮助到一些还在深受其困扰的朋友们，没有使用过此策略或者没有使用过Kubernetes也不要紧，其本质还是在以某种算法去解决某种问题，下面我会尽量以通俗易懂的话来解释需要解决的问题及算法优化过程。 ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:0:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"策略介绍 InterPodAffinity作为优选策略就是在预选通过后的一堆宿主中找到一个最合适的宿主，寻找规则为按拓扑把宿主分类，针对拓扑级别，如果出现符合Pod要求的其他Pod，则为此拓扑下的所有Node计算一个得分，Pod的要求可以有多条，每一条都有一个权重（用来计算得分），同时得分也分正负，例如Pod期望拓扑下没有同类型的Pod，如果有则得分为负。下面通过一下生活中的例子来做个类比帮助理解要解决的问题，再引出对应的算法。 ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:1:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"举个🌰 张三（Pod）大学刚毕业参加工作，需要租房（Node），于是找了中介公司（Kube-scheduler），张三提出了一些条件，比如房租不能高于2000一个月，上班耗时不超过1小时等，中介公司根据张三的需要筛选出来一批房子，张三看后觉得都不错，但只能租一个，于是又列出了一些其他非必须的加分减分项，比如房子所在小区如果有人是明星的话，加8分，如果房子所在街道有新冠肺炎患者，则减10分等条件，中介公司需要根据这些条件筛选出来最适合张三的一套房子。 上面提到的无论是小区还是街道都是一种拓扑，都可以看做是房子的一个属性并且有对应的值，例如小区：永泰东里，街道：西三旗街道。这些附加的加减分项是基于拓扑的，具有相同拓扑的房子内只要出现一个满足条件的人，则整个拓扑内的房子都会受到影响。例如小区内有一个明星，则整个小区符合条件的房子都会加8分。若街道有一个新冠患者，则整个街道所有符合条件的房子都会减10分。 化简一下为有M个符合条件的房源，每个房源里面住N个人（N可以不同），有X个加分减分项，求得分最高的一个房源。 ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:2:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"初代 ~1.15.4 InterPodAffinity采用的算法比较简单暴力，伪代码大致如下 for i房源 in 所有房源 for j租客 in i房源 for k项 in 加减分项 if j租客 满足 k项 for l房源 in 所有房源 if 拓扑相等(l房源, i房源) lock() map[l] += k项分数 unlock() 整体上是一个四层的循环，时间复杂度为M * N * X * M，工程中实现的时候采用多个goroutine进行计算，全局维护一个map，key为名字，值为得分，因此在每次更新map的时候都先加锁，更新完之后再释放锁。 ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:3:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"二代 1.15.5 ~ 1.16.* 时间复杂度上没有变化，本次做的改进围绕锁来进行。使用切片取代字典，切片长度等于房源数量，利用下标一一对应。改进后如下 for i房源 in 所有房源 for j租客 in i房源 for k项 in 加减分项 if j租客 满足 k项 for i房源 in 所有房源 if 拓扑相等(l房源, i房源) slice[i] = atomic add(slice[i], k项分数) ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:4:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"三代 1.17.0 ~ 1.18.* 时间复杂度发生变化，从M * N * X * M 变为 M * N * X。通过一个map[拓扑key]map[拓扑value]得分的结构取代了最里层的循环，在整个大循环结束后，最后再额外进行一次循环就可以了。 for i房源 in 所有房源 for j租客 in i房源 for k项 in 加减分项 if j租客 满足 k项 lock map[拓扑key]map[i房源拓扑value] += k项分数 unlock for i房源 in 所有房源 score[i房源] += map[拓扑key][拓扑value] 本次虽然时间复杂度降低了一个数量级，但是又引入了锁，类似一代的实现，在全局有个map用来存放所有拓扑对应的分数，为防止多个goroutine并行访问map出现问题，所以加了锁。 ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:5:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"四代 1.19.0~ 本次的改进主要是去掉了锁，在时间复杂上没有变化。同样采用了全局map，但是在进行拓扑分数计算时并没有传入全局的map，而是每个房源对应一个map，暂且称为sub_map。代码中维护了一个sub_maps的切片，由于sub_map各用各的不存在并发问题，且把sub_map添加到sub_maps中时可以通过原子操作实现，在并发计算完所有房源的拓扑的分后，再统一把所有的子map进行处理，分数统一设置到全局的map中，剩下的处理逻辑就和三代最后的处理逻辑一样了。 index := -1 for i房源 in 所有房源 for j租客 in i房源 for k项 in 加减分项 if j租客 满足 k项 sub_maps[atomic add(index, 1)][拓扑key]map[i房源拓扑value] += k项分数 for sub_map in sub_maps map[拓扑key][拓扑value] + sub_map[拓扑key][拓扑value] for i房源 in 所有房源 score[i房源] += map[拓扑key][拓扑value] ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:6:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["性能优化"],"content":"总结 工程不同于纯算法实现，不仅需要在算法维度降低时间复杂度，也要考虑具体工程上的实现，比如这个例子中在涉及到并行计算时如何用原子操作替换掉锁。想要知道哪里是性能瓶颈，可以使用perf具体分析，也可以使用具体语言对应的性能分析工具，算法优化和工程优化同样重要，算法终归还是为工程服务的。 ","date":"2021-06-08","objectID":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/:7:0","tags":["性能优化","kubernetes"],"title":"Kube-scheduler InterPodAffinity性能优化史","uri":"https://www.likakuli.com/posts/kubernetes-interpodaffinity/"},{"categories":["问题排查"],"content":"问题描述 开启特权模式（--privileged）的容器，在使用nvidia GPU时，无法通过cAdvisor获取GPU相关的metrics信息。Google大法可以搜到相关的Issue，于2018年提出，至今仍处于Open状态（给cAdvisor贡献代码的机会），由于涉及到的内容较多，分为三篇来讲。 本篇为最后一篇，在看本篇之前建议先查看前两篇： 容器开启特权模式后无法通过cadvisor获取GPU metrics指标 容器开启特权模式后无法通过cadvisor获取GPU metrics指标 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:1:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"回顾 首先通过两种图回顾一下容器使用NVIDIA GPU的原理，如下 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:2:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"Kubelet \u0026 Device Plugin ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:2:1","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"Nvidia-container-runtime ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:2:2","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"解决方案 总结一下cAdvisor无法提供特权模式容器GPU指标的根本原因： cAdvisor作为一个偏底层的通用指标能力的提供者，为了与其他组件解耦，其从最底层device cgroup来获取容器绑定的GPU信息； containerd在创建容器时判断是否开启特权模式，是的话会为容器设置可以访问所有设备，即a *:* rwm。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:3:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"必要性 使用GPU时容器开启特权模式是一种趋势，简单说就是通过Env的方式传递GPU卡信息时，用户如果知道这个能力，则很容易就可以越过device-plugin在Pod spec中设置Env来实现使用GPU卡的目的，为了安全性，nvidia k8s-deivice-plugin提供了另一种实现方式，参考这里，新的方式是通过挂载卷的方式识别所需的GPU设备信息，必须开启特权模式。 虽然两种方式都可以实现容器内使用GPU设备的目的，但实现方式都不够优雅，根本原因还是缺少一个将第三方设备资源通知给container runtime的规范，社区也意识到了这个问题，提出了CDI规范，有关进展详见这里。在此规范的标准实现上线之前，随着越来越多的容器在使用GPU时开启特权模式，哪怕不那么优雅的方案也是有必要先实现的。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:3:1","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"GPU设备表示方式 k8s-device-plugin支持两种GPU设备表示方式，通过deviceIDStrategyFlag参数设置： index uuid （默认） 如果以index方式设置，则可以直接在cAdvisor中获取index值，无需从device cgroup中获取，实现起来比较简单。下面的内容是针对以uuid方式设置时cAdvisor的处理逻辑。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:3:2","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"GPU设备信息来源 Pod-Resource Kubelet提供了pod-resource机制，对外提供rpc服务，供外部获取容器所需的资源信息。对Kubelet版本有一定的要求，且这种方式会把cAdvisor和Kubelet偶合在一起，不适合In-Tree方式，可以采取Out-Of-Tree的方式实现。 OCI spec cAdvisor中本身已经缓存了容器OCI spec信息，可以从中获取绑定的GPU信息，优点是上层无感知，缺点是加重了cAdvisor的依赖，cAdvisor需要知道当前容器使用的GPU信息和当前节点上所有GPU信息。 k8s-device-plugin提供了两种方式设置当前容器使用的GPU设备信息： Env Volume Mount 在获取当前节点所有GPU信息之前有个基础知识点需要了解，即Mig Strategy。策略不同，最终获取到的节点的GPU设备信息不同。当前支持三种策略： None：不使用MIG能力 Single：使用MIG能力，且各GPU实例被分为相同的规格的计算实例 Mixed：使用MIG能力，GPU实例可以划分为任意规格的计算实例，或者直接使用整个GPU实例 剩下的工作就是实现不同机制下获取GPU设备信息的功能，可以参考k8s-device-plugin的实现，因为其本身在Allocate时必然涉及到相关能力。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display3/:3:3","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display3/"},{"categories":["问题排查"],"content":"问题描述 开启特权模式（--privileged）的容器，在使用nvidia GPU时，无法通过cAdvisor获取GPU相关的metrics信息。Google大法可以搜到相关的Issue，于2018年提出，至今仍处于Open状态（给cAdvisor贡献代码的机会），由于涉及到的内容较多，分为三篇来讲。 接上一篇，在上一篇中我们已经清楚cAdvisor是如何获取容器所使用的GPU卡信息的，也清楚了为什么在容器开启特权模式时cAdvisor无法获取其所使用的的GPU卡信息。但距离给出有效且优雅的解决方案还有很多待探索的，例如devices.list内容是如何来的？如何知道应该为容器绑定哪些GPU卡等，这些问题都将会在接下来的内容中得到答案，让我们一步一步进行分析。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:1:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"抽丝剥茧 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"devics.lists内容从何而来 上面提到过三个device cgroup的重要文件，其中devices.allow和devices.deny为只写文件，devices.list为只读文件。device子系统是通过device whitelist实现的，此处涉及到内核的知识，想深入了解其实现的可以翻一下内核的代码。简单理解就是通过前面两个只写的文件对whitelist做设置，往devices.allow中添加条目相当于添加白名单，往devices.deny中添加条目相当于删除白名单，最后通过devices.list获取白名单内容。那接下来的问题就是devices.allow和devices.deny内容是谁根据什么规则写入的？ ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:1","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"devices.allow和devices.deny内容谁负责写入 nvidia-container-runtime-webhook对应的github项目为 nvidia-container-toolkit。 这里内容比较多，涉及到kubelet、device-plugin、 containerd、 runC、nvidia-container-runtime、nvidia-container-runtime-hook等。在继续之前先回顾一下docker的进程模型，了解了进程模型及各组件的作用之后定位问题会更有针对性，更容易理解下面的内容。 进程模型回顾 相信上图大家已经非常熟悉了，其中Docker Engine通过rpc与containerd通信，containerd通过rpc与containerd-shim 通信，最终containerd-shim通过runC来控制容器的生命周期。提到runC就不得不提OCI（Open Container Initiative）。OCI 的来源这里不多说，感兴趣的可以自行搜索。其主要包含了两种规范，即runtime-spec和image-spce，而 runC就是对runtime-spec的一种实现。既然是规范，那就会存在多种实现，nvidia-container-runtime就是为了支持容器使用GPU而做的另一种runtime-spec的实现，原理如下 右侧从runC开始的一整块就是nvidia-container-runtime。 runC实现原理 runC容器创建原理 一个容器启动主要分为三大部分： create: 主要是为了解析、组装容器启动的配置和与子进程的消息通道等； init : 主要根据容器配置启动容器整个运行环境，包括熟知ns，cgroups, seccomp, apparmor, caps等; start : 主要是为了通知init 进程启动容器； 大致流程如下图 nvidia-container-runtime原理 nvidia-container-runtime原理是利用runtime-spec规范中提到的PreStart Hook机制在执行完runc start之后，在真正的用户进程启动之前，执行nviadia-container-runtime-hook进行一些操作，其针对GPU设备的挂载和device cgroup的设置是通过一个由C和C++实现的叫做nvidia-container-cli（libnvidia-container）的程序实现的。 注意：prestart hook的生效时机有待确认，虽然OCI spec中提到是在start之后，但是经过看runc源码发现是在执行init的过程中，且尚未执行start nvidia-container-runtime-hook通过解析runtime-spec中为用户程序设置的**NVIDIA_VISIBLE_DEVICES**环境变量获取容器需要使用的GPU卡的ID，并调用nvidia-container-cli configure --device=ID(s) 来完成上述提到的操作。 至此可以对容器内的GPU设备挂载和device cgroup的设置过程和原理有个大概的了解，这里利用了OCI hook和spec，通过环境变量形式把需要的GPU设备ID设置到名为NVIDIA_VISIBLE_DEVICES的环境变量上，进行了比较巧妙的实现，那又是谁为此环境变量设置赋值的呢？继续往下看 NVIDIA_VISIBLE_DEVICES来自何处 Kubernetes 提供了一个 device plugin框架（beta from 1.10），可以用它来将系统硬件资源发布到Kubelet，再由Kubelet上报给Kube-apiserver，不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、 InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。 工作流程如上图所示，device plugin先注册到Kubelet，Kubelet开始watchdevice plugin变化，在容器真正创建之前Kubelet调用device plugin提供的Allocate服务去为Pod申请所需资源。Kubelet在内部维护了一份Pod与扩展资源的映射数据，且通过checkpoint形式写到本地文件中，在后续Kubelet重启时会用到。同时在开启KubeletPodResource特性开关后，Kubulet还可以通过rpc的形式对外提供pod-resources信息，详情可以参考https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/。 这里大致可以猜到赋值的地方了，因为这是一个专门用于GPU的环境变量，所以不可能直接在Kubelet中定义，那就只剩下nidia对应的k8s-device-plugin了，我们可以在其源码中找到对应的实现逻辑，如下： // Constants to represent the various device list strategies const ( DeviceListStrategyEnvvar = \"envvar\" DeviceListStrategyVolumeMounts = \"volume-mounts\" ) // migStrategyNone func (s *migStrategyNone) GetPlugins() []*NvidiaDevicePlugin { return []*NvidiaDevicePlugin{ NewNvidiaDevicePlugin( \"nvidia.com/gpu\", NewGpuDeviceManager(false), // Enumerate device even if MIG enabled \"NVIDIA_VISIBLE_DEVICES\", gpuallocator.NewBestEffortPolicy(), pluginapi.DevicePluginPath+\"nvidia-gpu.sock\"), } } // Allocate which return list of devices. func (m *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) { responses := pluginapi.AllocateResponse{} for _, req := range reqs.ContainerRequests { for _, id := range req.DevicesIDs { if !m.deviceExists(id) { return nil, fmt.Errorf(\"invalid allocation request for '%s': unknown device: %s\", m.resourceName, id) } } response := pluginapi.ContainerAllocateResponse{} uuids := req.DevicesIDs deviceIDs := m.deviceIDsFromUUIDs(uuids) // 默认为DeviceListStrategyEnvvar，m.deviceListEnvvar的值就是NVIDIA_VISIBLE_DEVICES if deviceListStrategyFlag == DeviceListStrategyEnvvar { response.Envs = m.apiEnvs(m.deviceListEnvvar, deviceIDs) } if deviceListStrategyFlag == DeviceListStrategyVolumeMounts { response.Envs = m.apiEnvs(m.deviceListEnvvar, []string{deviceListAsVolumeMountsContainerPathRoot}) response.Mounts = m.apiMounts(deviceIDs) } if passDeviceSpecsFlag { response.Devices = m.apiDeviceSpecs(nvidiaDriverRootFlag, uuids) } responses.ContainerResponses = append(responses.ContainerResponses, \u0026response) } return \u0026responses, nil } 可以看到在具体是在Kubelet调用Allocate服务时，k8s-device-plugin对response.Env进行了设置，添加了对应的环境变量，值就是容器所对应的GPU设备，多个设备用逗号分隔。 为什么开启特权模式的容器devices.list是*:* 同样是设置了NVIDIA_VISIBLE_DEVICES，同样最终都是nvidia-container-cli进行device cgroup的设置，为什么特权模式的容","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:2","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"完整流程 至此我们清楚了容器在使用GPU时的整个流程，从容器创建到真正挂载GPU设备以及开启特权模式后无法获取GPU指标的原因。 好了，有关cAdvisor无法提供特权模式容器的GPU指标的原理及原因至此已经都搞清楚了，下一篇我们讲介绍解决方案，敬请期待~ ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:3","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"问题描述 开启特权模式（--privileged）的容器，在使用nvidia GPU时，无法通过cAdvisor获取GPU相关的metrics信息。Google大法可以搜到相关的Issue，于2018年提出，至今仍处于Open状态（给cAdvisor贡献代码的机会），由于涉及到的内容较多，分为三篇来讲。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display/:1:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display/"},{"categories":["问题排查"],"content":"寻踪觅源 问题的最终表现是通过cAdvisor无法获取开启特权模式容器的gpu相关数据，即 curl localhost:4194/api/v1.3/docker/{containerID} 返回的结果中不包含任何gpu信息，而没有开特权模式的容器是可以正常返回gpu相关信息的。 未开启特权模式的容器返回结果类似如下： root@node1: # curl localhost:4194/api/v1.3/docker/a0f3c54d8c6beba5d9947723494e4c9b625f03ce4eb0c096e4cf46cae9e389e0 | jq [.[]][0].stats[0].accelerators [ { \"make\": \"nvidia\", \"model\": \"Tesla T4\", \"id\": \"GPU-a2d26dd1-5a8f-35e5-c325-2cd0756d5ed2\", \"memory_total\": 15843721216, \"memory_used\": 15127085056, \"duty_cycle\": 6 }, { \"make\": \"nvidia\", \"model\": \"Tesla T4\", \"id\": \"GPU-febe3f91-914e-cb7f-bf0a-d96ffeda2777\", \"memory_total\": 15843721216, \"memory_used\": 11367940096, \"duty_cycle\": 44 } ] 可以看到容器用到了两块GPU卡。 开启特权模式的容器执行上述命令后返回空信息。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display/:2:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display/"},{"categories":["问题排查"],"content":"cAdvisor 首先需要了解的就是cAdvisor获取gpu指标信息的原理，可以查看官方文档，简单翻译一下如下： cAdvisor可以对外暴露容器级别的硬件加速器的指标，且当前只支持英伟达GPU，不支持整机级别 cAdvisor只对在容器启动时显示设置了--device /dev/nvidia0:/dev/nvidia0信息的容器暴露指标，未显示指定的不暴露（对应容器启动时设置了--privileged参数） 通过介绍可以得出如下结论：无法获取开启特权模式容器的GPU指标是Feature而不是Bug 同时文档中最后提到如果cAdvisor容器化部署时如何设置参数，其中提到的三种方法，如下： cAdvisor容器以--privileged模式启动 Docker v17.04.0-ce 及以上版本的话，启动时设置--device-cgroup-rule 'c 195:* mrw'参数 启动时设置--device /dev/nvidiactl:/dev/nvidiactl /dev/nvidia0:/dev/nvidia0 /dev/nvidia1:/dev/nvidia1 \u003cand-so-on-for-all-nvidia-devices\u003e参数 现实是无论容器是否开启特权模式，我们都需要去获取容器级别的GPU使用率指标，那怎么办呢？大致有两种方法：修改cAdvisor使其支持（In-Tree）、添加其他组件来提供GPU使用率指标（Out-Of-Tree），无论哪种方式，我们都有必要先搞清楚如何采集GPU使用率指标，可以从cAdvisor下手，看他是如何采集的。 采集机制 在看具体实现之前，首先介绍一下cAdvisor运行原理，如下图 cAdvisor在采集过程中主要分两种数据： 容器数据 指标数据 容器数据来源 通过watch cgroup下文件目录的变化进行对应容器的处理，此处只获取到容器ID，在获取到新增容器时，通过containerHandler根据容器ID获取容器详情，例如判断出来watch到的容器是通过docker创建的，则会调用docker API获取指定ID的容器详情，其中就包含了容器启动时需要的Env信息，后面的解决方案中会用到这个属性。 指标数据来源 每个容器都有与之对应的collector来进行指标采集，其中nvidia指标由对应的nvidiaCollector负责采集。 源码分析 明白了原理之后，就可以有针对性的去源码中看其具体实现逻辑了。代码位置很好找，在项目根目录下有个accelerators文件夹，源码位于nvidia.go文件内。 // GetCollector returns a collector that can fetch NVIDIA gpu metrics for NVIDIA devices // present in the devices.list file in the given devicesCgroupPath. func (nm *nvidiaManager) GetCollector(devicesCgroupPath string) (stats.Collector, error) { nc := \u0026nvidiaCollector{} if !nm.devicesPresent { return \u0026stats.NoopCollector{}, nil } // Makes sure that we don't call initializeNVML() concurrently and // that we only call initializeNVML() when it's not initialized. nm.Lock() if !nm.nvmlInitialized { err := initializeNVML(nm) if err != nil { nm.Unlock() return \u0026stats.NoopCollector{}, err } } nm.Unlock() if len(nm.nvidiaDevices) == 0 { return \u0026stats.NoopCollector{}, nil } // 从cgroup中获取需要容器使用的nvidia设备信息，返回的是gpu序号数组 nvidiaMinorNumbers, err := parseDevicesCgroup(devicesCgroupPath) if err != nil { return \u0026stats.NoopCollector{}, err } for _, minor := range nvidiaMinorNumbers { device, ok := nm.nvidiaDevices[minor] if !ok { return \u0026stats.NoopCollector{}, fmt.Errorf(\"NVIDIA device minor number %d not found in cached devices\", minor) } nc.devices = append(nc.devices, device) } return nc, nil } 逻辑比较简单，先初始化NVML（基于C实现的API，用来监控和管理nvidia设备的状态信息，通过nvidia-smi对外暴露其各种能力。），接着调用parseDevicesCgroup函数获取容器自身所使用的GPU的序号，最后通过返回的设备序号找到使用的设备信息返回。 // parseDevicesCgroup解析device cgroup下的devices.list文件来获取允许被容器访问的GPU设备的minor号 // 如果容器可以访问所有设备或者所有nvidia设备的话，但这些设备并未在devices.list中，则返回一个空数组 var parseDevicesCgroup = func(devicesCgroupPath string) ([]int, error) { // Always return a non-nil slice nvidiaMinorNumbers := []int{} devicesList := filepath.Join(devicesCgroupPath, \"devices.list\") f, err := os.Open(devicesList) if err != nil { return nvidiaMinorNumbers, fmt.Errorf(\"error while opening devices cgroup file %q: %v\", devicesList, err) } defer f.Close() s := bufio.NewScanner(f) // See https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt for the file format for s.Scan() { text := s.Text() fields := strings.Fields(text) if len(fields) != 3 { return nvidiaMinorNumbers, fmt.Errorf(\"invalid devices cgroup entry %q: must contain three whitespace-separated fields\", text) } // Split the second field to find out major:minor numbers majorMinor := strings.Split(fields[1], \":\") if len(majorMinor) != 2 { return nvidiaMinorNumbers, fmt.Errorf(\"invalid devices cgroup entry %q: second field should have one colon\", text) } // NVIDIA graphics devices are character devices with major number 195. // https://github.com/torvalds/linux/blob/v4.13/Documentation/admin-guide/devices.txt#L2583 if fields[0] == \"c\" \u0026\u0026 majorMinor[0] == \"195\" { minorNumber, err := strconv.Atoi(majorMinor[1]) if err != nil { return nvidiaMinorNumbers, fmt.Errorf(\"invalid devices cgroup entry %q: minor number is not integer\", text) } // We don't want devices like nvidiactl (195:255) and nvidia-modeset (195:254) if minorNumber \u003c 128 { nvidiaMinorNumbers = append(nvidiaMinorNumbers, minorNumber) } // We are ignoring the \"195:*\" case","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display/:2:1","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display/"},{"categories":["使用说明"],"content":"通用Restful API项目模板 欢迎使用，这是一个用Go编写的简单通用的Restful API项目，遵循SOLID原则。 部分灵感来自于 service-pattern-go ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:0","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"依赖 Gin Gorm Testify (Test \u0026 Mock framework) Mockery (Mock generator) Hystrix-Go (Circuit Breaker) ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:1","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"开始 安装 介绍 目录结构 Mocking Tesing 能力支持 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:2","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"安装 克隆项目代码 git clone https://github.com/likakuli/generic-project-template.git 启动mysql服务并初始化数据库 docker run --name=mysql -it -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d registry.cn-beijing.aliyuncs.com/likakuli/mysql 注意：如果是在MacOS上使用Docker for Mac启动的容器，则需要安装 docker-connector ，否则无法在本机通过容器IP访问容器，原因参考这里。安装命令如下 # 安装 docker-connector brew install wenjunxiao/brew/docker-connector # 把 docker 的所有 bridge 网络都添加到路由中 docker network ls --filter driver=bridge --format \"{{.ID}}\" | xargs docker network inspect --format \"route {{range .IPAM.Config}}{{.Subnet}}{{end}}\" \u003e\u003e /usr/local/etc/docker-connector.conf # 启动服务 sudo brew services start docker-connector # 在 docker 端运行 wenjunxiao/mac-docker-connector，需要使用 host 网络，并且允许 NET_ADMIN docker run -it -d --restart always --net host --cap-add NET_ADMIN --name connector wenjunxiao/mac-docker-connector 镜像涉及到的Dockerfile与sql放置在docker文件夹下 运行单元测试 make test 编译程序 make build 运行程序 # 替换配置文件MySQL connectionString ./docker/replace_ip.sh # 启动程序 ./generic-project-template --config=./conf/config.toml --log_dir=./logs --v=1 访问程序 curl http://localhost:8080/api/v1/score/Lucy/vs/Lily ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:3","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"介绍 这是一个简单通用的Restful API项目，内置依赖注入、Mocking等功能，旨在方便快速的编写安全可靠的Restful API代码。不同的数据结构之间通过接口来访问，避免直接引用具体的实现，这样就可以实现依赖注入及采用Mock结构进行单元测试的效果。 举例来说： IPlayerServie --\u003e IPlayerRepository type PlayerController struct { service interfaces.IPlayerService } type playerService struct { repo interfaces.IPlayerRepository } ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:4","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"目录结构 /cmd /- apiserver /conf /pkg /- config /- controllers /- interfaces /- models /- repositories /- server /- middlewares |- container.go |- router.go |- server.go /- services /- viewmodels controllers 控制器文件夹下包含所有Gin Route Handler，里面只包含处理Request和Response的逻辑，不包含任何业务逻辑和数据访问逻辑。仅依赖于interfaces下的IService接口，不依赖于具体实现。 interafces 接口文件夹下存放所有IService和IRepository接口定义及通过Mockery自动生成的用于单元测试的文件，不包含具体接口实现。 models 模型文件下下存放所有与数据库映射的实体模型对应的Go Struct，只包含数据结构，不包含数据访问逻辑。可以由 gen 根据数据库表结构自动生成，详情参考这里 repositories 仓库文件夹下存放所有数据库访问逻辑，且实现了interfaces下定义的IRepository接口，主要用到models文件夹下定义的实体结构。 services 服务文件夹下存放所有实现了services下定义的IService接口的逻辑，供controllers直接使用。其中涉及到的数据库访问部分均通过调用interfaces下的IRepository接口实现，不依赖任何具体实现。 viewmodels 视图模型文件夹下存放所有需要与API交互的实体，主要包含从API获取到的结构和返回值的结构。与models的区别在于前者对应api层，后者对应数据库层。 router 路由文件夹下包含了所有可以对外提供服务的Restful API的路由注册逻辑。 container 容器文件下包含了所有依赖注入需要的Provider的逻辑，且在此选择具体使用的接口实现类型。 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:5","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"Mocking 为方便进行单元测试，使用Mockery自动interfaces下接口实现，例如生成IPlayerService的实现，只需要进入interfaces文件夹下执行如下命令即可，最后会在interfaces下自动创建mocks文件夹来存放自动生成的文件。 mockery -name=IPlayerService 需要提前安装mokery工具 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:6","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"Testing 有了依赖注入和Mock功能后，就可以针对任意接口实现编写单元测试了，示例中添加了针对serviceshecontrollers的单测，供参考。 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:7","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"能力支持 Tracing PProf Prometheus Metrics Health Check Mock Testing Circuit Breaker Rate Limit Common go-utils … ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:8","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["开源项目介绍"],"content":"CRUD还能这么玩？ 作为一名有多年经验的CRUD工程师，在做某项目时，需要在搭建完apiserver框架以及dao部分，三下五除二的就把apiserver部分搞定了，然后到了dao部分。用屁股想了想网上肯定有现成的工具可以自动生成go struct，于是在网上找了找，果不其然，很多，但基本都是根据table来自动生成go struct的，于是搞定了创建table的sql，创建完table之后顺利的用工具生成了go struct。你以为这就完了吗。当然没有，因为在此过程中我发现了一个BUG级的项目：https://github.com/smallnest/gen，点开作者主页一看，原来又是一位自己关注已久的博主：鸟窝，肃然起敬。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:1:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"项目介绍 这个项目能干什么呢？简单来说就是他可以根据你的数据库自动生成go struct及dao层、api层，直接生成一套可运行的restful api项目，而且还搭配了swagger文档。抛开最终生成的代码不说，单就这个思想就已经够我们学习了。 接下来简单介绍下这个项目，为啥简单介绍呢，因为使用起来确实很简单，而且其文档说明比较完善。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"安装 项目采用golang编写，生成的也是golang代码，安装方式自然也是golang项目的安装方式 go get -u github.com/smallnest/gen ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:1","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"使用 在项目代码的example文件夹下存在一个sample.db文件，可以直接根据这个文件来生成完成的项目，当然也支持自定义配置来生成自己需要的部分。 ## 根据sample.db生成项目代码 $ gen --sqltype=sqlite3 \\ --connstr \"./sample.db\" \\ --database main \\ --json \\ --gorm \\ --guregu \\ --rest \\ --out ./example \\ --module example.com/rest/example \\ --mod \\ --server \\ --makefile \\ --json-fmt=snake \\ --generate-dao \\ --generate-proj \\ --overwrite ## 编译 (使用make命令进行编译，packr2会被自动安装) $ cd ./example $ make example ## 二级制位置./bin/example $ cp ../../sample.db . $ ./example ## 打开浏览器访问这里 http://127.0.0.1:8080/swagger/index.html ## 同样可以使用命令行工具进行访问 curl http://localhost:8080/artists 可能会报错，提示找不到某些文件或者包，安装即可，部分文件判断方式是直接检查GOPATH下是否存在，所以会出现使用go mod模式时get下来之后仍然报错的情况，可以直接到GOPATH下下载对应的项目即可。 是不是so easy，只需要提供数据库即可生成完整项目，但是他的功能不止于此。因为每个人有自己的习惯的编码风格，自动生成的代码不可能满足所有人的需求，那怎么办呢？ ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:2","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"高级功能 gen支持自定义模板，可以把原模板导出，基于其修改，或者干脆按照自己的风格制作一套模板，通过--templateDir=参数指定自己的模板的路径即可 如果只想进行简单改动，可以尝试--exec=参数，允许开发者自定义代码生成规则，在项目自带的custom文件夹下有sample.gen文件，可以直接指定--exec=sample.gen看执行效果，如果使用的是最新的master，则执行会报错，看代码可以发现在2020/8/4的一次修改，改变了某些方法的形参，而sample.gen没有进行对应修改，按需修改即可。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:3","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"原理 项目用到的主要的技术就是golang的template功能，包括template的常用能力及定制函数等功能。另外包括GIN框架、GORM框架等crud工程师们常用的包。里面还有一个平时不怎么用的包packr，其主要功能就是把静态文件嵌入到最终生成的二进制文件中，程序就可以在任何位置直接运行而不需要受制于额外静态文件的位置等因素。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:3:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"结束语 被CRUD折磨的人啊，是否有考虑过将其自动化呢，虽然业务逻辑是在变化的，但项目框架部分是基本保持不变的，搞一套自己习惯的开箱即用框架岂不是更美，毕竟从头写的话也需要一点时间，懒果然是人类进步的阶梯，赶紧去下载项目试一试吧。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:4:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["思考感悟"],"content":"近期有个小需求，在不重建Container的前提下修改Pod结构中的Request值，限制仅可以调小。本以为很简单的一个需求，但实际花费了一天的时间才搞完，代码改动只有几行，但是在改完测试的过程中发现很多超出预期或者认知的现象，为了搞懂为什么会这样，又重新捋了捋kubelet源码。 在这件事结束之后也进行了反思，主要是有关源码阅读的，于是把这个过程和自己的感触以及后续的一些改进方法和计划记录下来。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:0:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"过程 先看下这件事的过程，我们先忽略这个需求的合理性，直接分析技术实现。 首先，kubernetes本身并不支持修改Pod的资源属性，无论Request还是Limit，可以通过修改apiserver中的校验逻辑来放开此限制； 其次，如何保证在Request改变之后容器不重启？我们知道，kubelet会为每个container都计算出一个hash值，其中用到了container的所有属性，在调用docker api进行容器创建的时候会把这个值设置到容器的Label中，后续如果kubelet检测到新计算出的hash值与在运行的容器的hash值不同，则会进行容器的原地重启操作，这也是为什么修改container的Image会出发容器原地重启的原因。很明显，如果放开Request的修改，Request值变了之后也会导致新的hash值变化从而导致容器重建，与我们的期望不符。也有办法来解决：记录container创建时的Request值，计算的时候还是使用创建时的值，此值只有在container创建时会记录，后续不再更新。 测试场景是创建了Qos类型为Guaranteed 类型的Pod，放开了kube-apiserver对Request修改的限制，kubelet保持原生不动，调小其Request值，大家可以先尝试自己思考一下会发生什么？ 那么在测试的时候遇到了什么问题呢？ 首先，发现放开Request修改之后，如果改了Request的值，容器重启了（这一步符合预期），但是重启次数加2（这里其实是之前的一个盲点） 接着，继续修改Request值，容器依然重启（符合预期），但是此次重启次数只加了1 最后，通过查看Pod Cgroup目录，确认修改后的Request已经在Pod级别和Container级别分别生效，但是同时存在两个Pod的目录，类似如下 # 修改之前的Pod对应的cgroup目录 /sys/fs/cgroup/cpu/kubepods/guaranteed/pod{uid} # 修改之后的Pod对应的cgroup目录 /sys/fs/cgroup/cpu/kubepods/burstable/pod{uid} 为什么同一个Pod会存在两个cgroup目录呢？ 容器重启了，重启次数应该只加1，那为什么在第一步中加了2？ 你可以在继续阅读之前先自己思考一下可能的原因。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:1:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"问题分析 首先看为什么会有两个cgroup目录，需要先搞清楚cgroup目录是如何创建、如何删除的。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:2:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"Cgorup创建 我们采用CgroupPerQos的方式进行管理，以cpu子系统为例，层级类似如下所示 /sys/fs/cgroup/cpu kubepods guaranteed pod{uid} {containerid} {containerid} pod{uid} {containerid} {containerid} burstable bestaffort 从创建者的角度分两种：kubelet创建的、docker创建的。其中container层由docker创建，container以上的pod层、qos层和root（kubepods）都是由kubelet创建的。那docker又是怎么知道容器的cgroup parent目录是谁呢？其实是kubelet在调用docker api时传给docker的一个参数，告诉了其cgroup parent路径，可以通过执行docker inspect {containerid} | grep -i cgroup来查看每个container的cgroup parent路径。 那为什么会在两个qos目录下分别存在一个Pod目录呢？因为我们修改了Pod的Qos类型，触发了syncPod逻辑，里面会去根据Pod的qos类型进行cgroup目录判断，如果qos改变，则会把原Pod下的所有container全部杀掉，然后创建新的cgroup目录，再启动容器。这也就可以解释为什么在第一次修改Request之后Pod重启次数增加了2，因为pause容器也发生了重建。为什么要重建容器呢，因为整个pod的qos发生了变化，Pod内的所有容器需要在新的qos目录下重建其目录，但是kubelet没有去更新container的cgroup设置，而是采用重建的方式来实现。 为什么kubelet不直接去更新cgroup目录，而是重建容器呢？首先修改Request不仅影响cgroup，容器的oomscore也将受到影响，docker虽然提供了api来修改资源大小，但并没有提供相关的api去进行cgroup目录及oomscore等属性的修改，其次cgroup迁移是一个比较复杂的工作，迁移过程会出现部分历史数据丢失等问题，所以kubele直接采用重建的方式来解决这个问题。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:2:1","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"Cgroup删除 经过分析Cgroup创建过程，重启两次的问题已经找到了答案。但为什么新的Pod cgroup目录创建出来之后，原有的目录没有被删除呢？这就需要搞清楚Pod Cgroup目录什么时候删除的，容器级别的cgroup目录是在容器被删除的时候删除的，这个很好理解，Pod级别的Cgroup目录是否也是在Pod删除时删除的呢？经过看代码发现并不是，Pod资源清理是一个异步的过程，定时监测Pod是否已经设置了deletionTimestamp属性和容器的运行状态，只有设置了此属性的Pod才有可能被清理，清理的过程中包含挂在卷、Cgroup等资源，会一并清理。因为修改Request的请求是不会去给Pod设置deletionTimestamp属性的，这就导致Pod级别的旧目录不会被删除，又因为新目录的创建，导致同时存在两个Pod级别的目录。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:2:2","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"反思 综合看下来，这两个点都没有那么难，而且之前也做过kubelet定制开发，syncPod部分代码更是看过数次。那为什么花费了这么久的时间呢？ ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"源码阅读的目的性 此前阅读源码的目的有几种，查问题、验证某些想法、探寻系统运行原理，还有一些人通过看源码来写blog、或者写源码分析之类的书。此前大部分场景是为了查问题、验证想法以及某些小功能的定制开发，可以聚焦到某些点或流程上，做完之后会对相关点印象比较深刻，但是相关性不大的地方就会很容易遗忘，或者说根本不会去刻意关注相关性不大的地方。偶尔会有想法去主动阅读源码，探求系统运行原理，实现方式，写blog等，但最终发现效果很差，因为在此过程中我们的目的性并不是真正的去理解系统，缺乏针对性。而且对于在还不了解系统运行原理的情况下想通过看源码去了解其原理就是本末倒置的事情。比如接手一个新项目或者其他人的项目的时候，如果没有相关背景，而且也没有项目文档，没有代码注释的情况，直接想通过代码去了解业务和系统运行原理是一件非常痛苦的事情。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:1","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"思考的必要性 无论处于什么目的去看代码，需要有自己的思考，可以假设系统由自己设计，那会设计成什么样子，代码由自己实现，会写成什么样子。在设计过程中可能会遇到一些问题，带着问题再去看代码，去验证别人是如何设计并实现的，尤其是遇到和自己预期设计不一致的地方，可以进行对比，分析那种方案好，或者他这么设计是处于什么考虑，为什么这么实现。以这样的方式看代码要比没有目的性的走马观花式的浏览代码收获更多，印象更深刻。这里推荐一本书《思考，快与慢》，解释了人的大脑是如何工作的，可以通过本书了解到思考是一个怎样的过程。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:2","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"后续计划 源码还是要去读的，后面会进行一些尝试，根据上面提到的阅读方式开始进行，即 思考系统运行方式 ==》自己设计系统实现 ==》带着问题读源码（验证想法） ==》思考与总结，试运行一段时间看看效果。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:3","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":[],"content":"Hi，我是 Kaku 李鹤，目前就职于Shopee。幼儿园小班工程师，终身学习者。 2016年开始从事容器云PAAS平台相关的研发工作，从0到1搭建了公司级容器容器云平台。 2018年开始主要研究大规模集群调度和管理，2019年开始负责滴滴弹性云平台基础架构方向的工作，包括大规模集群支撑、管理运维、调度优化等工作，同时对线上问题排查与处理也有一定的踩坑经验。 目前主要关注领域在大规模容器集群管理与调度、利用ebpf技术实现运行状态的可视化。 欢迎扫描下方二维码关注微信公众号IT散修进行订阅。也可以通过微信公众号留言同作者进行交流或者直接通过微信公众号加作者微信进行交流。 ","date":"2021-04-15","objectID":"https://www.likakuli.com/about/:0:0","tags":[],"title":"关于作者","uri":"https://www.likakuli.com/about/"},{"categories":[],"content":"组织：k-cloud-labs 项目： Kinitiras: 通用可编程 Kubernetes admission webhook 规则引擎； Pidalio: Kinitiras 的客户端实现，0 侵入，高性能； Kluster-capacity: 集群剩余容量评估、模拟调度、集群压缩、集群碎片率等分析工具； scheduler-stress-test: 调度器性能压测工具； 欢迎志同道合的朋友一起贡献~ ","date":"2021-04-15","objectID":"https://www.likakuli.com/projects/:0:0","tags":[""],"title":"组织项目介绍","uri":"https://www.likakuli.com/projects/"},{"categories":["源码分析"],"content":"开局一张图，剩下全靠编 详细内容参考： Informer DeltaFIFO Informer LocalStore 未完待续 ","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer/:0:0","tags":["informer","kubernetes"],"title":"Kubernetes Informer","uri":"https://www.likakuli.com/posts/kubernetes-informer/"},{"categories":["源码分析"],"content":"按照惯例，先上图，cache对应Informer中的Local Store。 直接撸源码（基于1.12版本，有删减，不影响主要逻辑） // 代码源自client-go/tools/cache/store.go type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. // 用传入的内容替换原有内容 Replace([]interface{}, string) error Resync() error } Store为最基本的存储接口，提供增删改查基本功能，要求对象有唯一键，键的计算方式由接口的具体实现决定，很好理解。 // 代码源自client-go/tools/cache/index.go // Indexer is a storage interface that lets you list objects using multiple indexing functions type Indexer interface { Store // Retrieve list of objects that match on the named indexing function Index(indexName string, obj interface{}) ([]interface{}, error) // IndexKeys returns the set of keys that match on the named indexing function. IndexKeys(indexName, indexKey string) ([]string, error) // ListIndexFuncValues returns the list of generated values of an Index func ListIndexFuncValues(indexName string) []string // ByIndex lists object that match on the named indexing function with the exact key ByIndex(indexNtame, indexKey string) ([]interface{}, error) // GetIndexer return the indexers GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error } Indexer在Store基础上扩展了索引能力，就好比给数据库添加的索引，以便查询更快，那么肯定需要有个结构来保存索引 // 代码源自client-go/tools/cache/index.go // IndexFunc knows how to provide an indexed value for an object. type IndexFunc func(obj interface{}) ([]string, error) // Index maps the indexed value to a set of keys in the store that match on that value type Index map[string]sets.String //sets.String为map[string]struct{}类型，减少内存占用 // Indexers maps a name to a IndexFunc type Indexers map[string]IndexFunc // Indices maps a name to an Index type Indices map[string]Index 到这里是不是一脸懵逼？？！！，光看注释根本不知道这是要干啥。经过搜索发现client-go里只用到了一个IndexFunc，即MetaNamespaceIndexFunc，对应的IndexName为namespace，所以，一般情况下Indexers总是这个值{“namespace”:MetaNamespaceIndexFunc}，举个栗子： 假如我要从Store中获取Key为default/test-app的Pod信息，那么上面的三个map中的内容分别为 Indexers: {“namespace”:MetaNamespaceIndexFunc} Index:{“default”:{“default/test-app”:}} Indices: {“namespace”:{“default”:{“default/test-app”:}}} 如果还是不懂，就先往下看，看Indexer的具体实现 // 代码源自client-go/tools/cache/store.go type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } // KeyFunc knows how to make a key from an object. Implementations should be deterministic. type KeyFunc func(obj interface{}) (string, error) // 代码源自client-go/tools/cache/thread_safe_store.go type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error Resync() error } // threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} // indexers maps a name to an IndexFunc indexers Indexers // indices maps a name to an Index indices Indices } 上面代码很好理解，cache主要由线程安全的map[string]interface{}实现，Go的","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer-localstore/:0:0","tags":["informer","kubernetes"],"title":"Informer LocalStore源码解析","uri":"https://www.likakuli.com/posts/kubernetes-informer-localstore/"},{"categories":["源码分析"],"content":"总结 会不会有人有疑问，所有的对象都存在一个索引键下面，这样的效率岂不是太低了?其实client-go为每类对象都创建了Informer(Informer内有Indexer)，所以存储在相同索引建下的对象都是同一类，这个问题自然也就没有了。cache就类似数据库的Table，而且带了索引功能，可以试想一下自己想要实现一个带索引功能的缓存的话，怎么设计数据结构 对象如何存储？每个对象都有唯一键，通过KeyFunc计算出来，不同对象的键不重复，可以用map实现，对应上面的ThreadSafeMap 索引策略有哪些？用map[string]Func实现，key代表该索引策略名字，Func为策略，传入对象，返回对象在该策略下的索引值，比如key为namespace，Func(obj)的返回值就是该obj的namespace，如default，这里要注意，Func应该返回数组，例如如果策略为Label，一个对象的Label是有多个的，索引值就会有多个，对应上面的Indexers 索引值和对象的关系如何存储？一个索引值可能回应多个对象，如按照Label索引，需要一个map来存储，key为索引的值，值为对象集合，但是因为对象有唯一键，所以值可以用对象的唯一键的集合来减少内存消耗，但是这种数据结构在查询某个对象是否满足指定的索引值时需要遍历值的集合，很简单吗，值也用map来寸就可以了，key为对象的键，值为空结构体，即值得类型为map[string]struct{}，对应sets.String，整个数据结构对应上面的Index 索引策略和索引值的关系如何存储？很简单，一个map就搞定了，key为策略名字，value为Index，对应上面的Indices 上面基本可以实现功能了，还有一个可以考虑的问题：计算对象唯一键的方法、索引相关的三个数据结构放在哪里，k8s的实现里，KeyFunc是调用者和cache都知道具体的算法，索引相关的数据结构都放在了threadSafeMap里。 ","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer-localstore/:1:0","tags":["informer","kubernetes"],"title":"Informer LocalStore源码解析","uri":"https://www.likakuli.com/posts/kubernetes-informer-localstore/"},{"categories":["源码分析"],"content":"DeltaFIFO位于Reflector和LocalStore之间，看名字知道是个先进先出的队列，作用就是缓存数据变化，直接看代码 // 代码源自client-go/tools/cache/delta_fifo.go type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. // 读写锁，性能要比Mutex好 lock sync.RWMutex // 供pop函数使用，没有对象的时候可以阻塞 cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. // 注意map值类型为Deltas，DeltaFIFO生产者和消费者是异步的，如果同一个目标的频繁操作，前面操作还缓存在队列中的时候，那么队列就要缓冲对象的所有操作 // queue里存的是map的keys，用map实现高速查询，用slice保证有序 items map[string]Deltas queue []string // 下面两个属性用来判断是否已完成同步， // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // knownObjects list keys that are \"known\", for the // purpose of figuring out which items have been deleted // when Replace() or Delete() is called. // 这里其实就是Indexer knownObjects KeyListerGetter // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRED operations. closed bool closedLock sync.Mutex } // 代码源自client-go/tools/cache/delta_fifo.go type Delta struct { Type DeltaType Object interface{} // 对象，也就是k8s里的资源 } type DeltaType string // Delta的类型用字符串表达 const ( Added DeltaType = \"Added\" // 增加 Updated DeltaType = \"Updated\" // 更新 Deleted DeltaType = \"Deleted\" // 删除 Sync DeltaType = \"Sync\" // 同步 ) type Deltas []Delta // Delta数组 Delta就是数据的变化，用字典存储达到更快的检索速度，而且Deltas是要求有序的，而字典的遍历是无序的，所以用slice保存key的集合，以达到先进先出效果。结合上图来看，DeltaFIFO就是一个用来存储kubernetes资源变化的先进先出的队列。DeltaFIFO队列功能是通过实现如下接口实现的 // 代码源自client-go/tools/cache/fifo.go type Queue interface { // 存储接口，再Indexer分析过了 Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. // pop在对列为空的时候会阻塞，就是通过上面提到的sync.Cond实现的 // 具体弹出的对象是通过传入的PopProcessFunc处理的，处理完返回错误后，可能会再入对列 Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // HasSynced returns true if the first batch of items has been popped HasSynced() bool // Close queue Close() } 从这个接口可以看出，其实Queue也是一个Store，只不过添加了Pop的操作，可以将对象有序弹出，对比Indexer，Indexer是在Store基础上增加了索引，所以一个当做对列用，一个当做存储用。 接着看DeltaFIFO具体实现，还是比较简单的 // 代码源自client-go/tools/cache/delta_fifo.go // 添加对象接口 func (f *DeltaFIFO) Add(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true // 队列第一次写入操作都要设置标记 return f.queueActionLocked(Added, obj) } // 更新对象接口 func (f *DeltaFIFO) Update(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true // 队列第一次写入操作都要设置标记 return f.queueActionLocked(Updated, obj) } // 删除对象接口 func (f *DeltaFIFO) Delete(obj interface{}) error { id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } f.lock.Lock() defer f.lock.Unlock() f.populated = true // 队列第一次写入操作都要设置标记 // 此处是需要注意的，knownObjects就是Indexer，里面存有已知全部的对象 if f.knownObjects == nil { // 在没有Indexer的条件下只能通过自己存储的对象查一下 if _, exists := f.items[id]; !exists { return nil } } else { // 自己和Indexer里面有任何一个有这个对象多算存在 _, exists, err := f.knownObjects.GetByKey(id) _, itemsExist := f.items[id] if err == nil \u0026\u0026 !exists \u0026\u0026 !itemsExist { return nil } } return f.queueActionLocked(Deleted, obj) } // 列举对象键接口 func (f *DeltaFIFO) ListKeys() []string { f.lock.RLock() defer f.lock.RUnlock() list := make([]string, 0, len(f.items)) for key := range f.items { list = append(list, key) } return list } // 列举对象接口 func (f *DeltaFIFO) List() []","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer-deltafifo/:0:0","tags":["informer","kubernetes"],"title":"Informer DeltaFIFO源码解析","uri":"https://www.likakuli.com/posts/kubernetes-informer-deltafifo/"},{"categories":["问题排查"],"content":"cgroup mount destination: unknown","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"问题 线上k8s集群在进行容器创建时报如下错误 Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod “xxx-sf-32c80-0”: Error response from daemon: cgroups: cannot find cgroup mount destination: unknown 之前遇到过cgroup相关问题，但是这个问题还是头一次见，网上搜索了关键字，社区有类似报错的issue，如cgroups: cannot found cgroup mount destination: unknown，联系最近做过的线上变更及问题，怀疑跟某自定义组件有关，详细背景参考这篇。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/:0:1","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"排查过程 光看问题云里雾里的，只知道和cgroup有关，登陆宿主查看此错误是kubelet请求docker时docker返回的，docker 18.06版本，没有更详细的日志了，但是开源的一个好处在于查问题的时候有源码，这大大降低了查问题的难度，直接去docker项目中搜索关键词，最终发现是在containerd的源码中，相关代码如下 // PidPath will return the correct cgroup paths for an existing process running inside a cgroup // This is commonly used for the Load function to restore an existing container func PidPath(pid int) Path { p := fmt.Sprintf(\"/proc/%d/cgroup\", pid) paths, err := parseCgroupFile(p) if err != nil { return errorPath(errors.Wrapf(err, \"parse cgroup file %s\", p)) } return existingPath(paths, \"\") } func existingPath(paths map[string]string, suffix string) Path { // localize the paths based on the root mount dest for nested cgroups for n, p := range paths { dest, err := getCgroupDestination(string(n)) if err != nil { return errorPath(err) } rel, err := filepath.Rel(dest, p) if err != nil { return errorPath(err) } if rel == \".\" { rel = dest } paths[n] = filepath.Join(\"/\", rel) } return func(name Name) (string, error) { root, ok := paths[string(name)] if !ok { if root, ok = paths[fmt.Sprintf(\"name=%s\", name)]; !ok { return \"\", fmt.Errorf(\"unable to find %q in controller set\", name) } } if suffix != \"\" { return filepath.Join(root, suffix), nil } return root, nil } } func getCgroupDestination(subsystem string) (string, error) { f, err := os.Open(\"/proc/self/mountinfo\") if err != nil { return \"\", err } defer f.Close() s := bufio.NewScanner(f) for s.Scan() { fields := strings.Split(s.Text(), \" \") if len(fields) \u003c 10 { // broken mountinfo? continue } if fields[len(fields)-3] != \"cgroup\" { continue } for _, opt := range strings.Split(fields[len(fields)-1], \",\") { if opt == subsystem { return fields[3], nil } } } if err := s.Err(); err != nil { return \"\", err } return \"\", ErrNoCgroupMountDestination } func parseCgroupFile(path string) (map[string]string, error) { f, err := os.Open(path) if err != nil { return nil, err } defer f.Close() return parseCgroupFromReader(f) } func parseCgroupFromReader(r io.Reader) (map[string]string, error) { var ( cgroups = make(map[string]string) s = bufio.NewScanner(r) ) for s.Scan() { if err := s.Err(); err != nil { return nil, err } var ( text = s.Text() parts = strings.SplitN(text, \":\", 3) ) if len(parts) \u003c 3 { return nil, fmt.Errorf(\"invalid cgroup entry: %q\", text) } for _, subs := range strings.Split(parts[1], \",\") { if subs != \"\" { cgroups[subs] = parts[2] } } } return cgroups, nil } 逻辑比较清晰，先从/proc/id/cgroup中解析得到所有的subsystem，对应上面parseCgroupFromReader函数，/proc/id/cgroup内容如下 先按冒号分隔每行字符串，然后取第2列，再根据逗号分隔得到所有的子系统，最终返回所有子系统。然后调用existingPath检查是否所有子系统都存在，内部又调用getCgroupDestination，最终的报错就是在这个函数里报出来的。 getCgroupDestination的逻辑是读取/proc/id/mountinfo信息，判断是否传入的子系统存在 先根据空格分隔，找到所有cgroup类型的目录，然后再根据逗号分隔遍历所有的子系统是否是传入的子系统。找不到的话就会报错，但是不得不吐槽的就是这个报错报的太没有诚意了，要是直接把找不到的子系统报出来，问题会直观很多。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/:0:2","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"结论 到此可以明白是agent隔离程序先mount了自定义目录cpu_mirror到cgroup目录下，然后影响到了java程序去获取正确的核数，为了修复特意执行了umount的操作，但是umount之后/proc/id/cgroup还是存在cpu_mirror相关信息而/proc/id/mountinfo中已经不存在了，在容器重新创建的时候进行检查进而报错。 对比线上其他docker版本，比如1.13.1中就没有此问题，因为1.13.1用的containerd中并没有上面提到的检验逻辑 通过这个问题也暴露出来我们在测试、灰度过程中的问题，由于线上环境复杂，系统版本众多、组件版本也不统一，在上线一个功能或者执行线上操作的时候，测试用例需要充分覆盖所有场景，灰度时也需要所有类型的机器至少都覆盖到了之后才可以放量继续靠扩大灰度范围，否则很容易出现类似的问题。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/:0:3","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"背景 容器原生设计为单进程模型，但公司线上运行的服务以多进程的方式运行，而且里面包含了很多的agent，例如日志采集、监控采集、数据配送等，耦合在了一个Container中，经过对线上资源使用率分析发现很大一部分资源消耗是在agent部分，而且与业务进程同时争抢业务容器申请的资源，彼此影响。虽然增量的容器部分agent迁移到了sidecar里面，解决了这些问题，但存量问题也需要解决，为此专门搞了一个项目用来优化这些问题。思想就是把agent进程从业务进程所在的cgroup中迁移出去，以不同cgroup层级存在，就可以避免相互影响，也可以限制各自资源大小，但是在灰度过程中发现部分Java容器服务开始出现毛刺。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-java-cpu/:0:1","tags":["docker","上云"],"title":"Java服务突现毛刺","uri":"https://www.likakuli.com/posts/docker-java-cpu/"},{"categories":["问题排查"],"content":"排查过程 java服务毛刺问题在最早上云的时候就出现过，当时是因为jdk版本太低，在容器内运行时无法正确获取容器申请的cpu大小，导致创建过多的线程，从而导致容器内的进程内部争抢过高，业务开始出现毛刺。在某个版本（jdk8u191）之后，开发者完全不需要关注程序是运行在物理机还是容器环境下。 对比有问题的容器内的业务进程使用的jdk版本，结果（202 \u003e 191）居然是没问题的版本，也就是已经可以自动识别cpu核数了，但为什么还是出现问题了呢？ 首先需要确定下容器内java服务获取到的真正的cpu核数，可以执行如下命令 java -XX:+UnlockDiagnosticVMOptions -XX:+PrintContainerInfo -version 输出结果中可以看到获取到的核数确实不是我们申请的核数，也就是说虽然采用了没问题的jdk版本，但还是获取到了错误的核数。 接下来就是看下为什么会获取到错误的核数信息，可以使用strace来分析java服务启动过程中的函数调用信息，其中在获取cpu核数的时候比较奇怪，正常是从cpu子系统获取，但是结果却显示从cpu_mirror获取，而这个目录下没有保存cpu核数的文件。后经过确定，这个目录就是前面提到的做agent隔离的时候自定义的挂在目录，容器内位于/sys/fs/cgroup下面。到此基本可以猜测是因为我们自己mount了一个目录到cgroup目录下，导致jdk获取到了错误目录，参数也获取错误。 最后，为什么jdk获取到了错误的路径呢？参考 Container Support doesn’t work for some Join Controllers combinations，代码比较好理解，其实就是有问题的版本在获取路径时采用的是字符串截取，而不是精确匹配，导致cpu_mirror这种目录会被误认为是正确目录。 老版处理/prof/self/mountinfo直接遍历每行，用strstr判断是否包含cpu、memory等，包含就是找到了，处理/proc/self/cgroup的时候根据 冒号 分隔，去掉第一列序号，然后剩下的再取第一列，然后用strstr比较是不是包含cpu、memory等 新版针对/proc/self/mountinfo读一条数据，最后一列根据 逗号 分隔，然后用strcmp比较是否和cpu、memory相等，处理 /proc/self/cgroup的时候根据 冒号 分隔，去掉第一列序号，然后剩下的再取第一列 ，然后根据逗号分隔，遍历结果用strcmp比较是否是cpu、memory等 总结就是老版本 直接if contains(str, “cpu”) then set cpu subsystme 新版本 for subStr in split(str, “,\") if subStr equals “cpu” then set cpu subsystem ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-java-cpu/:0:2","tags":["docker","上云"],"title":"Java服务突现毛刺","uri":"https://www.likakuli.com/posts/docker-java-cpu/"},{"categories":["问题排查"],"content":"解决方案 让业务升级jdk版本无法快速解决此问题，最终决定把挂载的cpu_mirror等问题目录umount掉，容器内看不到的话也就不会出问题了，经过测试发现可行，最终又在宿主上批量执行了umount的操作（为后面一个线上问题埋下了伏笔，详见这篇），长期方案是负责隔离的agent把问题目录改名，避免字符串匹配时匹配到。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-java-cpu/:0:3","tags":["docker","上云"],"title":"Java服务突现毛刺","uri":"https://www.likakuli.com/posts/docker-java-cpu/"},{"categories":["问题排查"],"content":"现象 线上k8s集群报警，宿主fd利用率超过80%，登陆查看dockerd内存使用26G ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:1:0","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"排查思路 由于之前已经遇到过多次dockerd资源泄露的问题，先看是否是已知原因导致的，参考前面两篇 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:0","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"fd的对端是谁？ 执行ss -anp | grep dockerd，结果如下图，可以看到和之前遇到的问题不同，第8列显示为0，与之前遇到的的情况不符，无法找到对端。 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:1","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"内存为什么泄露？ 为了可以使用pprof分析内存泄露位置，首先为dockerd打开debug模式，需要修改service文件，添加如下两句 ExecReload=/bin/kill -s HUP $MAINPID KillMode=process 同时在/etc/docker/daemon.json文件中添加 “debug”: true的配置，修改完之后执行systemctl daemon-reload重新加载docker服务配置，然后执行systemctl reload docker，重新加载docker配置，开启debug模式 dockerd默认使用uds对外提供服务，为了方便我们调试，可以使用socat对docker进行端口转发，如下 sudo socat -d -d TCP-LISTEN:8080,fork,bind=0.0.0.0 UNIX:/var/run/docker.sock，意思是外部可以通过访问宿主机的8080端口来调用docker api，至此一切就绪 在本地执行go tool pprof http://ip:8080/debug/pprof/heap查看内存使用情况，如下图 可以看到占用多的地方在golang自带的bufio NewWriterSize和NewReaderSize处，每次http调用都会都这里，也看出来有什么问题。 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:2","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"Goroutine也泄露？ 泄露位置 通过内存还是无法知道具体出问题的位置，问题不大，再看看goroutine的情况，直接在浏览器访问http://ip:8080/debug/pprof/goroutine?debug=1，如下图 一共1572822个goroutine，两个大头各占一半，各有786212个。看到这里基本就可以沿着文件行数去源码中查看了，这里我们用的docker 18.09.2版本，把源码切换到对应版本下，通过查看源码可以知道这两大类的goroutine泄露的原因，dockerd与containerd相关处理流程如下图 对应上图的话，goroutine泄露是由上面最后docker kill时的wait chan close导致的，wait的时候会启动另一个goroutine，每次docker kill都会造成这两个goroutine的泄露。对应代码如下 // Kill forcefully terminates a container. func (daemon *Daemon) Kill(container *containerpkg.Container) error { if !container.IsRunning() { return errNotRunning(container.ID) } // 1. Send SIGKILL if err := daemon.killPossiblyDeadProcess(container, int(syscall.SIGKILL)); err != nil { // While normally we might \"return err\" here we're not going to // because if we can't stop the container by this point then // it's probably because it's already stopped. Meaning, between // the time of the IsRunning() call above and now it stopped. // Also, since the err return will be environment specific we can't // look for any particular (common) error that would indicate // that the process is already dead vs something else going wrong. // So, instead we'll give it up to 2 more seconds to complete and if // by that time the container is still running, then the error // we got is probably valid and so we return it to the caller. if isErrNoSuchProcess(err) { return nil } ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second) defer cancel() if status := \u003c-container.Wait(ctx, containerpkg.WaitConditionNotRunning); status.Err() != nil { return err } } // 2. Wait for the process to die, in last resort, try to kill the process directly if err := killProcessDirectly(container); err != nil { if isErrNoSuchProcess(err) { return nil } return err } // Wait for exit with no timeout. // Ignore returned status. \u003c-container.Wait(context.Background(), containerpkg.WaitConditionNotRunning) return nil } // Wait waits until the container is in a certain state indicated by the given // condition. A context must be used for cancelling the request, controlling // timeouts, and avoiding goroutine leaks. Wait must be called without holding // the state lock. Returns a channel from which the caller will receive the // result. If the container exited on its own, the result's Err() method will // be nil and its ExitCode() method will return the container's exit code, // otherwise, the results Err() method will return an error indicating why the // wait operation failed. func (s *State) Wait(ctx context.Context, condition WaitCondition) \u003c-chan StateStatus { s.Lock() defer s.Unlock() if condition == WaitConditionNotRunning \u0026\u0026 !s.Running { // Buffer so we can put it in the channel now. resultC := make(chan StateStatus, 1) // Send the current status. resultC \u003c- StateStatus{ exitCode: s.ExitCode(), err: s.Err(), } return resultC } // If we are waiting only for removal, the waitStop channel should // remain nil and block forever. var waitStop chan struct{} if condition \u003c WaitConditionRemoved { waitStop = s.waitStop } // Always wait for removal, just in case the container gets removed // while it is still in a \"created\" state, in which case it is never // actually stopped. waitRemove := s.waitRemove resultC := make(chan StateStatus) go func() { select { case \u003c-ctx.Done(): // Context timeout or cancellation. resultC \u003c- StateStatus{ exitCode: -1, err: ctx.Err(), } return case \u003c-waitStop: case \u003c-waitRemove: } s.Lock() result := StateStatus{ exitCode: s.ExitCode(), err: s.Err(), } s.Unlock() resultC \u003c- result }() return resultC } 对照goroutine的图片，两个goroutine分别走到了Kill最后一次的container.Wait处、Wait的select处，正因为Wait方法的select一直不返回，导致resultC无数据，外面也就无法从container.Wait返回的chan中读到数据，从而导致每次docker stop调用阻塞两个goroutine。 为什么泄露？ 为什么select一直不返回呢？可以看到select在等三个chan，任意一个有数据或者关闭都会返回 ctx.Done()：不返回是因为最后一次调用Wait的时候传入的是context.Background()。这里其实也是dockerd对请求的处理方式，既然客户端要删除容器，那我就等着容器删除，什么时间删除什么时间退出，只要容器没删，就一直有个goroutine在等待。 waitStop和waitRemove：不返回是因为没收到containerd发来的task exit的信号，可以对照上图看下，在收到task exit后才会关闭chan 为什么没收","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:3","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"总结 Kubelet为了保证最终一致性，发现宿主上还有不应该存在的容器就会一直不断的去尝试删除，每次删除都会调用docker stop的api，与dockerd建立一个uds连接，dockerd删除容器的时候会启动一个goroutine通过rpc形式调用containerd来删除容器并等待最终删除完毕才返回，等待的过程中会另起一个goroutine来获取结果，然而containerd在调用runc去真正执行删除的时候因为容器内D进程，无法删除容器，导致没有发出task exit信号，dockerd的两个相关的goroutine也就不会退出。整个过程不断重复，最终就导致fd、内存、goroutine一步步的泄露，系统逐渐走向不可用。 回过头来想想，其实kubelet本身的处理都没有问题，kubelet是为了确保一致性，要去删除不应该存在的容器，直到容器被彻底删除，每次调用docker api都设置了timeout。dockerd的逻辑有待商榷，至少可以做一些改进，因为客户端请求时带了timeout，且dockerd后端在接收到task exit事件后是会去做container remove操作的，即使当前没有docker stop请求。所以可以考虑把最后传入context.Background()的Wait函数调用去掉，当前面带超时的Wait返回后直接退出就可以，这样就不会造成资源泄露了。 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:3:0","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"1. 背景 承接上文，近期我们排查弹性云线上几起故障时，故障由多个因素共同引起，列举如下： 弹性云在逐步灰度升级docker版本至 18.06.3-ce 由于历史原因，弹性云启用了docker服务的systemd配置选项 MountFlags=slave 为了避免dockerd重启引起业务容器重建，弹性云启用了 live-restore=true 配置，docker服务发生重启，dockerd与shim进程mnt ns不一致 在以上三个因素合力作用下，线上容器在重建与漂移场景下，出现删除失败的事件。 同样，文章最后也给出了两种解决方案： 长痛：修改代码，忽略错误 短痛：修改配置，一劳永逸 作为优秀的社会主义接班人，我们当然选择短痛了！依据官方提示 MountFlags=slave 与 live-restore=true 不能协同工作，那么我们只需关闭二者之一就能解决问题。 与我们而言，docker提供的 live-restore 能力是一个很关键的特性。docker重启的原因多种多样，既可能是人为调试因素，也可能是机器的非预期行为，当docker重启后，我们并不希望用户的容器也发生重建。似乎关闭 MountFlags=slave 成了我们唯一的选择。 等等，回想一下docker device busy问题解决方案，别人正是为了避免docker挂载泄漏而引起删除容器失败才开启的这个特性。 但是，这个17年的结论真的还具有普适性吗？是与不是，我们亲自验证即可。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:1","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"2. 对比实验 为了验证在关闭 MountFlags=slave 选项后，docker是否存在挂载点泄漏的问题，我们分别挑选了一台 1.13.1 与 18.06.3-ce 的宿主进行实验。实验步骤正如docker device busy问题解决方案所提示，在验证之前，环境准备如下： 删除docker服务的systemd配置项 MountFlags=slave 挑选启用systemd配置项 PrivateTmp=true 的任意服务，本文以 httpd 为例 下面开始验证： ////// docker 1.13.1 验证步骤及结果 // 1. 重新加载配置 [stupig@hostname2 ~]$ sudo systemctl daemon-reload // 2. 重启docker [stupig@hostname2 ~]$ sudo systemctl restart docker // 3. 创建容器 [stupig@hostname2 ~]$ sudo docker run -d nginx c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c // 4. 重启httpd [stupig@hostname2 ~]$ sudo systemctl restart httpd // 5. 停止容器 [stupig@hostname2 ~]$ sudo docker stop c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c // 6. 清理容器 [stupig@hostname2 ~]$ sudo docker rm c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c Error response from daemon: Driver overlay2 failed to remove root filesystem c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c: remove /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged: device or resource busy // 7. 定位挂载点 [stupig@hostname2 ~]$ grep -rwn /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged /proc/*/mountinfo /proc/19973/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19974/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19975/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19976/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19977/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19978/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // 8. 定位目标进程 [stupig@hostname2 ~]$ ps -ef|egrep '19973|19974|19975|19976|19977|19978' root 19973 1 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19974 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19975 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19976 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19977 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19978 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND docker 1.13.1 版本的实验结果正如网文所料，容器读写层挂载点出现了泄漏，并且 docker rm 无法清理该容器（注意 docker rm -f 仍然可以清理，原因参考上文）。 弹性云启用docker配置 MountFlags=slave 也是为了避免该问题发生。 那么现在压力转移到 docker 18.06.3-ce 这边来了，新版本是否仍然存在这个问题呢？ ////// docker 18.06.3-ce 验证步骤及结果 [stupig@hostname ~]$ sudo systemctl daemon-reload [stupig@hostname ~]$ sudo systemctl restart docker [stupig@hostname ~]$ sudo docker run -d nginx 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f [stupig@hostname ~]$ sudo systemctl restart httpd [stupig@hostname ~]$ sudo docker stop 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f [stupig@hostname ~]$ sudo docker rm 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f 针对docker 18.06.3-ce 的实验非常丝滑顺畅，不存在任何问题。回顾上文知识点，当容器读写层挂载点出现泄漏后，docker 18.06.3-","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:2","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"3. 蛛丝马迹 上一节对比实验的结果给了我们莫大的鼓励，本节我们探索两个版本的docker的表现差异，以期定位症结所在。 既然核心问题在于挂载点是否被泄漏，那么我们就以挂载点为切入点，深入分析两个版本docker的差异性。我们对比在两个环境下执行完 步骤4 后，不同进程内的挂载详情，结果如下： // docker 1.13.1 [stupig@hostname2 ~]$ sudo docker run -d nginx 0fe8d412f99a53229ea0df3ec44c93496e150a39f724ea304adb7f924910d61b [stupig@hostname2 ~]$ sudo docker inspect -f {{.GraphDriver.Data.MergedDir}} 0fe8d412f99a53229ea0df3ec44c93496e150a39f724ea304adb7f924910d61b /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged // 共享命名空间 [stupig@hostname2 ~]$ grep -rw /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged /proc/$$/mountinfo 223 1143 0:40 / /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX [stupig@hostname2 ~]$ sudo systemctl restart httpd [stupig@hostname2 ps -ef|grep httpd|head -n 1 root 16715 1 2 16:09 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND // httpd进程命名空间 [stupig@hostname2 ~]$ grep -rw /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged /proc/16715/mountinfo 257 235 0:40 / /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged rw,relatime shared:123 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // docker 18.06.3-ce [stupig@hostname ~]$ sudo docker run -d nginx ce75d4fdb6df6d13a7bf4270f71b3752ee2d3849df1f64d5d5d19a478ac7db8d [stupig@hostname ~]$ sudo docker inspect -f {{.GraphDriver.Data.MergedDir}} ce75d4fdb6df6d13a7bf4270f71b3752ee2d3849df1f64d5d5d19a478ac7db8d /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged // 共享命名空间 [stupig@hostname ~]$ grep -rw /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged /proc/$$/mountinfo 218 43 0:105 / /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged rw,relatime shared:109 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX [stupig@hostname ~]$ sudo systemctl restart httpd [stupig@hostname ~]$ ps -ef|grep httpd|head -n 1 root 63694 1 0 16:14 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND // httpd进程命名空间 [stupig@hostname ~]$ grep -rw /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged /proc/63694/mountinfo 435 376 0:105 / /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged rw,relatime shared:122 master:109 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX 咋一看，好像没啥区别啊！睁大你们的火眼金睛，是否发现差异所在了？ 如果细心对比，还是很容易分辨出差异所在的： 共享命名空间中 docker 18.06.3-ce 版本创建的挂载点是shared的 而docker 1.13.1 版本创建的挂载点是private的 httpd进程命名空间中 docker 18.06.3-ce 创建的挂载点仍然是共享的，并且接收共享组109传递的挂载与卸载事件，注意：共享组109正好就是共享命名空间中对应的挂载点 而docker 1.13.1 版本创建的挂载点虽然也是共享的，但是却与共享命名空间中对应的挂载点没有关联关系 可能会有用户不禁要问：怎么分辨挂载点是什么类型？以及不同类型挂载点的传递属性呢？请参阅：mount命名空间说明文档。 问题已然明了，由于两个版本docker所创建的容器读写层挂载点具备不同的属性，导致它们之间的行为差异。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:3","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"4. 刨根问底 相信大家如果理解了上一节的内容，就已经了解了问题的本质。本节我们继续探索问题的根因。 为什么两个版本的docker行为表现不一致？不外乎两个主要原因： docker处理逻辑发生变动 宿主环境不一致，主要指内核 第二个因素很好排除，我们对比了两个测试环境的宿主内核版本，结果是一致的。所以，基本还是因docker代码升级而产生的行为不一致。理论上，我们只需逐个分析docker 1.13.1 与 docker 18.06.3-ce 两个版本间的所有提交记录，就一定能够定位到关键提交信息，大力总是会出现奇迹。 但是，我们还是希望能够从现场中发现有用信息，缩小检索范围。 仍然从挂载点切入，既然两个版本的docker所创建的挂载点在共享命名空间中就已经出现差异，我们顺藤摸瓜，找找容器读写层挂载点链路上是否存在差异： // docker 1.13.1 // 本挂载点 [stupig@hostname2 ~]$ grep -rw /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged /proc/$$/mountinfo 223 1143 0:40 / /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // 定位本挂载点的父挂载点 [stupig@hostname2 ~]$ grep -rw 1143 /proc/$$/mountinfo 1143 44 8:4 /docker_rt/overlay2 /home/docker_rt/overlay2 rw,relatime - xfs /dev/sda4 rw,attr2,inode64,logbsize=256k,sunit=512,swidth=512,prjquota // 继续定位祖父挂载点 [stupig@hostname2 ~]$ grep -rw 44 /proc/$$/mountinfo 44 39 8:4 / /home rw,relatime shared:28 - xfs /dev/sda4 rw,attr2,inode64,logbsize=256k,sunit=512,swidth=512,prjquota // 继续往上 [stupig@hostname2 ~]$ grep -rw 39 /proc/$$/mountinfo 39 1 8:3 / / rw,relatime shared:1 - ext4 /dev/sda3 rw,stripe=64,data=ordered // docker 18.06.3-ce // 本挂载点 [stupig@hostname ~]$ grep -rw /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged /proc/$$/mountinfo 218 43 0:105 / /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged rw,relatime shared:109 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // 定位本挂在点的父挂载点 [stupig@hostname ~]$ grep -rw 43 /proc/$$/mountinfo 43 61 8:17 / /home rw,noatime shared:29 - xfs /dev/sdb1 rw,attr2,nobarrier,inode64,prjquota // 继续定位祖父挂载点 [stupig@hostname ~]$ grep -rw 61 /proc/$$/mountinfo 61 1 8:3 / / rw,relatime shared:1 - ext4 /dev/sda3 rw,data=ordered 两个版本的docker所创建的容器读写层挂载点链路上差异还是非常明显的： 容器读写层挂载点的父级挂载点不同 docker 18.06.3-ce 创建的容器读写层挂载点的父级挂载点是 /home/ ，并且是共享的 docker 1.13.1 创建的容器读写层挂载点的父级挂载点是 /home/docker_rt/overlay2 ，并且是私有的 这里补充一个背景，弹性云机器在初始化阶段，会将 /home 初始化为xfs文件系统类型，因此所有宿主上 /home 挂载点都具备相同属性。 那么，问题基本就是由 docker 1.13.1 中多出的一层挂载层 /home/docker_rt/overlay2 引起。 如何验证这个猜想呢？现在，其实我们已经具备了检索代码的关键目标，docker 1.13.1 会设置容器镜像层根目录的传递属性。拿着这个先验知识，我们直接查代码，检索过程基本没费什么功夫，直接展示相关代码： // filepath: daemon/graphdriver/overlay2/overlay.go func init() { graphdriver.Register(driverName, Init) } func Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) { if err := mount.MakePrivate(home); err != nil { return nil, err } supportsDType, err := fsutils.SupportsDType(home) if err != nil { return nil, err } if !supportsDType { // not a fatal error until v1.16 (#27443) logrus.Warn(overlayutils.ErrDTypeNotSupported(\"overlay2\", backingFs)) } d := \u0026Driver{ home: home, uidMaps: uidMaps, gidMaps: gidMaps, ctr: graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicOverlay)), supportsDType: supportsDType, } d.naiveDiff = graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps) if backingFs == \"xfs\" { // Try to enable project quota support over xfs. if d.quotaCtl, err = quota.NewControl(home); err == nil { projectQuotaSupported = true } } return d, nil } 很明显，问题就出在 mount.MakePrivate 函数调用上。 官方将 GraphDriver 根目录设置为 Private，本意是为了避免容器读写层挂载点泄漏。那为什么在高版本中去掉了这个逻辑呢？显然官方也意识到这么做并不能实现期望的目的，官方也在修复中给出了详细说明。 实际上，不设置 GraphDriver 根目录的传播属性，反而能避免绝大多数挂载点泄漏的问题。。。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:4","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"5. 结语 现在，我们已经了解了问题的来龙去脉，我们总结下问题的解决方案： 针对 1.13.1 版本docker，存量宿主较多，我们可以忽略 device or resource busy 问题，基本也不会给线上服务带来什么影响 针对 18.06.3-ce 版本docker，存量宿主较少，我们删除docker服务的systemd配置项 MountFlags，通过故障自愈解决docker卡在问题 在容器创建后，卸载容器读写层挂载，如果不影响容器内文件访问。那么可以直接卸载所有挂载点，修改docker配置，并重启docker服务【本方案尚未验证】 针对增量宿主，全部删除docker服务的systemd配置项 MountFlags ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:5","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":" 转载自组内同事stupig ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:0","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"1. 背景 近期，弹性云线上集群发生了几起特殊的容器漂移失败事件，其特殊之处在于容器处于Pod Terminating状态，而宿主则处于Ready状态。 宿主状态为Ready说明其能够正常处理Pod事件，但是Pod却卡在了退出阶段，说明此问题并非由kubelet引起，那么docker就是1号犯罪嫌疑人了。 下文将详细介绍问题的排查与分析全过程。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:1","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"2. 抽丝剥茧 2.1 排除kubelet嫌疑 Pod状态如下： [stupig@master ~]$ kubectl get pod -owide pod-976a0-5 0/1 Terminating 0 112m 尽管kubelet的犯罪嫌疑已经很小，但是我们还是需要排查kubelet日志进一步确认。截取kubelet关键日志片段如下： I1014 10:56:46.492682 34976 kubelet_pods.go:1017] Pod \"pod-976a0-5_default(f1e03a3d-0dc7-11eb-b4b1-246e967c4efc)\" is terminated, but some containers have not been cleaned up: {ID:{Type:docker ID:41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef} Name:stupig State:exited CreatedAt:2020-10-14 10:49:57.859913657 +0800 CST StartedAt:2020-10-14 10:49:57.928654495 +0800 CST FinishedAt:2020-10-14 10:50:28.661263065 +0800 CST ExitCode:0 Hash:2101852810 HashWithoutResources:2673273670 RestartCount:0 Reason:Completed Message: Resources:map[CpuQuota:200000 Memory:2147483648 MemorySwap:2147483648]} E1014 10:56:46.709255 34976 remote_runtime.go:250] RemoveContainer \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\" from runtime service failed: rpc error: code = Unknown desc = failed to remove container \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\": Error response from daemon: container 41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef: driver \"overlay2\" failed to remove root filesystem: unlinkat /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged: device or resource busy E1014 10:56:46.709292 34976 kuberuntime_gc.go:126] Failed to remove container \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\": rpc error: code = Unknown desc = failed to remove container \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\": Error response from daemon: container 41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef: driver \"overlay2\" failed to remove root filesystem: unlinkat /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged: device or resource busy 日志显示kubelet处于Pod Terminating状态的原因很清楚：清理容器失败。 kubelet清理容器的命令是 docker rm -f ，其失败的原因在于删除容器目录 xxx/merged 时报错，错误提示为 device or resource busy 。 除此之外，kubelet无法再提供其他关键信息。 登陆宿主，我们验证对应容器的状态： [stupig@hostname ~]$ sudo docker ps -a | grep pod-976a0-5 41020461ed4d Removal In Progress k8s_stupig_pod-976a0-5_default_f1e03a3d-0dc7-11eb-b4b1-246e967c4efc_0 f0a75e10b252 Exited (0) 2 minutes ago k8s_POD_pod-976a0-5_default_f1e03a3d-0dc7-11eb-b4b1-246e967c4efc_0 [stupig@hostname ~]$ sudo docker rm -f 41020461ed4d Error response from daemon: container 41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef: driver \"overlay2\" failed to remove root filesystem: unlinkat /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged: device or resource busy 问题已然清楚，现在我们有两种排查思路： 参考Google上解决 device or resource busy 问题的思路 结合现象分析代码 2.2 Google大法 有问题找Google！所以，我们首先咨询了Google，检索结果显示很多人都碰到了类似的问题。 而网络上主流的解决方案：配置docker服务MountFlags为slave，避免docker挂载点信息泄漏到其他mnt命名空间，详细原因请参阅：docker device busy问题解决方案。 这么简单？？？显然不能，检查发现docker服务当前已配置MountFlags为slave。网络银弹再次失去功效。 so，我们还是老老实实结合现场分析代码吧。 2.3 docker处理流程 在具体分析docker代码之前，先简单介绍下docker的处理流程，避免作为一只无头苍蝇处处碰壁。 清楚了docker的处理流程之后，我们再来分析现场。 2.4 提审docker 问题发生在docker清理阶段，docker清理容器读写层出错，报错信息为 device or resource busy，说明docker读写层并没有被正确卸载，或者是没有完全卸载。下面的命令可以验证这个结论： [stupig@hostname ~]$ grep -rwn '/home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged' /proc/*/mountinfo /proc/22283/mountinfo:50:386 542 0:92 / /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/22407/mountinfo:50:386 542 0:92 / /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/28454/mountinfo:50:386 542 0:92 / /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:2","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"3. 问题影响 既然所有版本的docker都存在这个问题，那么其影响是什么呢？ 在高版本docker中，其影响是显式的，会引起容器清理失败，进而造成Pod删除失败。 而在低版本docker中，其影响是隐式的，造成挂载点泄漏，进而可能会造成的影响如下： inode被打满：由于挂载点泄漏，容器读写层不会被清理，长时间累计可能会造成inode耗尽问题，但是是小概率事件 容器ID复用：由于挂载点未被卸载，当docker复用了原来已经退出的容器ID时，在挂载容器init层与读写层时会失败。由于docker生成容器ID是随机的，因此也是小概率事件 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:3","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"4. 解决方案 问题已然明确，如何解决问题成了当务之急。思路有二： 治标：对标 1.13.1 版本的处理逻辑，修改 18.06.3-ce 处理代码 治本：既然官方也提及 MountFlags=slave 与 live-restore 不能同时使用，那么我们修改两个配置选项之一即可 考虑到 重启docker不重启容器 这样一个强需求的存在，似乎我们唯一的解决方案就是关闭 MountFlags=slave 配置。关闭该配置后，与之而来的疑问如下： 能够解决本问题？ 网传其他systemd托管服务启用PrivateTmp是否会造成挂载点泄漏？ 欲知后事如何，且听下回分解！ ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:4","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":" 转载自组内同事 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:0","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"1. 背景 最近升级了一版kubelet，修复因kubelet删除Pod慢导致平台删除集群超时的问题。在灰度redis隔离集群的时候，发现升级kubelet并重启服务后，少量宿主状态变成了NotReady，并且回滚kubelet至之前版本，宿主状态仍然是NotReady。查看宿主状态时提示 ‘container runtime is down’ ，根据经验，此时一般就是容器运行时出了问题。弹性云使用的容器运行时是docker，我们就去检查docker的状态，检测结果如下： docker ps 查看所有容器状态，执行正常 docker inspect 查看某一容器详细状态，执行阻塞 典型的docker hang死行为。因为我们最近在升级docker版本，存量宿主docker的版本为1.13.1，并且在逐步升级至18.06.3，新宿主的docker版本都是18.06.3。docker hang死问题在1.13.1版本上表现得更彻底，在执行docker ps的时候就已经hang死了，一旦某个容器出了问题，docker就处于无响应状态；而docker 18.06.3做了一点小小的优化，在执行docker ps时去掉了针对容器级别的加锁操作，但是docker inspect依然会加容器锁，因此某一个容器出现问题，并不会造成docker服务不可响应，受影响的也仅仅是该容器，无法执行任何操作。 至于为什么以docker ps与docker inspect为指标检查docker状态，因为kubelet就是依赖这两个docker API获取容器状态。 所以，现在问题有二： docker hang死的根因是什么？ docker hang死时，为什么重启kubelet，会导致宿主状态变为NotReady？ ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:1","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"2.重启kubelet变更宿主状态 kubelet重启后宿主状态从Ready变为NotReady，这个问题相较docker hang死而言，没有那么复杂，所以我们先排查这个问题。 kubelet针对宿主会设置多个Condition，表明宿主当前所处的状态，比如宿主内存是否告急、线程数是否告急，以及宿主是否就绪。其中ReadyCondition表明宿主是否就绪，kubectl查看宿主状态时，展示的Statue信息就是ReadCondition的内容，常见的状态及其含义定义如下： Ready状态：表明当前宿主状态一切OK，能正常响应Pod事件 NotReady状态：表明宿主的kubelet仍在运行，但是此时已经无法处理Pod事件。NotReady绝大多数情况都是容器运行时出了问题 Unknown状态：表明宿主kubelet已停止运行 kubelet定义的ReadyCondition的判定条件如下： // defaultNodeStatusFuncs is a factory that generates the default set of // setNodeStatus funcs func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error { ...... setters = append(setters, nodestatus.OutOfDiskCondition(kl.clock.Now, kl.recordNodeStatusEvent), nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent), nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent), nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent), nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, validateHostFunc, kl.containerManager.Status, kl.recordNodeStatusEvent), nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse), // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event // and record state back to the Kubelet runtime object. In the future, I'd like to isolate // these side-effects by decoupling the decisions to send events and partial status recording // from the Node setters. kl.recordNodeSchedulableEvent, ) return setters } 深入nodestatus.ReadyCondition的实现可以发现，宿主是否Ready取决于很多条件，包含运行时判定、网络判定、基本资源判定等。这里我们只需关注运行时判定即可： func (s *runtimeState) runtimeErrors() []string { s.RLock() defer s.RUnlock() var ret []string if !s.lastBaseRuntimeSync.Add(s.baseRuntimeSyncThreshold).After(time.Now()) { // 1 ret = append(ret, \"container runtime is down\") } if s.internalError != nil { ret = append(ret, s.internalError.Error()) } for _, hc := range s.healthChecks { // 2 if ok, err := hc.fn(); !ok { ret = append(ret, fmt.Sprintf(\"%s is not healthy: %v\", hc.name, err)) } } return ret } 当出现如下两种状况之一时，则判定运行时检查不通过： 距最近一次运行时同步操作的时间间隔超过指定阈值（默认30s） 运行时健康检查未通过 那么，当时宿主的NotReady是由哪种状况引起的呢？结合kubelet日志分析，kubelet每隔5s就输出一条日志： ...... I0715 10:43:28.049240 16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down] I0715 10:43:33.049359 16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down] I0715 10:43:38.049492 16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down] ...... 因此，状况1是宿主NotReady的元凶。 我们继续分析为什么kubelet没有按照预期设置lastBaseRuntimeSync。kubelet启动时会创建一个goroutine，并在该goroutine中循环设置lastBaseRuntimeSync，循环如下： func (kl *Kubelet) Run(updates \u003c-chan kubetypes.PodUpdate) { ...... go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) ...... } func (kl *Kubelet) updateRuntimeUp() { kl.updateRuntimeMux.Lock() defer kl.updateRuntimeMux.Unlock() ...... kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules) kl.runtimeState.setRuntimeSync(kl.clock.Now()) } 正常情况下，kubelet每隔5s会将lastBaseRuntimeSync设置为当前时间，而宿主状态异常时，这个时间戳一直未被更新。也即updateRuntimeUp一直被阻塞在设置lastBaseRuntimeSync之前的某一步。我们只需逐个排查updateRuntimeUp内的函数调用即可，具体过程不再展示，最终的函数调用链路如下： initializeRuntimeDependentModules -\u003e kl.cadvisor.Start -\u003e cc.Manager.Start -\u003e self.createContainer -\u003e m.createContainerLocked -\u003e container.NewContainerHandler -\u003e factory.CanHandleAndAccept -\u003e self.client.ContainerInspect 由于某个容器状态异常，kubelet执行docker inspect操作也被hang死。 因此，重启kubelet引起宿主状态从Ready变为NotReady，其根因在于某个容器状态异常，执行docker inspect时被hang死。而如果docker inspect hang死发生在kubelet重启之后，则不会对宿主的Ready状态造成任何影响，因为oneTimeInitializer是sync.Once类型，也即仅仅会在kebelet启动时执行一次。那时kubelet仅仅是不能处理该Pod相关的任何事件，包含删除、变更等，但是仍然能够处理其他Pod的任意事件。 可能有人会问，为什么kubelet重启时访问docker inspect操作不加超时控制？确实，如果添加了超时控制，kubelet重启不会引起宿主状态变更。待详细挖掘后再来补充，我们先继续分析docker hang死的问题。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:2","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"3. docker hang死 我们对docker hang死并不陌生，因为已经发生了好多起。其发生时的现象也多种多样。以往针对docker 1.13.1版本的排查都发现了一些线索，但是并没有定位到根因，最终绝大多数也是通过重启docker解决。而这一次发生在docker 18.06.3版本的docker hang死行为，经过我们4人小分队接近一周的望闻问切，终于确定了其病因。注意，docker hang死的原因不止一种，因此本处方并非是个万能药。 现在，我们掌握的知识仅仅是docker异常了，无法响应特定容器的docker inspect操作，而对详细信息则一无所知。 3.1 链路跟踪 首先，我们希望对docker运行的全局状况有一个大致的了解，熟悉go语言开发的用户自然能联想到神器pprof。我们借助pprof描绘出了docker当时运行的蓝图： goroutine profile: total 722373 717594 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2132ef 0x7fe8bc238c1a 0x7fe8bd56f7fe 0x7fe8bd56f6bd 0x7fe8bcea8719 0x7fe8bcea938b 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711 # 0x7fe8bc2132ee sync.runtime_SemacquireMutex+0x3e /usr/local/go/src/runtime/sema.go:71 # 0x7fe8bc238c19 sync.(*Mutex).Lock+0x109 /usr/local/go/src/sync/mutex.go:134 # 0x7fe8bd56f7fd github.com/docker/docker/daemon.(*Daemon).ContainerInspectCurrent+0x8d /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:40 # 0x7fe8bd56f6bc github.com/docker/docker/daemon.(*Daemon).ContainerInspect+0x11c /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:29 # 0x7fe8bcea8718 github.com/docker/docker/api/server/router/container.(*containerRouter).getContainersByName+0x118 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/inspect.go:15 # 0x7fe8bcea938a github.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.getContainersByName)-fm+0x6a /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container.go:39 # 0x7fe8bcb726c9 github.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1+0xd9 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/experimental.go:26 # 0x7fe8bcb72b00 github.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1+0x400 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/version.go:62 # 0x7fe8bc71c26a github.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1+0x7aa /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/pkg/authorization/middleware.go:59 # 0x7fe8bcb85f49 github.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1+0x199 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/server.go:141 # 0x7fe8bc4b9895 net/http.HandlerFunc.ServeHTTP+0x45 /usr/local/go/src/net/http/server.go:1947 # 0x7fe8bc72a437 github.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP+0x227 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103 # 0x7fe8bcb849e1 github.com/docker/docker/api/server.(*routerSwapper).ServeHTTP+0x71 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router_swapper.go:29 # 0x7fe8bc4bc67d net/http.serverHandler.ServeHTTP+0xbd /usr/local/go/src/net/http/server.go:2694 # 0x7fe8bc4b88a2 net/http.(*conn).serve+0x652 /usr/local/go/src/net/http/server.go:1830 4175 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2132ef 0x7fe8bc238c1a 0x7fe8bcc2eccf 0x7fe8bd597af4 0x7fe8bcea2456 0x7fe8bcea956b 0x7fe8bcb73dff 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711 # 0x7fe8bc2132ee sync.runtime_SemacquireMutex+0x3e /usr/local/go/src/runtime/sema.go:71 # 0x7fe8bc238c19 sync.(*Mutex).Lock+0x109 /usr/local/go/src/sync/mutex.go:134 # 0x7fe8bcc2ecce github.com/docker/docker/container.(*State).IsRunning+0x2e /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/container/state.go:240 # 0x7fe8bd597af3 github.com/docker/docker/daemon.(*Daemon).ContainerStats+0xb3 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:3","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"4. 解决方案 当大家了解了docker hang死的成因之后，我们可以针对性的提出如下解决办法。 4.1 最直观的办法 既然docker exec可能会引起docker hang死，那么我们禁用系统中所有的docker exec操作即可。最典型的是kubelet的probe，当前我们默认给所有Pod添加了ReadinessProbe，并且是以exec的形式进入容器内执行命令。我们调整kubelet的探测行为，修改为tcp或者http probe即可。 这里虽然改动不大，但是涉及业务容器的改造成本太大了，如何迁移存量集群是个大问题。 4.2 最根本的办法 既然当前containerd-shim读pipe需要等待runc exec执行完毕，如果我们将读pipe的操作提前至runc exec命令执行之前，理论上也可以避免死锁。 同样。这种方案的升级成本太高了，升级containerd-shim时需要重启存量的所有容器，这个方案基本不可能通过。 4.3 最简单的办法 既然runc init阻塞在写pipe，我们主动读取pipe内的内容，也能让runc init顺利推出。 再将本解决方案自动化的过程中，如何能够识别如docker hang死是由于写pipe导致的，是一个小小的挑战。但是相对于以上两种解决方案，我认为还是值得一试，毕竟影响面微乎其微。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:4","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"5. 后续 docker hang死的原因远非这一种，本次排查的结果也并非适用于所有场景。希望各位看官能够根据自己的现场排查问题。另外查看linux文档，pipe capacity是可以设置的，高版本中since 4.5也可以通过设置/proc/sys/fs/pipe-user-pages-soft来解决，但是如文档所说，在4.9之前/proc/sys/fs/pipe-user-pages-soft还是存在一些bug的，至于采用什么办法解决，还得根据自己情况来做选择。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:5","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"1. 揭开面纱 周一，接到RD反馈线上容器网络访问存在异常，具体线上描述如下： 上游服务driver-api所有容器访问下游服务duse-api某一容器TCP【telnet测试】连接不通，访问其余下游容器均正常 上游服务容器测试下游容器IP连通性【ping测试】正常 从以上两点现象可以得出一个结论： 容器的网络设备存在，IP地址连通，但是容器服务进程未启动，端口未启动 但是，当我们和业务RD确认之后，发现业务容器状态正常，业务进程也正运行着。嗯，问题不简单。 此外，飞哥这边排查还有一个结论： arp反向解析duse-api特殊容器IP时，不返回MAC地址信息 当telnet失败后，立即执行arp，会返回MAC地址信息 当我们拿着arp解析的MAC地址与容器当前的MAC地址作比较时，发现MAC地址不一致。唔，基本上确定问题所在了，net ns泄漏了。执行如下命令验证： sudo ip netns ls | while read ns; do sudo ip netns exec $ns ip addr; done | grep inet | grep -v 127 | awk '{print $2}' | sort | uniq -c 确实发现该容器对应的IP出现了两次，该容器IP对应了两个网络命名空间，也即该容器的网络命名空间出现了泄漏。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:1","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"2. 误入迷障 当确定了问题所在之后，我们立马调转排查方向，重新投入到net ns泄漏的排查事业当中。 既然net ns出现了泄漏，我们只需要排查被泄露的net ns的成因即可。在具体定位之前，首先补充一个背景： ip netns 命令默认扫描 /var/run/netns 目录，从该目录下的文件读取net ns的信息 默认情况下，kubelet调用docker创建容器时，docker会将net ns文件隐藏，如果不做特殊处理，我们执行 ip netns 命令将看不到任何数据 当前弹性云为了方便排查问题，做了一个特殊处理，将容器的网络命名空间mount到 /var/run/netns 目录 【注意，这里有个大坑】 有了弹性云当前的特殊处理，我们就可以知道所有net ns的创建时间，也即 /var/run/netns 目录下对应文件的创建时间。 我们查看该泄漏ns文件的创建时间为2020-04-17 11:34:07，排查范围进一步缩小，只需从该时间点附近排查即可。 接下来，我们分析了该附近时间段，容器究竟遭遇了什么： 2020-04-17 11:33:26 用户执行发布更新操作 2020-04-17 11:34:24 平台显示容器已启动 2020-04-17 11:34:28 平台显示容器启动脚本执行失败 2020-04-17 11:36:22 用户重新部署该容器 2020-04-17 11:36:31 平台显示容器已删除成功 既然是容器网络命名空间泄漏，则说明再删除容器的时候，没有执行ns的清理操作。【注：这里由于基础知识不足，导致问题****排查绕了地球一圈】 我们梳理kubelet在该时间段对该容器的清理日志，核心相关日志展示如下： I0417 11:36:30.974674 37736 kubelet_pods.go:1180] Killing unwanted pod \"duse-api-sf-xxxxx-0\" I0417 11:36:30.976803 37736 plugins.go:391] Calling network plugin cni to tear down pod \"duse-api-sf-80819-0_default\" I0417 11:36:30.983499 37736 kubelet_pods.go:1780] Orphaned pod \"4ae28778-805c-11ea-a54c-b4055d1e6372\" found, removing pod cgroups I0417 11:36:30.986360 37736 pod_container_manager_linux.go:167] Attempt to kill process with pid: 48892 I0417 11:36:30.986382 37736 pod_container_manager_linux.go:174] successfully killed all unwanted processes. 简单描述流程： I0417 11:36:30.974674 根据删除容器执行，执行杀死Pod操作 I0417 11:36:30.976803 调用cni插件清理网络命名空间 I0417 11:36:30.983499 常驻协程检测到Pod已终止运行，开始执行清理操作，包括清理目录、cgroup I0417 11:36:30.986360 清理cgroup时杀死容器中还未退出的进程 I0417 11:36:30.986382 显示所有容器进程都已被杀死 这里提示一点：正常情况下，容器退出时，容器内所有进程都已退出。而上面之所以出现清理cgroup时需要杀死容器内未退出进程，是由于常驻协程的检测机制导致的，常驻协程判定Pod已终止运行的条件是： // podIsTerminated returns true if pod is in the terminated state (\"Failed\" or \"Succeeded\"). func (kl *Kubelet) podIsTerminated(pod *v1.Pod) bool { // Check the cached pod status which was set after the last sync. status, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok { // If there is no cached status, use the status from the // apiserver. This is useful if kubelet has recently been // restarted. status = pod.Status } return status.Phase == v1.PodFailed || status.Phase == v1.PodSucceeded || (pod.DeletionTimestamp != nil \u0026\u0026 notRunning(status.ContainerStatuses)) } 这个容器命中了第三个或条件：容器已被标记删除，并且所有业务容器都不在运行中（业务容器启动失败，根本就没运行起来过），但是Pod的sandbox容器可能仍然处于运行状态。 仅依据上面的kubelet日志，难以发现问题所在。我们接着又分析了cni插件的日志，截取cni在删除该Pod容器网络时的日志如下： [pid:98497] 2020/04/17 11:36:30.990707 main.go:89: ===== start cni process ===== [pid:98497] 2020/04/17 11:36:30.990761 main.go:90: os env: [CNI_COMMAND=DEL CNI_CONTAINERID=c2ef79f7596b6b558f0c01c0715bac46714eefd1e9966625a09414c7218e1013 CNI_NETNS=/proc/48892/ns/net CNI_ARGS=IgnoreUnknown=1;K8S_POD_NAMESPACE=default;K8S_POD_NAME=duse-api-sf-xxxxx-0;K8S_POD_INFRA_CONTAINER_ID=c2ef79f7596b6b558f0c01c0715bac46714eefd1e9966625a09414c7218e1013 CNI_IFNAME=eth0 CNI_PATH=/home/kaku/kakucloud/cni-plugins/bin LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin KUBE_LOGTOSTDERR=--logtostderr=false KUBE_LOG_LEVEL=--v=3 KUBE_ALLOW_PRIV=--allow-privileged=true KUBE_MASTER=--master=https://10.xxx.xxx.xxx:xxxx KUBELET_ADDRESS=--address=0.0.0.0 KUBELET_HOSTNAME=--hostname_override=10.xxx.xxx.xxx KUBELET_POD_INFRA_CONTAINER= KUBELET_ARGS=--network-plugin=cni --cni-bin-dir=/home/kaku/kakucloud/cni-plugins/bin --cni-conf-dir=/home/kaku/kakucloud/cni-plugins/conf --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl --log-dir=/var/log/kubernetes --stderrthreshold=3 --allowed-unsafe-sysctls=net.*,kernel.shm*,kernel.msg*,kernel.sem,fs.mqueue.* --pod-infra-container-image=registry.kaku.com/kakuk8s/pause:3.0 --eviction-hard= --image-gc-high-threshold=75 --image-gc-low-threshold=65 --feature-gates=KubeletPluginsWatcher=false --restart-count-limit=5 --last-upgrade-time=2019-07-01] [pid:98497] 2020/04/17 11:36:30.990771 main.go:91: stdin : {\"cniVersion\":\"0.3.0\",\"logDir\":\"/home/kaku/kakucloud/cni-plugins/acllogs\",\"name\":\"ddcloudcni\",\"type\":\"aclCni\"} [pid:98497] 2020/04/17 11:36:30.990790 main.go:181: failed to ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:2","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"3. 拨云现月 这个结论对我们来说，不是一个好消息。费力不小，不说南辕北辙，但是确实还未发现问题的根因。 为了进一步缩小问题排查范围，我们找内核组同学请教了一个基础知识： 在删除net ns时，如果该ns内仍有网络设备，系统自动先删除网络设备，然后再删除ns 掌握了这个基础知识，我们再来排查。既然原生k8s集群不存在net ns泄漏问题，那问题一定由我们定制的某个模块引起。由于net ns泄漏发生在node上，当前弹性云在node节点上部署的模块包含： kubelet cni plugins other tools 由于kubelet已经被排除嫌疑，那么罪魁祸首基本就是cni插件了。对比原生集群与弹性云线上集群的cni插件，发现一个极有可能会造成net ns泄漏的点： 定制的cni插件为了排查问题的方便，将容器的网络命名空间文件绑定挂载到了/var/run/netns 目录下 【参考上面的大坑】 我们赶紧着手验证元凶是否就是它。修改cni插件代码，删除绑定挂载操作，然后在测试环境验证。验证结果符合预期，net ns不在泄漏。至此，真相终于大白于天下了。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:3","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"4. 亡羊补牢 当初为net ns做一个绑定挂载，其目的就是为了方便我们排查问题，使得 ip netns 命令能够访问当前宿主上所有Pod的网络命名空间。 但其实一个简单的软链操作就能够实现这个目标。Pod退出时，如果这个软链文件未被清理，也不会引起net ns的泄漏，同时 ls -la /var/run/netns 命令可以清晰的看到哪些net ns仍有效，哪些已无效。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:4","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"5. 事后诸葛 为什么绑定挂载能够导致net ns泄漏呢？这是由linux 网络命名空间特性决定的： 只要该命名空间中仍有一个进程存活，或者存在绑定挂载的情况（可能还存在其他情况），该ns就不会被回收 而一旦所有进程都已退出，并且也无特殊状况，linux将自动回收该ns 最后，这个问题本身并不复杂，之所以问题存在如此之久，排查如此曲折，主要暴露了我们的基础知识有所欠缺。 好好学习，天天向上，方是王道！ ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:5","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"问题描述 在修复cgroup泄漏问题时会现停掉kubelet，待修复完成后启动kubelet组件，重启后收到业务反馈，业务容器重启了。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/:0:1","tags":["kubernetes"],"title":"Kubelet重启导致容器重启","uri":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/"},{"categories":["问题排查"],"content":"问题排查 这个问题具体原因的排查还是花了一定时间的，下面会列一下大致的排查思路并结合源码（自定义的1.12.4分支）进行分析。 排查过程中涉及到了3个容器，如下 名称 集群 宿主 结果 重启次数 1 auto-srv-cwhttp-sf-30b71-0 py 10.86.98.42 重启 1 2 conf-master-sf-19cf6-0 us01 10.15.29.31 重启 1 3 opensource-sf-dc750-2 us01 10.15.29.31 未重启 1 容器启停相关的组件首先想到的就是kubelet，因此去查看kubelet日志，拿py的举例，重启时间为2020-03-12 10:42:27，所以只需要看这之前的一些日志 这里直接贴出来最后过滤后的日志，省略一些中间过程 root@ddcloud-underlay-kube-node065.py:~$ grep 0312 /var/log/kubernetes/kubelet.INFO | grep -E \"10:42:26|10:42:27\" | grep -E \"8a61fda8d43e4c28d4092a1bc8e5f372846d955ffffe0353a754c2e42f271b56|auto-srv-cwhttp-sf-30b71-0|bfde4b15-cb98-11e8-a3c8-6c92bf85beda\" | grep -v -i volume I0312 10:42:26.101676 2353235 kubelet.go:1887] SyncLoop (ADD, \"api\"): \"lifejuhe-pre-sf-d46e3-0_default(9c048a53-53b2-11ea-b6ae-6c92bf85be08), prime-manager-sf-71a2e-2_default(35bc54e5-4ef1-11ea-b6ae-6c92bf85be08), ai-business-sf-0cac5-19_default(b60c0a18-4d70-11ea-b6ae-6c92bf85be08), disconf-sf-92894-2_default(4ae4dfb8-fee4-11e9-b433-6c92bf85be08), am-base-api-new-sf-de855-4_default(e59f4da9-cd68-11e9-92ca-6c92bf85be08), auto-srv-menu-sf-96f75-1_default(eb6ef2a6-4c7c-11ea-b6ae-6c92bf85be08), pre-000_default(d8f7688a-cc39-11e8-a3c8-6c92bf85beda), auto-wechat-srv-sf-e680a-0_default(9a6e0fc8-051c-11ea-b433-6c92bf85be08), oracle-pre-sf-253a7-0_default(c0cef405-5fa8-11ea-b6ae-6c92bf85be08), auto-srv-cwhttp-sf-30b71-0_default(bfde4b15-cb98-11e8-a3c8-6c92bf85beda), auto-srv-chewu-sf-dc750-0_default(1d9eac73-620a-11ea-b6ae-6c92bf85be08), member-sf-747bb-64_default(4e761a14-634e-11ea-b6ae-6c92bf85be08), ep-arch-tmp-bh-sf-1eaad-4_default(7a2d3f79-001a-11e9-a95b-6c92bf85beda), nsky-htw-h5-extranet-sf-900e8-1_default(10ac4571-08c5-11e9-a95b-6c92bf85beda), fate-saver-pre-sf-5cee9-0_default(39915cd3-f70f-11e9-b433-6c92bf85be08), shortserver-sf-ab776-4_default(d1909587-6362-11ea-b6ae-6c92bf85be08), rainbow-h5-sf-c72e4-3_default(cba38b41-1cbe-11ea-b433-6c92bf85be08), agency-call-service-sf-674c2-29_default(e74635f4-5b46-11ea-b6ae-6c92bf85be08), hnc-ddrccp-sf-db379-1_default(f16fbe5f-9ba4-11e9-a777-6c92bf85be08), log-hnc-sf-6a5be-152_default(a87148b5-c002-11e9-92ca-6c92bf85be08), actinia-service-py-sf-61fb7-0_default(75ef3250-3998-11e9-a95b-6c92bf85beda), its-timing-test-sf-8dcfc-0_default(3ad3b185-d175-11e8-a3c8-6c92bf85beda), de-st-forecastor-sf-5a382-3_default(5c6c02b8-ea6c-11e9-8a9f-6c92bf85be08), transit-compass-sf-69886-3_default(5f28569f-7152-11e9-9a4a-6c92bf85be08), gundam-centos6-002_default(3d075c28-042a-11e9-b650-6c92bf85be08), loan-credit-server-sf-c411b-4_default(6455d8e9-61fe-11ea-b6ae-6c92bf85be08), marketingmodel-service-sf-b1260-16_default(9d570b88-5f31-11e9-9b14-6c92bf85beda), csi-hnc-sf-a8523-16_default(b6078000-5f60-11ea-b6ae-6c92bf85be08), ms-shutdown-datadriver-sf-30b86-4_default(805e9837-f0d7-11e9-a648-6c92bf85be08), abnormal-shutdown-service-sf-342c4-18_default(ccb413e6-d832-11e9-92ca-6c92bf85be08), fate-lancer-sf-49896-2_default(241ad7ce-5c53-11ea-b6ae-6c92bf85be08), jetfire-sf-7dbf2-2_default(13f50c1d-5de8-11ea-b6ae-6c92bf85be08), hnc-pre-v-sf-745f9-0_default(70a98be0-3fbe-11e9-a95b-6c92bf85beda), orderpre-sf-b433d-0_default(03f84b30-1760-11ea-b433-6c92bf85be08), gatewayserver-sf-fc2e7-9_default(310030c4-62b3-11ea-b6ae-6c92bf85be08), base-message-service-sf-1b6f0-2_default(f63261ce-d0aa-11e9-92ca-6c92bf85be08), athena-api-pre-sf-d61bf-0_default(9f7e4183-0089-11ea-b433-6c92bf85be08), soda-f-api-sf-c88f2-2_default(fc9520db-8211-11e9-913a-6c92bf85beda), soda-d-api-py-sf-c6659-5_default(af4a67f9-3216-11ea-b433-6c92bf85be08), rollsroyce-pre-sf-6ed43-0_default(78a053cf-6375-11ea-b6ae-6c92bf85be08), member-sf-747bb-74_default(7323a872-634e-11ea-b6ae-6c92bf85be08), settle-consumer-abtest-hnc-pre-v-sf-b18cf-0_default(d199f994-6363-11ea-b6ae-6c92bf85be08), dpub-vote-pre-sf-bc747-0_default(7388d4f7-1a82-11ea-b433-6c92bf85be08), delta-hub-web-sf-bc67e-2_default(35699c85-620c-11ea-b6ae-6c92bf85be08), drunkeness-model-service-sf-a38a4-43_default(c3cd5daa-5b47-11ea-b6ae-6c92bf85be08), lifeapi-hnc-sf-4a88c-2_default(46","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/:0:2","tags":["kubernetes"],"title":"Kubelet重启导致容器重启","uri":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/"},{"categories":["问题排查"],"content":"结论 赋值和获取值的操作在两个goroutine，且严重依赖于第一次的赋值操作，所以应该在保证第一次赋值后再进行取值操作才能确保容器不重启，虽然分支3判断有没有默认探针中在设置和获取的时候都加了锁，但还是无法保证代码执行顺序，所以即使走分支3，也有可能会出现设置ready为true的情况（本该设置为false），但是因为这正是我们想要的效果，所以我们是不会觉察到这种情况的问题的，也就是说开源的代码中可能也是存在类似误判的风险的，至少1.12.4版本中是存在的。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/:0:3","tags":["kubernetes"],"title":"Kubelet重启导致容器重启","uri":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/"},{"categories":["问题排查"],"content":"knative健康检查","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 在支持了前面的定制功能后，集群中部署ksvc服务时会报IngressNotConfigured错误 ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:1","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"原因分析 首先根据错误提示及日志信息，可以发现是在做健康检查的时候出的问题，期望得到200，但是得到了404 func (m *Prober) probeVerifier(item *workItem) prober.Verifier { return func(r *http.Response, _ []byte) (bool, error) { // In the happy path, the probe request is forwarded to Activator or Queue-Proxy and the response (HTTP 200) // contains the \"K-Network-Hash\" header that can be compared with the expected hash. If the hashes match, // probing is successful, if they don't match, a new probe will be sent later. // An HTTP 404/503 is expected in the case of the creation of a new Knative service because the rules will // not be present in the Envoy config until the new VirtualService is applied. // No information can be extracted from any other scenario (e.g. HTTP 302), therefore in that case, // probing is assumed to be successful because it is better to say that an Ingress is Ready before it // actually is Ready than never marking it as Ready. It is best effort. switch r.StatusCode { case http.StatusOK: hash := r.Header.Get(network.HashHeaderName) switch hash { case \"\": m.logger.Errorf(\"Probing of %s abandoned, IP: %s:%s: the response doesn't contain the %q header\", item.url, item.podIP, item.podPort, network.HashHeaderName) return true, nil case item.ingressState.hash: return true, nil default: m.logger.Warnf(\"unexpected hash: want %q, got %q\", item.ingressState.hash, hash) return true, nil } // 日志中报错的地方，探活希望得到200，但是得到了404 case http.StatusNotFound, http.StatusServiceUnavailable: return false, fmt.Errorf(\"unexpected status code: want %v, got %v\", http.StatusOK, http.StatusNotFound) default: m.logger.Errorf(\"Probing of %s abandoned, IP: %s:%s: the response status is %v, expected 200 or 404\", item.url, item.podIP, item.podPort, r.StatusCode) return true, nil } } } 其实这时候大致也能猜到是什么原因了，因为我们定制了通过USN进行过滤，探活的时候，Url中其实是没有USN的。下一步就是顺藤摸瓜，找到探活对应的代码验证我们的猜想，也比较简单 // processWorkItem processes a single work item from workQueue. // It returns false when there is no more items to process, true otherwise. func (m *Prober) processWorkItem() bool { ... // probePath /healthz probeURL.Path = path.Join(probeURL.Path, probePath) ok, err := prober.Do( item.context, transport, probeURL.String(), prober.WithHeader(network.UserAgentKey, network.IngressReadinessUserAgent), prober.WithHeader(network.ProbeHeaderName, network.ProbeHeaderValue), m.probeVerifier(item)) ... } 可以看到探活的时候就是拿Path拼上/heathz，验证了我们的猜想 ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:2","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"修复 修改也就比较简单了，在添加wotkitem时，预先把USN添加到path中即可 func (l *gatewayPodTargetLister) ListProbeTargets(ctx context.Context, ing *v1alpha1.Ingress) ([]status.ProbeTarget, error) { ... // Use sorted hosts list for consistent ordering. for i, host := range gatewayHosts[gatewayName].List() { newURL := *target.URLs[0] newURL.Host = host + \":\" + target.Port var usn string if ing.Annotations != nil { usn = ing.Annotations[\"serverless.kakuchuxing.com/usn\"] } newURL.Path = path.Join(newURL.Path, usn) qualifiedTarget.URLs[i] = \u0026newURL ... } ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:3","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"总结 通过这个问题也看到了对于一些细节和关键流程掌握的还不够，还是需要进行系统性的学习。至于健康检查的逻辑，和k8s的健康检查稍有不同，参考这篇文章 ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:4","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["定制开发"],"content":"通过header访问指定版本","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 最近在搭建公司级的serverless平台，需要用到域名来访问内部服务，采取的是通过PATH来区分不同的服务，域名采用同一个。上一篇已经解决了通过Path访问不同服务的问题，但是在灰度过程中可能会想测试下新版本时候正常，如何将流量打到指定版本上呢？原生的knative是通过url的不同实现的，可以配置一个根据版本生成url的模板，设置后不同版本的服务url不同。但是我们的场景是所有服务url相同，于是我们约定通过在设置特殊的header的来实现此功能 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/:0:1","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"方案 原生通过url来区分不同版本，实现方式是通过在生成vs时，设置其Match的条件Authroty为对应的url prefix即可。显然无法满足我们当前统一使用一个url的场景。但是我们可以参考其实现方式，换一个维度，靠header实现即可，但是又不能影响正常访问，即不添加header的时候，流量按照设置的比例打到不同的revision上，添加了header后，需要将流量打到指定版本，所以不能简单的在Match中添加Header，需要分别设置正常访问的情况和访问指定版本的情况，且访问指定版本的配置应该顺序靠前。 // MakeIngressSpec creates a new IngressSpec func MakeIngressSpec( ctx context.Context, r *servingv1.Route, tls []v1alpha1.IngressTLS, targets map[string]traffic.RevisionTargets, visibility map[string]netv1alpha1.IngressVisibility, acmeChallenges ...v1alpha1.HTTP01Challenge, ) (v1alpha1.IngressSpec, error) { ... // add custom external domains customHostStr := r.Annotations[\"serverless.kakuchuxing.com/domains\"] // 倒序，否则不生效，因为访问指定版本时name不为空，不区分版本时name默认为空 sort.Sort(sort.Reverse(sort.StringSlice(names))) if len(customHostStr) \u003e 0 { customHosts := strings.Split(customHostStr, \";\") for _, name := range names { if name != \"default\" { visibility := netv1alpha1.IngressVisibilityExternalIP rule := *makeIngressRule(customHosts, r.Namespace, visibility, name, targets[name]) // If this is a public rule, we need to configure ACME challenge paths. rule.HTTP.Paths = append( makeACMEIngressPaths(challengeHosts, customHosts), rule.HTTP.Paths...) rules = append(rules, rule) } } } ... } func makeIngressRule(domains []string, ns string, visibility netv1alpha1.IngressVisibility, name string, targets traffic.RevisionTargets) *v1alpha1.IngressRule { ... return \u0026v1alpha1.IngressRule{ Hosts: domains, Visibility: visibility, HTTP: \u0026v1alpha1.HTTPIngressRuleValue{ Paths: []v1alpha1.HTTPIngressPath{ { Splits: splits, // TODO(lichuqiang): #2201, plumbing to config timeout and retries. // 把tag name保存下来，传递给vs，用来区分是否需要设置header AppendHeaders: map[string]string{ \"RevisionName\": name, }, }, }, }, } } func makeVirtualServiceRoute(hosts sets.String, usn string, http *v1alpha1.HTTPIngressPath, gateways map[v1alpha1.IngressVisibility]sets.String, visibility v1alpha1.IngressVisibility) *istiov1alpha3.HTTPRoute { ... // add revision tag header to custom domain // 获取传递过来的tag，设置match header if tag := http.AppendHeaders[\"RevisionName\"]; tag != \"\" { for i := 0; i \u003c len(matches); i++ { if matches[i].Headers == nil { matches[i].Headers = make(map[string]*istiov1alpha3.StringMatch) } matches[i].Headers[\"RevisionName\"] = \u0026istiov1alpha3.StringMatch{ MatchType: \u0026istiov1alpha3.StringMatch_Exact{ Exact: http.Splits[0].ServiceName, }, } } } ... } ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/:0:2","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"总结 至此，已经实现了通过统一域名访问集群内服务，且根据Path转发请求，并且可以通过在访问时添加指定的header来把流量打到指定版本上，这在灰度或者测试时是一个非常实用的功能。 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/:0:3","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"Knative根据Path转发请求","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["定制开发"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 最近在搭建公司级的serverless平台，需要用到域名来访问内部服务，采取的是通过PATH来区分不同的服务，域名采用同一个。这与原生knative的设计存在差异，原生的做法是每个服务一个自己的域名，通过域名把流量打到不同的服务上，我们已经在上一篇中解决了自定义域名无法访问knative集群的问题，这一篇来解决如何通过不同的Path访问到不同的服务 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/:0:1","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["定制开发"],"content":"方案 两个问题需要我们来解决： 不同服务的Path可能相同，如何区分 原生通过ksvc的方式不支持设置Path（通过自己创建各种类型的资源可以实现，但是控制比较复杂，而且上层需要修改适配） 解决方案： 每个服务一个USN，使用USN作为唯一标识 修改knative，支持通过Path访问 转发后需要rewrite url，把USN去掉，因为业务代码中的路由里不可能包含USN 其中第一点不需要代码改动，我们主要来实现第二、三点。 vs本身是支持根据Path转发的功能的，但是并没有在ksvc中暴露出来，所以我们需要在king创建vs的时候动态注入进去，同时在destination中添加url rewrite的逻辑。 func makeVirtualServiceSpec(ing *v1alpha1.Ingress, gateways map[v1alpha1.IngressVisibility]sets.String, hosts sets.String) *istiov1alpha3.VirtualService { spec := istiov1alpha3.VirtualService{ Hosts: hosts.List(), } // 自定义功能 usn := ing.Annotations[\"serverless.kakuchuxing.com/usn\"] if usn != \"\" { usn = \"/\" + strings.Trim(usn, \"/\") + \"/\" } gw := sets.String{} for _, rule := range ing.Spec.Rules { for _, p := range rule.HTTP.Paths { hosts := hosts.Intersection(sets.NewString(rule.Hosts...)) if hosts.Len() != 0 { http := makeVirtualServiceRoute(hosts, usn, \u0026p, gateways, rule.Visibility) // Add all the Gateways that exist inside the http.match section of // the VirtualService. // This ensures that we are only using the Gateways that actually appear // in VirtualService routes. for _, m := range http.Match { gw = gw.Union(sets.NewString(m.Gateways...)) } // rewrite path，重定向，消除USN if usn != \"\" { if http.Rewrite == nil { http.Rewrite = \u0026istiov1alpha3.HTTPRewrite{} } http.Rewrite.Uri = \"/\" } spec.Http = append(spec.Http, http) } } } spec.Gateways = gw.List() return \u0026spec } func makeVirtualServiceRoute(hosts sets.String, usn string, http *v1alpha1.HTTPIngressPath, gateways map[v1alpha1.IngressVisibility]sets.String, visibility v1alpha1.IngressVisibility) *istiov1alpha3.HTTPRoute { ... for _, host := range hosts.List() { g := gateways[visibility] if strings.HasSuffix(host, clusterDomainName) \u0026\u0026 len(gateways[v1alpha1.IngressVisibilityClusterLocal]) \u003e 0 { // For local hostname, always use private gateway g = gateways[v1alpha1.IngressVisibilityClusterLocal] } matches = append(matches, makeMatch(host, usn, http.Path, g)...) } ... } func makeMatch(host string, usn string, pathRegExp string, gateways sets.String) []*istiov1alpha3.HTTPMatchRequest { ... // add custom usn filter，添加USN的过滤条件 if usn != \"\" { if i == 0 { matches[i].Uri = \u0026istiov1alpha3.StringMatch{ MatchType: \u0026istiov1alpha3.StringMatch_Prefix{Prefix: usn}, } } else { matches[i].Uri = \u0026istiov1alpha3.StringMatch{ MatchType: \u0026istiov1alpha3.StringMatch_Exact{Exact: strings.TrimRight(usn, \"/\")}, } } } ... } 修改比较简单，完全就是按照之前说的两点进行的。其中有一个比较tricky的地方就是实现url rewrite的方式，因为社区中的vs（istio里的crd）其实是存在问题的，我们为了规避这个问题，特意做了一些特殊设置。参考这里，大致意思就是目前vs不支持url rewrite为空，rewrite为空之后，实际访问的时候需要在url的最后加上/，否则会返回400，但是我们很多前端网站主页就是一个域名，后面不跟任何内容，那这时候就有问题了，总不能再告诉用户在最后输入一个/。规避方案其实也比较简单，就是上面代码中最后makeMatch处的if else语句，且一定要保证顺序，即最长的要在前面，因为遇到第一个匹配的规则后，后续规则会被忽略。 http: - match: - uri: prefix: \"/echo/\" - uri: prefix: \"/echo\" rewrite: uri: \"/\" 如果顺序颠倒，那么当访问/echo/abc时，会重定向到//abc，返回404错误。 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/:0:2","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["定制开发"],"content":"总结 至此，已经支持通过统一域名访问，且通过Path把请求转发到不通的服务 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/:0:3","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["使用说明"],"content":"knative build docker image","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["使用说明"],"content":"背景 knative 0.14.0 最近在搭建公司级的serverless平台，遇到某些问题，看了源码发现无法通过其扩展机制来解决，遂决定修改源码来解决 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/:0:1","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["使用说明"],"content":"过程 源码很快修改完了，本地编译通过，knative的组件是容器化运行的，这就需要我们再制作镜像，但是浏览完官方github项目，并未发现有Dockerfile文件，于是决定使用逆向方法通过image反推出来Dockerfile，于是利用之前保存的的shell脚本进行反向解析，如下 docker history --no-trunc 8b13dd01e81b | tac | tr -s ' ' | cut -d \" \" -f 5- | sed 's,^/bin/sh -c #(nop) ,,g' | sed 's,^/bin/sh -c,RUN,g' | sed 's, \u0026\u0026 ,\\n \u0026 ,g' | sed 's,\\s*[0-9]*[\\.]*[0-9]*\\s*[kMG]*B\\s*$,,g' | head -n -1 bazel build ... ko publish knative.dev/net-istio/cmd/webhook 463kB kodata contents, at $KO_DATA_PATH ko publish knative.dev/net-istio/cmd/webhook 52.9MB go build output, at /ko-app/webhook WTF? 这和我认知里的Dockerfile完全不是一回事啊，赶紧google，首先google搜索了bazel，然后区项目中查看，并没有发现有啥相关的文件，倒是有个.ko.yaml的文件，里面有一条语句，是个镜像名称，然后google搜索了ko，果然，大公司就是不一样，一个ko解决了从diamante编译，打镜像，上传镜像，部署到k8s集群中的所有步骤（心中暗自感叹google是真的牛），当然也支持只把镜像load到本地，而不进行push，也不在k8s中创建，加个–local就好了。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/:0:2","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["使用说明"],"content":"总结 其实整个过程还是花了较长时间的，主要有两个原因 欠缺某些知识：这种情况下我们往往无法直接找到正确答案，只能通过踩坑之后逐步排除掉错误答案，才能一步步的找到正确的答案 knative比较新（0.14.0），网上很难找到需要的答案 整个过程虽然花费较多时间，但是收获颇丰。之所以写这一篇内容，也是希望为后来人解决一下此类烦恼，在比较紧急时，为大家节省时间，希望可以帮助到一部分人。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/:0:3","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["定制开发"],"content":"通过外部域名访问knative集群内的服务","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 最近在搭建公司级的serverless平台，需要用到域名来访问内部服务，采取的是通过PATH来区分不同的服务 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:1","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"问题 申请完域名后，分别通过域名和IP:PORT形式访问已部署的helloworld服务 curl -v -H \"Host: api-test.sls.intra.kaku.com\" http://api-test.sls.intra.kaku.com/ * Trying 10.88.128.112... * TCP_NODELAY set * Connected to api-test.sls.intra.kaku.com (10.88.128.112) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: api-test.sls.intra.kaku.com \u003e User-Agent: curl/7.54.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 426 Upgrade Required \u003c Date: Thu, 09 Jul 2020 11:59:20 GMT \u003c Content-Length: 0 \u003c Connection: keep-alive \u003c server: istio-envoy \u003c * Connection #0 to host api-test.sls.intra.kaku.com left intact # 10.190.16.26 为 knative-ingress-gateway的容器IP curl -v -H \"Host:api-test.sls.intra.kaku.com\" http://10.190.16.26/ * Trying 10.190.16.26... * TCP_NODELAY set * Connected to 10.190.16.26 (10.190.16.26) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host:api-test.sls.intra.kaku.com \u003e User-Agent: curl/7.54.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 404 Not Found \u003c date: Thu, 09 Jul 2020 12:03:05 GMT \u003c server: istio-envoy \u003c content-length: 0 \u003c * Connection #0 to host 10.190.16.26 left intact 可以看到都无法正常返回，通过域名访问的时候返回了426，通过IP:PORT访问的时候返回了404。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:2","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"排查 426 Upgrade Required 这个问题直接google一搜就出来答案了，参考 这里，其实这是envoy的能力，只要在envoy运行的容器中设置ISTIO_META_HTTP10环境变量为\"1\"问题就解决了，即**ISTIO_META_HTTP10: '\"1\"'** 404 Not Found 这个问题就涉及到VirtualService了，简称vs，在介绍vs之前我们先大致过一下knative创建集群的流程 假设我们通过kubectl操作，此时我们通过kubectl create -f helloworld.yaml的方式创建ksvc服务，如果集群各组件正常工作，且ksvc内容正确，那么稍微过一会就可以在集群中看到我们的服务了，我们需要做的仅仅是执行一条命令而已。可以看到knative封装的太好了，极大的简化了用户操作，对于对集群没有高级需求的用户非常友好，同时也有利于我们快速入门，否则，如果要执行一堆命令的话，就真的可以从入门到放弃了 但是我们毕竟是管理员，还是要对自己提高要求的，一定要搞清楚里面的原理，各组件之间的交互，否则系统对于我们来说就完全是个黑盒，不出问题还好，出问题就傻眼了。了解源码也是必须的，说到源码，只能感叹knative的源码要比k8s的源码封装的好太多了，其中一个原因也使得益于k8s提供的丰富的扩展机制：crd、operator、informer、webhook等。knative的源码真应该也值得拿出来一起分享，仔细研读。 回到正题，网路路由能力我们选择的是istio，我们大致分两种类型的资源进行介绍，和网络有关的 vs 和网络无关的 和网络无关的资源创建流程 ksvc --\u003e configuration --\u003e revision --\u003e deployment 和网络有关的资源创建流程 ksvc --\u003e route --\u003e king--\u003e virtualservice 我们的问题是和网络有关的，所以重点关注下面这个流程，最终对接istio的是vs，于是我们直接去看vs的配置，发现和域名相关的有两个地方，spec.hosts 和 spec.http.match.authority，于是想到的最简单的修改方式就是把我们的域名加入到spec.hosts中，去掉spec.http.match.authority，通过看代码发现这两处并没有可以修改的机制，于是想到利用MutatingWebhook来实现修改 控制这两个属性的地方都在net-istio的controller中，webhook对应的是net-istio的webhook，按照上面的分析，我们需要在webhook中添加对应的代码，主要改动两个地方，如下 // 注册virtualservice类型，表示要对其进行mutate的操作，我们只需要在此注册即可，controller会自动修改对应的MutatingWebhookConfiguration，添加对应的资源和操作 var types = map[schema.GroupVersionKind]resourcesemantics.GenericCRD{ appsv1.SchemeGroupVersion.WithKind(\"Deployment\"): \u0026defaults.IstioDeployment{}, v1alpha3.SchemeGroupVersion.WithKind(\"VirtualService\"): \u0026defaults.IstioVirtualService{}, } // pkg/defaults/virtualservice_default.go，以去掉match.authority举例 package defaults import ( \"context\" \"istio.io/client-go/pkg/apis/networking/v1alpha3\" \"knative.dev/pkg/apis\" ) type IstioVirtualService struct { v1alpha3.VirtualService `json:\",inline\"` } func (vs *IstioVirtualService) Validate(context.Context) *apis.FieldError { return nil } func (vs *IstioVirtualService) SetDefaults(ctx context.Context) { for _, http := range vs.Spec.Http { for _, match := range http.Match { match.Authority = nil } } } 可以看到整体修改很少，修改完之后重新编译，制作镜像，修改线上Pod的Image，触发原地重启，然后删除掉原有的vs，新的vs自动生成，查看新的vs，wtf？ 居然和之前一样，没有实现我们的效果，查kube-apiserver日志没有看到在创建vs时调用webhook，查看webhook的日志，也没有发现调用，但是在创建deployment时却会调用，然后查看webhook的配置，发现资源里也已经加上了，查了好久还是没有找到原因，不知道是哪个姿势不对了，由于时间关系暂时换另一种方式实现。 因为vs是由king创建的，所以在创建king的地方修改，这样在king创建vs的时候会自动带上我们自定义的domains，如下 // 通过annotation的方式，把需要添加到hosts中的域名放到annotation中 // MakeIngressSpec creates a new IngressSpec func MakeIngressSpec( ctx context.Context, r *servingv1.Route, tls []v1alpha1.IngressTLS, targets map[string]traffic.RevisionTargets, visibility map[string]netv1alpha1.IngressVisibility, acmeChallenges ...v1alpha1.HTTP01Challenge, ) (v1alpha1.IngressSpec, error) { ... // add custom external domains customHostStr := r.Annotations[\"serverless.kakuchuxing.com/domains\"] sort.Sort(sort.Reverse(sort.StringSlice(names))) if len(customHostStr) \u003e 0 { customHosts := strings.Split(customHostStr, \";\") for _, name := range names { if name != \"default\" { visibility := netv1alpha1.IngressVisibilityExternalIP rule := *makeIngressRule(customHosts, r.Namespace, visibility, name, targets[name]) // If this is a public rule, we need to configure ACME challenge paths. rule.HTTP.Paths = append( makeACMEIngressPaths(challengeHosts, customHosts), rule.HTTP.Paths...) rules = append(rules, rule) } } } ... } 首先修改ksvc，添加对应的annotaiton，然后继续之前的操作进行编译，打镜像，原地升级，删除vs，新的vs自送生成，此时可以看到已经使我们期望的效果了，然后用域名访问，HelloWorld终于可以正常访问了。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:3","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"总结 问题是解决了，但是为什么通过webhook的方式不生效，现象看起来是没调用webhook，还需要再去看下k8s有关webhook调用的部分的代码，很可能又是一个知识盲区。 knative中很多类型的属性并没有在上层暴露，导致无法直接使用ksvc进行管理，要么改源码，要么自己负责管理原本由ksvc统一管理的组件，虽然更加灵活，但是使用成本也更高，违背ksvc设计的初衷 通过此次问题排查，学习到了knative整个流程、原理，理清了各组件的交互，对后续问题排查有很大的帮助 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:4","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["问题排查"],"content":"endpoint变化","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"背景 k8s 1.12.4 包含自定义功能 线上集群在批量原地升级时出现流量异常问题，大体流程如下： 批量摘流，并等待7秒 批量删除容器 watch到Endpoint ready 变化，汇总2s内的变化，摘流或者接流（通用的处理方式，幂等） 原地升级是靠修改image实现的，利用的就是k8s原生的能力。第三步中为了降低对第三方API的访问次数，等待2s，汇总2s内所有变化统一调用一次API来进行摘流或者接流。问题表现为上述过程中容器先摘流，再接流（异常），再摘流，最后再接流，期望的场景是容器摘流，完后等待容器重启，正常之后再接流。 ","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/:0:1","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"分析 近期上线了原地重建的功能，出问题的集群都是使用此功能进行发布更新，所以猜测可能和这个功能有关系。在删除集群或者批量漂移容器时，也涉及对应流程，但是一直没有问题，总的排查方向如下： endpoint 变化机制 为什么批量删除时没有出现问题 原地升级和删除有什么差异 Endpoint变化机制 众所周知，k8s针对不同的资源类型会有相应的controller与之对应，控制其及其关联资源的生命周期的变化，Endpoint也不例外，在kube-controller-manager中有endpoint controller，查看其逻辑，主要相关的部分如下所示 func (e *EndpointController) syncService(key string) error { ... subsets := []v1.EndpointSubset{} var totalReadyEps int = 0 var totalNotReadyEps int = 0 for _, pod := range pods { if len(pod.Status.PodIP) == 0 { glog.V(5).Infof(\"Failed to find an IP for pod %s/%s\", pod.Namespace, pod.Name) continue } if !tolerateUnreadyEndpoints \u0026\u0026 pod.DeletionTimestamp != nil { glog.V(5).Infof(\"Pod is being deleted %s/%s\", pod.Namespace, pod.Name) continue } epa := *podToEndpointAddress(pod) hostname := pod.Spec.Hostname if len(hostname) \u003e 0 \u0026\u0026 pod.Spec.Subdomain == service.Name \u0026\u0026 service.Namespace == pod.Namespace { epa.Hostname = hostname } // Allow headless service not to have ports. if len(service.Spec.Ports) == 0 { if service.Spec.ClusterIP == api.ClusterIPNone { subsets, totalReadyEps, totalNotReadyEps = addEndpointSubset(subsets, pod, epa, nil, tolerateUnreadyEndpoints) // No need to repack subsets for headless service without ports. } } else { for i := range service.Spec.Ports { servicePort := \u0026service.Spec.Ports[i] portName := servicePort.Name portProto := servicePort.Protocol portNum, err := podutil.FindPort(pod, servicePort) if err != nil { glog.V(4).Infof(\"Failed to find port for service %s/%s: %v\", service.Namespace, service.Name, err) continue } var readyEps, notReadyEps int epp := \u0026v1.EndpointPort{Name: portName, Port: int32(portNum), Protocol: portProto} subsets, readyEps, notReadyEps = addEndpointSubset(subsets, pod, epa, epp, tolerateUnreadyEndpoints) totalReadyEps = totalReadyEps + readyEps totalNotReadyEps = totalNotReadyEps + notReadyEps } } } ... glog.V(4).Infof(\"Update endpoints for %v/%v, ready: %d not ready: %d\", service.Namespace, service.Name, totalReadyEps, totalNotReadyEps) ... } 主要的处理函数为syncService，去掉了一些逻辑，主要的处理逻辑在32行，遍历Pod，查看其PodReady Condition是否为true，true的会会把其IP放入subnet的Addresses结构中，否则放入NotReadyAddresses中。Condition主要是kubelet设置的，在generateAPIPodStatus的时候会进行设置，如下 // generateAPIPodStatus creates the final API pod status for a pod, given the // internal pod status. func (kl *Kubelet) generateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus { glog.V(3).Infof(\"Generating status for %q\", format.Pod(pod)) // check if an internal module has requested the pod is evicted. for _, podSyncHandler := range kl.PodSyncHandlers { if result := podSyncHandler.ShouldEvict(pod); result.Evict { return v1.PodStatus{ Phase: v1.PodFailed, Reason: result.Reason, Message: result.Message, } } } s := kl.convertStatusToAPIStatus(pod, podStatus) // Assume info is ready to process spec := \u0026pod.Spec allStatus := append(append([]v1.ContainerStatus{}, s.ContainerStatuses...), s.InitContainerStatuses...) s.Phase = getPhase(spec, allStatus) // Check for illegal phase transition if pod.Status.Phase == v1.PodFailed || pod.Status.Phase == v1.PodSucceeded { // API server shows terminal phase; transitions are not allowed if s.Phase != pod.Status.Phase { glog.Errorf(\"Pod attempted illegal phase transition from %s to %s: %v\", pod.Status.Phase, s.Phase, s) // Force back to phase from the API server s.Phase = pod.Status.Phase } } kl.probeManager.UpdatePodStatus(pod.UID, s) s.Conditions = append(s.Conditions, status.GeneratePodInitializedCondition(spec, s.InitContainerStatuses, s.Phase)) s.Conditions = append(s.Conditions, status.GeneratePodReadyCondition(spec, s.Conditions, s.ContainerStatuses, s.Phase)) s.Conditions = append(s.Conditions, status.GenerateContainersReadyCondition(spec, s.ContainerStatuses, s.Phase)) // Status manager will take care of the LastTransitionTimestamp, either preserve // the timestamp from apiserver, or set a new one. When kubelet sees the pod, // `PodScheduled` condition must be true. s.Conditions = append(s.Conditions, v1.PodCon","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/:0:2","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"修改方案 通过mutatingwebhook实现一个通用的能力，针对endpoint的create和update事件，从配置中心（内部组件）中获取对应的配置，并通过规则引擎（开源版本可参考 https://github.com/antonmedv/expr ），对subnet中的Addresses和NotReadyAddresses做一些修改，这样可以实现无侵入式的修改，也比较灵活，可以对配置进行实时修改等，后续像sidecar这种根据用户需求来设置pod ready condition的情况，也无需修改代码，只需要添加配置即可，而且也可以通过condition看到真实的Container、Pod状态 ","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/:0:3","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"优雅退出","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/","tags":["kubernetes"],"title":"Sidecar优雅退出","uri":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/"},{"categories":["问题排查"],"content":"背景 codis集群在接入弹性云测试时发现容器漂移失败，通过集群日志看，提示 调度超时，去界面查看，已经调度成功了（调度成功的标志就是已经有宿主机IP了），状态显示的pending并不一定就是调度失败。但这反应不出来问题出在哪里，接下来就需要到master机器上执行命名，查看日志来分析问题出在哪里 首先，我们要确定是不是调度超时了，可以直接通过 kubectl describe po kirovpre-krds-sf-f3dec-0 来看Pod的创建时间为 20:40:11，调度成功的时间为20:40:12，可以看到调度还是很快的。再看下相关日志，显示20:39:38删除成功，然后等待容器调度，等待30s后发现容器未调度成功，则判定为调度超时。简单梳理下时间线 20:39:38 删除成功 （kube-odin日志） 20:40:11 容器创建成功 （k8s） 20:40:12 容器调度成功 (k8s) 从时间线来看，确实从删除到调度成功耗时超过了30s，但是从容器创建出来到调度成功才1s，大部分耗时是在删除到新创建的阶段。熟悉k8s的同学应该知道，删除Pod的api支持设置删除方式：backgroud、foregroud，区别就是后台删除是异步的，调用api后立马返回，前台删除的话是同步的，会一直等容器真正删除后才返回，默认为backgroud。也就是说20:39:38提示的删除成功只是api调用成功，容器并未真正的删除，容器的删除操作一直在后台执行，直到20:40:11才删除成功，删除后立马创建新的容器。可以通过kubelet的日志证明这一点，下面的日志是经过筛选后的 I0603 20:39:37.908557 3033 kubelet.go:1913] SyncLoop (DELETE, \"api\"): \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" I0603 20:39:37.908655 3033 kubelet_pods.go:1433] Generating status for \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" I0603 20:39:37.908799 3033 kuberuntime_container.go:468] Running preStop hook for container \"docker://5fe57cf36af267adae571272f234762ad8741922e24074182ff25301e953ec72\" I0603 20:39:37.916683 3033 status_manager.go:499] Patch status for pod \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" with \"{}\" I0603 20:39:37.916699 3033 status_manager.go:506] Status for pod \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" updated successfully: (6, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:09 +0800 CST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:26 +0800 CST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:26 +0800 CST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:08 +0800 CST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.89.231.24 PodIP:10.169.92.60 StartTime:2020-05-29 15:07:09 +0800 CST InitContainerStatuses:[] ContainerStatuses:[{Name:agent-kirovpre-krds-ys02 State:{Waiting:nil Running:\u0026ContainerStateRunning{StartedAt:2020-05-29 15:07:18 +0800 CST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.kaku.com/kakuonline/kvstore-sidecar-python2-centos7-agent:ea6d410 ImageID:docker-pullable://registry.kaku.com/kakuonline/kvstore-sidecar-python2-centos7-agent@sha256:6d61df206fef0fbfd940e1139d7dff6b0dfaacd847d8d64b2b480cf5afd8a513 ContainerID:docker://5fe57cf36af267adae571272f234762ad8741922e24074182ff25301e953ec72} {Name:kirovpre-krds-ys02 State:{Waiting:nil Running:\u0026ContainerStateRunning{StartedAt:2020-05-29 15:07:22 +0800 CST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.kaku.com/kakuonline/kvstore-sidecar-krds:stable ImageID:docker-pullable://registry.kaku.com/kakuonline/kvstore-sidecar-krds@sha256:3945a5304ee92b89605831d991874d0b9fdffb89b19fc92d5bbc4969b8a3cf1f ContainerID:docker://2e2354889588dc7483d2bb9be27a5253f292374c8e179b12367e0deea8b2d825}] QOSClass:Burstable}) I0603 20:39:37.916795 3033 kubelet_pods.go:993] Pod \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" is terminated, but some containers are still running I0603 20:39:40.975805 3033 kuberuntime_container.go:485] preStop hook for container {\"docker\" \"5fe57cf36af267adae571272f234762ad8741922e24074182ff25301e953ec72\"} completed I0603 20:39:40.975845 3033 kuberuntime_container.go:563] Killing container \"docker://2e2354889588dc7483d2bb9be27a5253f292374c8e179b12367e0deea8b2d825\" with 5 second grace period I0603 20:39:40.975856 3033 kuberuntime_container.go:468] Running preStop hook for container \"docker://2e2354889588dc7483d2bb9be27a5253f292374c8e179b123","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/:0:1","tags":["kubernetes"],"title":"Sidecar优雅退出","uri":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/"},{"categories":["问题排查"],"content":"总结 明白了问题出在哪里，修改其实就很简单了，这里不再多说如何修改。 此次问题排查使我深刻的认识到了一点：一些看起来很容易理解的东西可能正是我们思维定式的误区，越简单的东西我们往往越容易先入为主，不再进行深入思考。而很多问题往往就是这些不起眼的细节日积月累导致的，我们要尽可能的对我们所用到的东西有深刻全面的理解，降低出问题的概率，毕竟线上无小事。有精力的话还是要看看源码，比如 max（graceperiod - cost, 2）,意思就是会为容器至少设置2s的时间，超过后才能会进行强删；再比如外面不传graceperiod时graceperiod用pod的terminationGracePeriod，传的话就是传进来的参数，这就可能导致出现负数的情况，导致不会强删，terminationGracePeriod失效等。 ","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/:0:2","tags":["kubernetes"],"title":"Sidecar优雅退出","uri":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/"},{"categories":["bug修复","问题排查","性能优化"],"content":"orphan controller revision cannot be adopt","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"背景 版本1.12.4 线上遇到kube-controller-manager重启慢的问题，具体表现为进程重启虽然速度快，但是重启完所有数据都同步完一遍耗时很长，集群中大约5000个statefulset，在还没同步完一遍数据之前如果有statefulset的创建、删除、修改等操作，可能（和具体statefulset的操作有关，新建的情况肯定是在最后，更新和删除的情况需要看同名的statefulset是否已经被处理过了，如果是的话也会在最后处理，如果没有的话，则不会排在最后）就需要等到所有数据都同步完之后才能继续处理。 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:1","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"问题原因 并发goroutine数 当前版本的statefulset controller只使用了一个goroutine来串行的处理所有的statefulset，在最新版的代码中已经支持了多个goroutine并行处理，且可配置 Informer List 经过添加trace信息打印每个阶段耗时，发现在根据statefulset获取其对应的controllerrevision时比较慢，如下代码段 // adoptOrphanRevisions adopts any orphaned ControllerRevisions matched by set's Selector. func (ssc *StatefulSetController) adoptOrphanRevisions(set *apps.StatefulSet) error { // 比较慢 revisions, err := ssc.control.ListRevisions(set) ... } 而且同样的逻辑会在这里在执行一次 // syncStatefulSet syncs a tuple of (statefulset, []*v1.Pod). func (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error { ... // TODO: investigate where we mutate the set during the update as it is not obvious. // UpdateStatefulSet里面会再次调用ListRevisions函数 if err := ssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil { return err } ... } 最终就导致同步一个statefulset就需要100+ms，总的5000+ statefulset同步完一遍就需要将近20m。 ControllerRevision 每个statefulset都对应一些controllerrevision资源，从字面意思就可以看出来其作用就是记录此statefulset的历史信息，这也是为什么我们可以直接对statefulset做回滚之类的操作的原因，默认情况下会为每个statefulset保留10条历史记录，在每个statefulset上有属性可配置。 其实上面说到的耗时的逻辑就是针对每个statefulset去获取孤儿controllerrevision，如果有则会领养孤儿。所以一个优化项就是直接从孤儿中找，而不是从全量中找，且把所有的controllerrevision缓存到本地，不再使用ListInformer提供的那些方法，因为这些方法始终会在全量中寻找满足条件的，而且还会用到反射，虽然数据也都在本地，但性能还是比较差的。大体思路就是在创建statesetfulset controller时同时注册controllerrevision相关的事件，把所有的revision和孤儿revision缓存到自定义的数据结构中，后续直接从里面获取即可。 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:2","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"最终效果 优化完之后最终重启一次controller-manager知道全量数据同步完一遍的耗时由20m左右缩减到1m左右，可以看到效果还是很明显的，而且还是有优化空间的，比如继续以空间换时间，在备controller-manager启动时就先把所有的数据同步完，所有更新缓存的逻辑照样执行，只是不触发其他操作，这样在主备切换时就能省掉网络传输数据的耗时，当然得衡量数据量大小，随着集群规模越来越大，master上各组件占用的内存势必越来越多，将来可能就又会面临内存不够用的情况。 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:3","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"意外收获 测试的时候发现了一个bug，孤儿controllerrevision无法被领养，且会导致statefulset同步失败，这是statefulset controller的代码bug，pr已合入master，随着v1.18版本发布。具体可参考这里 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:4","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["问题排查"],"content":"flannel启动提示key not found","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"问题描述 etcd 3.3.1 flannel 0.11.0 flannel启动时报错，启动参数如下 ./flannel -etcd-keyfile=/etc/kubernetes/ssl/etcd-client-key.pem -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-endpoints=https://ip:port -etcd-certfile=/etc/kubernetes/ssl/etcd-client.pem -etcd-prefix=/coreos.com/network 错误信息如下： E0908 20:27:17.671602 2331 main.go:382] Couldn't fetch network config: 100: Key not found (/coreos.com) [22] timed out E0908 20:27:18.680096 2331 main.go:382] Couldn't fetch network config: 100: Key not found (/coreos.com) [22] timed out E0908 20:27:19.688339 2331 main.go:382] Couldn't fetch network config: 100: Key not found (/coreos.com) [22] 其中coreos.com是启动flannel时-etcd-prefix参数的默认值（/coreos.com/network） ","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/:0:1","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"解决办法 报错提示很明显，没有对应的key，于是执行etcdctl的命令插入对应的key并设置其值 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd-client.pem --key=/etc/kubernetes/ssl/etcd-client-key.pem --endpoints=https://ip:port put /coreos.com/network/config '{\"Network\":\"192.168.0.0/16\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"vxlan\"}}' OK 重新启动flannel，依旧报错，执行etcdctl get获取key的信息也可以正常拿到之前的设置，一脸懵逼。网上搜了下说是etcd api版本的问题，不是很明白，然后去看代码，发现flannel在使用etcd时只支持etcd v2版本的api，因为上线添加key-value时使用的是v3版本的api，所以导致虽然添加成功了，但是用v2获取的时候还是失败，解决办法就是用v2版本的api添加一遍即可 etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd-client.pem --key=/etc/kubernetes/ssl/etcd-client-key.pem --endpoints=https://ip:port set /coreos.com/network/config '{\"Network\":\"192.168.0.0/16\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"vxlan\"}}' 区别就是去掉设置v3的环境变量，put改为set，需要注意一下，在master最新代码中，不设置ETCDCTL_API就默认用v3版本的api，后续使用时还需要具体版本具体对待。 ","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/:0:2","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"收获 etcd不同版本的api对应的url path的prefix不同，v2前缀为/v2/keys，v3前缀为/v3[alpha|beta]/kv，用法也不同，具体可以参考官网API说明。平时直接使用client包时这些信息都会忽略掉，封装的太好了会使使用者变傻，还是有必要看看源码是怎么实现的。 ","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/:0:3","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"背景 线上master的apiserver组件内存报警，内存使用量持续增长，监控如下 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:1","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["问题排查"],"content":"排查过程 从监控上看和另外一个程序（管理员平台）的内存使用情况吻合，使用率降下来是因为重启了apiserver和管理员平台，且问题只出现在最近两天的晚上，管理员平台中有一段逻辑是定时全量拉取集群数据（设计不合理，后续需要改），管理员平台的日志里显示拉取数据超时，基本猜测和管理员平台调用k8s api不合理有关，且k8s apiserver应该也有bug，导致内存泄露或者goroutine泄露。但是最近代码都没动过，为啥之前没事呢，后负责管理员平台的同事说近两天美东专线有问题，延迟是之前的3倍，而且出现问题的时间正好匹配，那接下来就查一下具体原因。 查看apiserver日志 apiserver错误日志里有大量的上述日志，可以看到是apiserver因为响应超时触发的，里面也有详细的函数调用堆栈信息，也有ip的信息，正好对应了master和管理员平台的地址，通过pprof也可以看到此时的goroutine使用量一直在增加，已45000+，确认是产生了goroutine泄露。下图为pprof tree看到的部分内容，里面显示了占用量最多的地方 同时在浏览器中访问http://ip:port/debug/pprof/goroutine 可以看到具体goroutine数量和执行函数的行号，此处忘记截图了，不过和上面的信息吻合，且更信息因为携带了行号的信息，可以看到是如下代码出的问题（代码版本1.12.4） // k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/filters/timeout.go func (t *timeoutHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { r, after, postTimeoutFn, err := t.timeout(r) if after == nil { t.handler.ServeHTTP(w, r) return } errCh := make(chan interface{}) tw := newTimeoutWriter(w) go func() { defer func() { err := recover() if err != nil { const size = 64 \u003c\u003c 10 buf := make([]byte, size) buf = buf[:runtime.Stack(buf, false)] err = fmt.Sprintf(\"%v\\n%s\", err, buf) } errCh \u003c- err }() t.handler.ServeHTTP(tw, r) }() select { case err := \u003c-errCh: if err != nil { panic(err) } return case \u003c-after: postTimeoutFn() tw.timeout(err) } } 泄露的goroutine就是第11行处的，简单解释一下上面的逻辑：生成一个timeout的handler，起一个新的goroutine进行后续handler的处理，当前goroutine中使用select进行等待，分为两种case，分别对应新goroutine中panic的情况和整个函数超时的情况，分别看两个case的内容 第25行：从errCh读取数据，其中errCh中的数据是在新的goroutine中产生的，对应到实际情况就是22行的代码出发生了panic，在13行捕获到了，最后在20行把err写入到errCh中，但是这里需要注意一下这个errCh是个无缓存的； 第30行：after是调用time.After后产生的一个chan，在超时后可以从这个chan中获取到数据，然后在32行处会调用tw.timeout函数，里面会触发panic； 那为什么11行处的goroutine泄露了呢？ 问题就出现在了刚才提到的无缓冲的errCh上，因为触发了timeout，代码逻辑没有执行到25行，直接去了30行，然后整个函数panic，导致20行执行的时候卡住了，从而阻止了11行出的新的goroutine的退出，每有一个timeout的请求，这里就会泄露一个goroutine，从而导致内存随之泄露，cpu的话其实不受什么影响，因为泄露的goroutine已经执行过gopark，不是runnable状态的。 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:2","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["问题排查"],"content":"解决方案 印象中记得之前看k8s版本升级的release-note时有提到过修复apiserver leak字样的信息，然后就去官方项目中查，结果没找到，然后直接去看了对应文件的最新版本代码，看history，终于找到了相关的修复的commit，合入1.17。 case \u003c-after: defer func() { // resultCh needs to have a reader, since the function doing // the work needs to send to it. This is defer'd to ensure it runs // ever if the post timeout work itself panics. go func() { res := \u003c-resultCh if res != nil { switch t := res.(type) { case error: utilruntime.HandleError(t) default: utilruntime.HandleError(fmt.Errorf(\"%v\", res)) } } }() }() postTimeoutFn() tw.timeout(err) } 可以看到其思想就是在外层panic后，新加一个defer func用来从之前的errCh（新版改名为resultCh）接收数据，从而避免之前的问题。 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:3","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["问题排查"],"content":"总结 通过上面的修改，确实可以解决goroutine泄露的问题，但是也存在一个隐患：第6行新加的goroutine会从resultCh读数据，因为在上一段代码处有个处理，无论是否panic，都会往errCh（resultCh）写入err，从而可以避免同时泄露两个goroutine的情况，但是如果短时间内大量请求到来且处理时间都比较慢直至超时，虽然goroutine不会泄露，但是会产生两倍于之前的goroutine，可能会在短时间内造成内存暴涨，也算是一个稳定性风险，需要合理设置限流来降低风险。 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:4","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["算法"],"content":"背包问题","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"最近的工作都跟集群调度有关，一直在为了满足用户需求添加各种调度策略，现在也暂时告一段落了，抽时间总结思考了之前的工作，调度本质上就是背包问题，但是相当复杂，涉及到多维多重背包、组合背包、依赖背包等。又重新开始学习背包问题，这里先从简单的01背包开始讲，在网上也找到了很多相关的文章，但是很遗憾，我找到的很多关于用golang实现背包的文章中给出的代码都是有问题的，后决定自己写出来，也希望大家一起思考，不要被误导了。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:0","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"题目 有有N件物品和一个容量为V的背包。第i件物品的费用是c[i]，价值是w[i]。求解将哪些物品装入背包可使价值总和最大。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:1","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"基本思想 这是最基础的背包问题，特点是：每种物品仅有一件，可以选择放或不放。 用子问题定义状态：即f[i][v]表示前i件物品恰放入一个容量为v的背包可以获得的最大价值。则其状态转移方程便是： f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]+w[i]} 这个方程非常重要，基本上所有跟背包相关的问题的方程都是由它衍生出来的。所以有必要将它详细解释一下：“将前i件物品放入容量为v的背包中”这个子问题，若只考虑第i件物品的策略（放或不放），那么就可以转化为一个只牵扯前i-1件物品的问题。如果不放第i件物品，那么问题就转化为“前i-1件物品放入容量为v的背包中”，价值为f[i-1][v]；如果放第i件物品，那么问题就转化为“前i-1件物品放入剩下的容量为v-c[i]的背包中”，此时能获得的最大价值就是f[i-1][v-c[i]]再加上通过放入第i件物品获得的价值w[i]。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:2","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"代码实现 测试用例 func main() { v := []int{7, 5, 8} //物品大小 w := []int{2, 3, 4} //物品价值 goods := [][]int{ []int{7, 2}, []int{5, 3}, []int{8, 4}, } fmt.Println(zeroonepack1(v, w, 5)) fmt.Println(zeroonepack(goods, 5)) fmt.Println(zeroonepack2(v, w, 5)) } // 结果应该都是3才对 二维数组实现 // 0 消耗 1 价值 // i 物品数 j 容量 // dp[i][j] = max(dp[i-1][j], goods[i][1] + dp[i-1][j-goods[i][0]]) func zeroonepack(goods [][]int, capacity int) int { num := len(goods) dp := make([][]int, num) for i := 0; i \u003c num; i++ { dp[i] = make([]int, capacity+1) } for i := goods[0][0]; i \u003c capacity+1; i++ { dp[0][i] = goods[0][1] } for i := 1; i \u003c num; i++ { // j不能直接从good[i][0]开始 for j := 0; j \u003c= capacity; j++ { // 此if else判断必须要有 if j \u003e= goods[i][0] { dp[i][j] = max(dp[i-1][j], dp[i-1][j-goods[i][0]]+goods[i][1]) } else { dp[i][j] = dp[i-1][j] } } } return dp[num-1][capacity] } func zeroonepack1(weight []int, value []int, capacity int) int { num := len(weight) dp := make([][]int, num) for i := 0; i \u003c num; i++ { dp[i] = make([]int, capacity+1) } for i := weight[0]; i \u003c= capacity; i++ { dp[0][i] = value[0] } for i := 1; i \u003c num; i++ { for j := 0; j \u003c= capacity; j++ { if j \u003e= weight[i] { dp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i]]+value[i]) } else { dp[i][j] = dp[i-1][j] } } } return dp[num-1][capacity] } 上面都是二维数组的实现，空间复杂度是O(VN)，区别就是传入的参数不同，用二维数组表示物品，或者用两个一维数组分别表示物品的消耗（重量或体积等）和价值，特别注意上面的注释，很多golang版本的代码就是那里出的问题，错误版本的代码参考这里 一维数组实现 以上方法的时间和空间复杂度均为O(VN)，其中时间复杂度应该已经不能再优化了，但空间复杂度却可以优化到O。 先考虑上面讲的基本思路如何实现，肯定是有一个主循环i=1..N，每次算出来二维数组f[i][0..V]的所有值。那么，如果只用一个数组f[0..V]，能不能保证第i次循环结束后f[v]中表示的就是我们定义的状态f[i][v]呢？f[i][v]是由f[i-1][v]和f[i-1][v-c[i]]两个子问题递推而来，能否保证在推f[i][v]时（也即在第i次主循环中推f[v]时）能够得到f[i-1][v]和f[i-1][v-c[i]]的值呢？事实上，这要求在每次主循环中我们以v=V..0的顺序推f[v]，这样才能保证推f[v]时f[v-c[i]]保存的是状态f[i-1][v-c[i]]的值。代码如下： func zeroonepack2(weight []int, value []int, capacity int) int { num := len(weight) dp := make([]int, capacity+1) for i := weight[0]; i \u003c capacity; i++ { dp[i] = value[0] } for i := 1; i \u003c num; i++ { for j := capacity; j \u003e= 0; j-- { if j \u003e= weight[i] { dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } } } return dp[capacity] } 其中的f[v]=max{f[v],f[v-c[i]]}一句恰就相当于我们的转移方程f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]}，因为现在的f[v-c[i]]就相当于原来的f[i-1][v-c[i]]。如果将v的循环顺序从上面的逆序改成顺序的话，那么则成了f[i][v]由f[i][v-c[i]]推知，与本题意不符，但它却是另一个重要的背包问题最简捷的解决方案，故学习只用一维数组解01背包问题是十分必要的。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:3","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["源码分析"],"content":"kube-apiserver watch实现","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["源码分析"],"content":"List-Watch是kubernetes的核心机制。组件kubelet、kube-controller-manager、kube-scheduler需要监控各种资源(pod、service等)的变化，当这些对象发生变化时(add、delete、update)，kube-apiserver会主动通知这些组件。这个过程类似一个发布-订阅系统。本文章将从代码角度探究一下list-watch的实现方式。 转载自https://zhuanlan.zhihu.com/p/33335726，有修改 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/:0:0","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["源码分析"],"content":"**第一部分：**kube-apiserver对etcd的List-watch机制 流程示意图 构建PodStorage kube-apiserver针对每一类资源(pod、service、endpoint、replication controller、depolyments)都会构建Storage对象，如:PodStorage； PodStorage.Pod.Store封装了对etcd的操作； store.CompleteWithOptions会调用etcdOptions.GetRESTOptions，此方法将 调用generic.UndecoratedStorage创建无cache的Store； 或者调用genericregistry.StorageWithCacher创建带Cache的Store； StorageWithCacher 调用NewCacherFromConfig，将创建Cacher对象； 创建Cacher 首先，创建watchCache对象和cacheListerWatcher对象，cacheListWatcher对象是ListerWatcher接口实现，实现了List()和Watch()方法； 构建Cacher对象，主要的数据成员：watchCache、reflector、watchers及incoming channel； watchCache是一个cache，用来存储apiserver从etcd那里watch到的对象； watchers是一个map，map的值类型为cacheWatcher，当kubelet、kube-scheduler需要watch某类资源时，他们会向kube-apiserver发起watch请求，kube-apiserver就会生成一个cacheWatcher，cacheWatcher负责将watch的资源通过http从apiserver传递到kubelet、kube-scheduler； Reflector对象，主要数据成员：ListerWatcher，ListerWatcher是接口对象，包括方法List()和Watch()；listerWatcher包装了Storage，主要是将watch到的对象存到watchCache中； incoming channel接收watchCacheEvent； 协程调用cacher.dispatchEvents，watchCache将incoming channel接收watchCacheEvent添加到watchers的inputChan中； 协程调用cacher.startCaching; StartCaching 执行cacheListerWatcher的List方法和Watch方法； 调用reflector的watchHandler方法； cacheListWatcher.List／cacheListWatcher.Watch List方法将调用storage.List方法，这里是etcdHelper.List方法； Watch方法将调用storage.watch方法，这里是etcdHelper.WatchList方法； etcdHelper.List／etcdHelper.Watch etcdHelper对象是Storage接口对象的实现; etcdHelper的List方法： 获取etcd的对象（包括resourceVersion信息）； etcdHelper的WatchList方法： 创建etcdWatcher； etcdWatcher对象，实现了Watch接口； etcdWatcher对象，主要的数据成员是etcdIncoming channel和outgoing channel； 协程执行etcdWatcher.translate； 最后，协程运行etcdWatcher.etcdWatch； etcdWatcher.etcdWatch 如果resourceVersion==0, 运行etcdGetInitialWatchState(),获取所有的pods，并将结果输入到etcdIncoming channel; 之后，不停的调用watcher.Next()，并将结果输入到etcdIncoming channel; etcdWatcher.translate 读取etcdIncoming channel信息； 调用etcdWatcher.sendResult进行转化; 发送到outgoing channel； reflector.watchHandler 读取outgoing channel信息，操作watchCache； 操作watchCache 处理事件watchCache.processEvent 创建watchCacheEvent 调用watchCache.updateCache，更新watchCache; 到此分析完kube-apiserver对etcd的watch机制，除此之外，kube-apiserver会向其他组件提供watch接口，下面将分析kube-apiserver的watch API。 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/:1:0","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["源码分析"],"content":"第二部分：kube-apiserver的watch restful API kube-apiserver提供watch restful API给其他组件(kubelet、kube-controller-manager、kube-scheduler、kube-proxy)。watch restful API的处理流程和PUT、DELETE、GET等REST API处理流程类似。 流程示意图 registerResourceHandlers ListResource 1.调用rw.watch方法，这里将会调用Store.watch； 2.调用serveWatch方法； Store.watch 调用Storage.Watch方法和Storage.WatchList方法，这里将调用Cacher.watch方法和Cacher.WatchList方法 Cacher.watch watch方法中将调用newCacheWatcher； newCacheWatcher方法： 生成一个watcher，并将watcher插入到cacher.watchers中； 协程调用cacheWatcher.process方法，此方法将会操作input channel的消息； 操作input channel 读取input channel的信息，并调用sendWatchCacheEvent方法； sendWatchCacheEvent kube-apiserver的watch会带过滤功能； 对watchCacheEvent进行Filter，发送到cacher.result channel中； ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/:2:0","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["问题排查","bug修复"],"content":"现象 k8s master进行线上升级，notifier利用client-go提供的informer机制注册了EndPoint的Update Handler，当kube-apiserver重启时触发了大量的update事件，触发依赖的第三方服务限流。 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/:0:1","tags":["kubernetes"],"title":"Kube-apiserver重启导致产生全量的update event","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/"},{"categories":["问题排查","bug修复"],"content":"原因排查 在测试环境进行了测试，并且在注册update事件处理函数中调用 reflect.DeepEqual(old, new) 进行了比较，发现返回true，即old与new完全相同却产生了update事件。 接下来就是到事件产生的地方去寻找原因，主要有两个地方，一个是reflect的ListAndWatch，相当于元数据的生产者，另一个是sharedIndexedInformer的HandleDeltas，消费元数据并生成对应类型的事件分发下去，接下来分别看 HandleDeltas （事件来源） func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) // 重点关注 if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026\u0026 exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 很容易看出来，当delta类型为非Delete时，informer会从自己的indexer（带索引的缓存）中获取指定的object是否存在（注意这里其实是从object计算出key，然后用key寻找到的），如果存在则更新缓存且分发一个update事件。可以继续看后续对分发的这个notification的处理，都是直接处理，没有任何去重逻辑。到这里就可以理解为啥会收到全量的update事件了，正式因为此时缓存里已经有了对应数据，而在分发事件时并没有比较缓存中的object是否和新来的object一致就直接当成update处理了，导致客户端收到全量的更新事件。那问题又来了，为什么重启apiserver时会往deltafifo里全量扔一遍数据，正常不应该是从最后的resourceVersion开始重新watch吗？继续看下面的处理 ListAndWatch （全量数据的来源） // 代码位置 k8s.io/client-go/tools/cache/reflector.go // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { klog.V(3).Infof(\"Listing and watching %v from %s\", r.expectedType, r.name) var resourceVersion string // Explicitly set \"0\" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1.ListOptions{ResourceVersion: \"0\"} if err := func() error { initTrace := trace.New(\"Reflector ListAndWatch\", trace.Field{\"name\", r.name}) defer initTrace.LogIfLong(10 * time.Second) var list runtime.Object var err error listCh := make(chan struct{}, 1) panicCh := make(chan interface{}, 1) go func() { defer func() { if r := recover(); r != nil { panicCh \u003c- r } }() // Attempt to gather list in chunks, if supported by listerWatcher, if not, the first // list request will return the full response. pager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) { return r.listerWatcher.List(opts) })) if r.WatchListPageSize != 0 { pager.PageSize = r.WatchListPageSize } // Pager falls back to full list if paginated list calls fail due to an \"Expired\" error. list, err = pager.List(context.Background(), options) close(listCh) }() select { case \u003c-stopCh: return nil case r := \u003c-panicCh: panic(r) case \u003c-listCh: } if err != nil { return fmt.Errorf(\"%s: Failed to list %v: %v\", r.name, r.expectedType, err) } initTrace.Step(\"Objects listed\") listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v: %v\", r.name, list, err) } resourceVersion = listMetaInterface.GetResourceVersion() initTrace.Step(\"Resource version extracted\") items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v (%v)\", r.name, list, err) } initTrace.Step(\"Objects extracted\") if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err) } initTrace.Step(\"SyncWith done\") r.setLastSyncResourceVersion(resourceVersion) initTrace.Step(\"Resource version updated\") return nil }(); err != nil { return err } resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) g","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/:0:2","tags":["kubernetes"],"title":"Kube-apiserver重启导致产生全量的update event","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/"},{"categories":["问题排查","bug修复"],"content":"总结 至此，已经清楚了具体的原因，ListAndWatch的修改很简单，已经给官方提了pull request 修复这个问题。 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/:0:3","tags":["kubernetes"],"title":"Kube-apiserver重启导致产生全量的update event","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/"},{"categories":["源码分析"],"content":"kubelet pod创建主流程","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":" kubernetes 版本： v1.12 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:0:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"kubelet 工作原理 kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。 本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:1:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"kubelet 创建 pod 的流程 kubelet 创建 pod 的流程 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"1、kubelet 的控制循环（syncLoop） syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。 func (kl *Kubelet) syncLoop(updates \u003c-chan kubetypes.PodUpdate, handler SyncHandler) { glog.Info(\"Starting kubelet main sync loop.\") // syncTicker 每秒检测一次是否有需要同步的 pod workers syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() // 每两秒检测一次是否有需要清理的 pod housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() // pod 的生命周期变化 plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 { time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } ... kl.syncLoopMonitor.Store(kl.clock.Now()) // 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface， // 在该文件开头处定义 if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:1","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"2、监听 pod 变化（syncLoopIteration） syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息： configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。 syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态 houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作 plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。 livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作 func (kl *Kubelet) syncLoopIteration(configCh \u003c-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u003c-chan time.Time, housekeepingCh \u003c-chan time.Time, plegCh \u003c-chan *pleg.PodLifecycleEvent) bool { select { case u, open := \u003c-configCh: if !open { glog.Errorf(\"Update channel is closed. Exiting the sync loop.\") return false } switch u.Op { case kubetypes.ADD: ... case kubetypes.UPDATE: ... case kubetypes.REMOVE: ... case kubetypes.RECONCILE: ... case kubetypes.DELETE: ... case kubetypes.RESTORE: ... case kubetypes.SET: ... } ... case e := \u003c-plegCh: ... case \u003c-syncCh: ... case update := \u003c-kl.livenessManager.Updates(): ... case \u003c-housekeepingCh: ... } return true } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:2","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"3、处理新增 pod（HandlePodAddtions） 对于事件中的每个 pod，执行以下操作： 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了 3、如果是 mirror pod 调用其单独的方法 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测 func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() // 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理 sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods { if kl.dnsConfigurer != nil \u0026\u0026 kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } existingPods := kl.podManager.GetPods() // 把 pod 加入到 podManager 中 kl.podManager.AddPod(pod) // 判断是否是 mirror pod（即 static pod） if kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } if !kl.podIsTerminated(pod) { activePods := kl.filterOutTerminatedPods(existingPods) // 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue } } mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) // 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法. kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) // 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测 kl.probeManager.AddPod(pod) } } static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:3","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"4、下发任务（dispatchWork） dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。 func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { kl.statusManager.TerminatePod(pod) } return } // 落实在 podWorkers 中 kl.podWorkers.UpdatePod(\u0026UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:4","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"5、更新事件的 channel（UpdatePod） podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。 func (p *podWorkers) UpdatePod(options *UpdatePodOptions) { pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() // 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel if podUpdates, exists = p.podUpdates[uid]; !exists { // 创建 channel podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // 启动 goroutine go func() { defer runtime.HandleCrash() p.managePodLoop(podUpdates) }() } // 下发更新事件 if !p.isWorking[pod.UID] { p.isWorking[pod.UID] = true podUpdates \u003c- *options } else { update, found := p.lastUndeliveredWorkUpdate[pod.UID] if !found || update.UpdateType != kubetypes.SyncPodKill { p.lastUndeliveredWorkUpdate[pod.UID] = *options } } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:5","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"6、调用 syncPodFn 方法同步 pod（managePodLoop） managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情: 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。 func (p *podWorkers) managePodLoop(podUpdates \u003c-chan UpdatePodOptions) { var lastSyncTime time.Time for update := range podUpdates { err := func() error { podUID := update.Pod.UID status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil { ... } err = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, }) lastSyncTime = time.Now() return err }() if update.OnCompleteFunc != nil { update.OnCompleteFunc(err) } if err != nil { ... } p.wrapUp(update.Pod.UID, err) } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:6","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"7、完成创建容器前的准备工作（SyncPod） 在这个方法中，主要完成以下几件事情： 如果是删除 pod，立即执行并返回 同步 podStatus 到 kubelet.statusManager 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup 如果是 static Pod，就创建或者更新对应的 mirrorPod 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑 这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。 func (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // 是否为 删除 pod if updateType == kubetypes.SyncPodKill { ... } ... // 检查 pod 是否能运行在本节点 runnable := kl.canRunPod(pod) if !runnable.Admit { ... } // 更新 pod 状态 kl.statusManager.SetPodStatus(pod, apiPodStatus) // 如果 pod 非 running 状态则直接 kill 掉 if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { ... } // 加载网络插件 if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026\u0026 !kubecontainer.IsHostNetworkPod(pod) { ... } pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) { ... // 创建并更新 pod 的 cgroups if !(podKilled \u0026\u0026 pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { ... } } } // 为 static pod 创建对应的 mirror pod if kubepod.IsStaticPod(pod) { ... } // 创建数据目录 if err := kl.makePodDataDirs(pod); err != nil { ... } // 挂载 volume if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } // 获取 secret 信息 pullSecrets := kl.getPullSecretsForPod(pod) // 调用 containerRuntime 的 SyncPod 方法开始创建容器 result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { ... } return nil } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:7","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"8、创建容器 containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。 syncPod 主要执行以下几个操作： 1、计算 sandbox 和 container 是否发生变化 2、创建 sandbox 容器 3、启动 init 容器 4、启动业务容器 initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。 func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { // 1、计算 sandbox 和 container 是否发生变化 podContainerChanges := m.computePodActions(pod, podStatus) if podContainerChanges.CreateSandbox { ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err) } ... } // 2、kill 掉 sandbox 已经改变的 pod if podContainerChanges.KillPod { ... } else { // 3、kill 掉非 running 状态的 containers ... for containerID, containerInfo := range podContainerChanges.ContainersToKill { ... if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { ... } } } m.pruneInitContainersBeforeStart(pod, podStatus) podIP := \"\" if podStatus != nil { podIP = podStatus.IP } // 4、创建 sandbox podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox { podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil { ... } ... podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID) if err != nil { ... } // 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置 if !kubecontainer.IsHostNetworkPod(pod) { podIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus) glog.V(4).Infof(\"Determined the ip %q for pod %q after sandbox changed\", podIP, format.Pod(pod)) } } configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID) result.AddSyncResult(configPodSandboxResult) // 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等) podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt) ... // 5、启动 init container if container := podContainerChanges.NextInitContainerToStart; container != nil { ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil { ... } } // 6、启动业务容器 for _, idx := range podContainerChanges.ContainersToStart { ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { ... } } return } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:8","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"9、启动容器 最终由 startContainer 完成容器的启动，其主要有以下几个步骤： 1、拉取镜像 2、生成业务容器的配置信息 3、调用 docker api 创建容器 4、启动容器 5、执行 post start hook func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) { // 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。 imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { ... } ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { ... } // 设置 RestartCount restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } // 2、生成业务容器的配置信息 containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } ... // 3、通过 client.CreateContainer 调用 docker api 创建业务容器 containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { ... } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { ... } ... // 3、启动业务容器 err = m.runtimeService.StartContainer(containerID) if err != nil { ... } containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } // 4、执行 post start hook if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } // runner.Run 这个方法的主要作用就是在业务容器起来的时候， // 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。 // 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。 msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { ... } } return \"\", nil } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:9","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"总结 本文主要讲述了 kubelet 从监听到有容器调度至本节点再到容器创建的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。 参考： k8s源码分析-kubelet Kubelet源码分析(一):启动流程分析 kubelet 源码分析：pod 新建流程 kubelet创建Pod流程解析 Kubelet: Pod Lifecycle Event Generator (PLEG) Design- proposals ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:3:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"kubelet与cni交互流程","date":"2019-07-16","objectID":"https://www.likakuli.com/posts/kubernetes-cni/","tags":["kubernetes"],"title":"Kubelet与CNI交互源码","uri":"https://www.likakuli.com/posts/kubernetes-cni/"},{"categories":["源码分析"],"content":" 转载自：https://www.cnblogs.com/haoqingchuan/p/8668746.html，有修改 代码版本1.12.4 整体介绍 kubelet通过调用 grpc 接口调用实现了 CRI 的 dockershim 完成 rpc 通信，CNI 是由 dockershim grpc server 中调用的 kubelet -\u003e CRI shim -\u003e container runtime -\u003e container POD 创建过程中从 kubelet 到 docker server 到 cni 的 UML 结构如下 CNI 插件初始化 kubelet 在初始化的时候如果使用containerRuntime为Docker，则会起动dockershim rpc server case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) // 主要函数 ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil { return nil, err } if crOptions.RedirectContainerStreaming { klet.criHandler = ds } // The unix socket for kubelet \u003c-\u003e dockershim communication. glog.V(5).Infof(\"RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q\", remoteRuntimeEndpoint, remoteImageEndpoint) glog.V(2).Infof(\"Starting the GRPC server for the docker CRI shim.\") server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } // Create dockerLegacyService when the logging driver is not supported. supported, err := ds.IsCRISupportedLogDriver() if err != nil { return nil, err } if !supported { klet.dockerLegacyService = ds legacyLogProvider = ds } 创建 dockerservice 对象时初始化cniplugin // rpc server端 func NewDockerService(config *ClientConfig, podSandboxImage string, streamingConfig *streaming.Config, pluginSettings *NetworkPluginSettings, cgroupsName string, kubeCgroupDriver string, dockershimRootDir string, startLocalStreamingServer bool) (DockerService, error) { ... // 解析kubelet配置的pluginbindir pluginSettings.PluginBinDirs = cni.SplitDirs(pluginSettings.PluginBinDirString) cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs) cniPlugins = append(cniPlugins, kubenet.NewPlugin(pluginSettings.PluginBinDirs)) netHost := \u0026dockerNetworkHost{ \u0026namespaceGetter{ds}, \u0026portMappingGetter{ds}, } plug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) if err != nil { return nil, fmt.Errorf(\"didn't find compatible CNI plugin with given settings %+v: %v\", pluginSettings, err) } ds.network = network.NewPluginManager(plug) glog.Infof(\"Docker cri networking managed by %v\", plug.Name()) ... } // 根据指定confdir 和 bindir获取networkplugin func ProbeNetworkPlugins(confDir string, binDirs []string) []network.NetworkPlugin { old := binDirs binDirs = make([]string, 0, len(binDirs)) for _, dir := range old { if dir != \"\" { binDirs = append(binDirs, dir) } } plugin := \u0026cniNetworkPlugin{ defaultNetwork: nil, loNetwork: getLoNetwork(binDirs), execer: utilexec.New(), confDir: confDir, binDirs: binDirs, } // sync NetworkConfig in best effort during probing. plugin.syncNetworkConfig() return []network.NetworkPlugin{plugin} } 初始化cniplugin，会根据pluginDir查找符合条件的第一个 CNI config 文件，并以此 config 文件查找到对应的 CNI bin // InitNetworkPlugin inits the plugin that matches networkPluginName. Plugins must have unique names. func InitNetworkPlugin(plugins []NetworkPlugin, networkPluginName string, host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) (NetworkPlugin, error) { if networkPluginName == \"\" { // default to the no_op plugin plug := \u0026NoopNetworkPlugin{} plug.Sysctl = utilsysctl.New() if err := plug.Init(host, hairpinMode, nonMasqueradeCIDR, mtu); err != nil { return nil, err } return plug, nil } pluginMap := map[string]NetworkPlugin{} allErrs := []error{} for _, plugin := range plugins { name := plugin.Name() if errs := validation.IsQualifiedName(name); len(errs) != 0 { allErrs = append(allErrs, fmt.Errorf(\"network plugin has invalid name: %q: %s\", name, strings.Join(errs, \";\"))) continue } if _, found := pluginMap[n","date":"2019-07-16","objectID":"https://www.likakuli.com/posts/kubernetes-cni/:0:0","tags":["kubernetes"],"title":"Kubelet与CNI交互源码","uri":"https://www.likakuli.com/posts/kubernetes-cni/"},{"categories":["问题排查"],"content":"cgroup泄露","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"背景 线上k8s节点创建容器时提示\"no space left on device\"，为已知问题，参考 https://tencentcloudcontainerteam.github.io/2018/12/29/cgroup-leaking/ http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:1","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"解决方案 按照上述链接中的提示，首先看runc部分，docker用的从opencontainers/runc项目fork出来的docker/runc项目，目前线上用的docker版本为1.13.1，对应的docker-runc的commit为 9df8b306d01f59d3a8029be411de015b7304dd8f，查看其相关代码 func (s *MemoryGroup) Apply(d *cgroupData) (err error) { path, err := d.path(\"memory\") if err != nil \u0026\u0026 !cgroups.IsNotFound(err) { return err } if memoryAssigned(d.config) { if path != \"\" { if err := os.MkdirAll(path, 0755); err != nil { return err } } // 默认关闭 if d.config.KernelMemory != 0 { if err := EnableKernelMemoryAccounting(path); err != nil { return err } } } defer func() { if err != nil { os.RemoveAll(path) } }() // We need to join memory cgroup after set memory limits, because // kmem.limit_in_bytes can only be set when the cgroup is empty. _, err = d.join(\"memory\") if err != nil \u0026\u0026 !cgroups.IsNotFound(err) { return err } return nil } 此版本默认关闭KernelMemory功能，所以docker-runc暂时不需要改，接下来看kubelet相关代码，kubelet为1.12.4版本，pkg/kubelet/cm/cgroup_manager_linux.go下 func(s*MemoryGroup)Apply(d*cgroupData)(errerror){path,err:=d.path(\"memory\")iferr!=nil\u0026\u0026!cgroups.IsNotFound(err){returnerr}elseifpath==\"\"{returnnil}ifmemoryAssigned(d.config){if_,err:=os.Stat(path);os.IsNotExist(err){iferr:=os.MkdirAll(path,0755);err!=nil{returnerr}//Onlyenablekernelmemoryaccoutingwhenthiscgroup//iscreatedbylibcontainer,otherwisewemightget//errorwhenpeopleuse`cgroupsPath`tojoinanexisted//cgroupwhosekernelmemoryisnotinitialized.//强制开启iferr:=EnableKernelMemoryAccounting(path);err!=nil{returnerr}}}deferfunc(){iferr!=nil{os.RemoveAll(path)}}()//Weneedtojoinmemorycgroupaftersetmemorylimits,because//kmem.limit_in_bytescanonlybesetwhenthecgroupisempty._,err=d.join(\"memory\")iferr!=nil\u0026\u0026!cgroups.IsNotFound(err){returnerr}returnnil} 上面的代码为默认开启KernelMemory，且无法关闭，解决方案是注释掉EnableKernelMemoryAccounting调用，然后重新编译kubelet即可。由于线上docker和cgroup使用的cgroup-driver为cgroupfs而不是systemd**，所以这里并没有修改systemd****对应文件里有关KernelMemory****的代码。** ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:2","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"验证 找了一台新机器，上面没有任何容器，先看下改之前的kubelet所创建的/sys/fs/cgroup/memory/kubepods/memory.kmem.slabinfo文件，如下 说明已经开启了kmem，然后替换kubelet并重启宿主，观察上面文件，如下 说明kmem已经关闭了。这里重点强调一下，必须重启宿主才能生效，只重启kubelet无法生效，因为需要修改/sys/fs/cgroup/memory/kubepods，kubelet启动时会检测此目录是否存在，不存在则创建，存在则直接使用，只重启kubelet时此目录依然存在，因为容器业务进程还在使用着相关的cgroup。新创建的Pod会以继承此目录下的cgroup的配置，所以需要重启宿主才能关闭kmem。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:3","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"总结 本篇是一种快速暴力的解决问题手段，后经过调研测试，有不需要重启宿主的方案，在这一篇中介绍 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:4","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"cgroup泄露","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":" 线上宿主使用的kubernetes版本是1.12 ，kubelet默认是开启了kmem accounting的功能。kernel memory 在内核4.0以下的版本是一个实验特性，存在使用后不能删除cgroup的问题，造成cgroup泄漏。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:0:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"现象描述 宿主机上创建容器时失败，kubelet log中可见 报错信息如下 mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod79fe803c-072f-11e9-90ca-525400090c71/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b: no space left on devic ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:1:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"到底在泄漏什么 内核中对于每个子系统的的条目数是有限制的，限制的大小定义在kernel/cgroup.c#L139。 当正常在cgroup创建一个group的目录时，条目数就加1 .我们遇到的情况就是因为开启了kmem accounting功能，虽然cgroup的目录删除了，但是条目没有回收。这样后面就无法创建65535个cgroup了。也就是说，在当前内核版本下，开启了kmem accounting功能，会导致memory cgroup的条目泄漏无法回收。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:2:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"如何查看泄漏程度 正常情况下，可以通过以下命令来查看cgroup的数目 。 cat /proc/cgroups 但是存在泄漏的宿主机上这个数字是不可信的，因为这个计数就是随着cgroup下目录的增删进行。 经过请教内核组同学，他给出了一个可以统计cgroup条目的方法 # usage : stap -g get_memcg_count.stp %{ #include \u003clinux/rcupdate.h\u003e#include \u003clinux/cgroup.h\u003e#include \u003clinux/memcontrol.h\u003e /* The embedded c function must have a return value. * If it doesn't has an argument, we must use a specified void here. */ int get_memcg_count(void) { struct cgroup_subsys_state *tmp; int count = 1; int i; rcu_read_lock(); for (i = 1; i \u003c 65536; i++) { tmp = css_lookup(\u0026mem_cgroup_subsys, i); if (tmp) count++; } rcu_read_unlock(); return count; } %} function do_calc:long() %{ int count = 0; count = get_memcg_count(); STAP_RETVALUE = count; %} probe begin { printf(\"probe begin\\n\"); printf(\"count %ld\\n\", do_calc()); exit(); } probe end { printf(\"probe end\\n\"); } ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:3:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"如何引入 \u0026 如何解决 这个问题是kubernetes 1.9 版本引入的，kubelet创建容器代码中EnableKernelMemoryAccounting 导致的。关于这个问题的分析，网络上有很多文章进行分析，比如 https://github.com/kubernetes/kubernetes/issues/61937 https://github.com/moby/moby/issues/29638 https://tencentcloudcontainerteam.github.io/2018/12/29/cgroup-leaking/ http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage 在上篇cgroup泄露问题1进行了详细的分析，并提供了1.12.4版本（线上版本）的修复方案。 社区版本1.14，提供了开关可以关闭kmem accounting。 文章给出的方案简单总结就是重启宿主机后+关闭kmem account的kubelet，可以彻底解决这个问题。对于设置了node affinity的场景，重启宿主机成本较高，因此也就有了探索是否可以不重启宿主机的方案。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:4:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"cgroup迁移 下面对于memory cgroup子系统，简称为memcg 。对于已经泄漏的memcg，新创建的容器会继承父group，所以会加剧这个问题。如果我们通过一种cgroup迁移方式，将当前的memcg 迁移到另一个group，然后重新创建关闭了kmem accounting的group，并把原来的子group迁移回来是否就可以搞定这个问题了呢。 cgroup 本身是支持cgroup迁移功能的 4.2 Task migration When a task migrates from one cgroup to another, its charge is not carried forward by default. The pages allocated from the original cgroup still remain charged to it, the charge is dropped when the page is freed or reclaimed. You can move charges of a task along with task migration. See 8. \"Move charges at task migration\" This feature is disabled by default. It can be enabledi (and disabled again) by writing to memory.move_charge_at_immigrate of the destination cgroup. If you want to enable it: # echo (some positive value) \u003e memory.move_charge_at_immigrate Note: Each bits of move_charge_at_immigrate has its own meaning about what type of charges should be moved. See 8.2 for details. Note: Charges are moved only when you move mm-\u003eowner, in other words, a leader of a thread group. Note: If we cannot find enough space for the task in the destination cgroup, we try to make space by reclaiming memory. Task migration may fail if we cannot make enough space. Note: It can take several seconds if you move charges much. 我们是全量task 迁移，因此也不存在上面注意事项中提到的只支持迁移主线程的问题。 那我们梳理下，memcg 需要迁移内容包含哪些。 迁移后需要保证容器内存quota不变，容器的内存使用量不变，容器内的进程（对于cgroup来说，都是task）迁移后不丢。这三项分别对应的是memory.limit_in_bytes/memory.usage_in_bytes/tasks 因为内存使用量是memcg来控制的，我们看到的memory.usage_in_bytes 是只读的，所以这个文件中的数据迁移是依赖于task迁移来实现。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:5:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"dockerd内存泄露","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"背景 线上部分宿主机dockerd占用内存过大，有的甚至超过100G，而整个宿主上的容器使用的内存还不如dockerd一个进程使用的多，现在的处理办法是故障自愈，检测到dockerd使用内存超过10G后会设置live-restore，然后重启dockerd，而不影响正常运行的容器，但是重启后还一直存在内存泄露的问题。可以总结为两类内存泄露情况：没有设置live-restore: true的和设置了live-restore: true且重启过dockerd的，这里是针对后者的排查，因为线上默认dockerd没有开启debug模式，要想排查前者的问题，就需要重启docker，又因为没有配置live-restore: true，就会影响到正在运行的容器。 ","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:1","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"dockerd日志 tail -f /var/log/messages | grep dockerd，结果如下图，存在内存泄露的dockerd的日志都有如下的日志记录，且看时间规律是相同sandbox的记录每秒打印一遍 从源码中搜索日志内容，对应下面的源码分析-2里的内容。 查看dockerd启动时的日志，如下 ","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:2","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"步骤 步骤比较长，尤其是源码那部分，不关注源码的可以直接跳过源码，直接看代码的解释 pprof分析 开启dockerd的debug模式，即编辑/etc/docker/daemon.json，加上debug: true的配置并重启dockerd，方便利用pprof来定位内存泄露对应的代码位置。 执行go tool pprof http://ip:port/debug/pprof/heap，输入top命令查看内存分配情况，如下图 可以看到占用内存较多的函数调用，但是并不是很直观，可以继续输入web命令，会生成svg图片并通过画图软件或浏览器打开，如下图 这样就可以清楚地看到整个调用流程及各函数占用内存大小，可以发现是外部某程序调用了docker的api，最终调用SubscribeTopic，此函数里面存在内存泄露。 源码分析 明确了发生泄露的源码位置，接下来就是去看下源码的具体逻辑，下面贴出部分docker源码(tag v1.13.1)，省略部分不影响结果的代码 docker // 位置github.com/docker/docker/pkg/pubsub/publisher.go // SubscribeTopic adds a new subscriber that filters messages sent by a topic. func (p *Publisher) SubscribeTopic(topic topicFunc) chan interface{} { ch := make(chan interface{}, p.buffer) p.m.Lock() p.subscribers[ch] = topic p.m.Unlock() return ch } 这段代码很短，每次先new一个新的chan，然后把chan加入到字典中。可以看到如果发生内存泄漏，那八成就是这句p.subscribers[ch] = topic，也就是说会一直往map里添加新元素而得不到删除。为了验证，继续向上找此函数的调用方，沿着调用堆栈（svg图片里显示了具体的调用堆栈）往上找，如下 // Subscribe adds a new subscriber to the publisher returning the channel. func (p *Publisher) Subscribe() chan interface{} { // 这里调用SubscribeTopic return p.SubscribeTopic(nil) } // collect registers the container with the collector and adds it to // the event loop for collection on the specified interval returning // a channel for the subscriber to receive on. func (s *statsCollector) collect(c *container.Container) chan interface{} { s.m.Lock() defer s.m.Unlock() publisher, exists := s.publishers[c] if !exists { publisher = pubsub.NewPublisher(100*time.Millisecond, 1024) s.publishers[c] = publisher } // 这里调用Subscribe return publisher.Subscribe() } func (daemon *Daemon) subscribeToContainerStats(c *container.Container) chan interface{} { return daemon.statsCollector.collect(c) } // ContainerStats writes information about the container to the stream // given in the config object. func (daemon *Daemon) ContainerStats(ctx context.Context, prefixOrName string, config *backend.ContainerStatsConfig) error { ... // subscribe updates := daemon.subscribeToContainerStats(container) // unsubscribe defer daemon.unsubscribeToContainerStats(container, updates) noStreamFirstFrame := true for { select { case v, ok := \u003c-updates: if !ok { return nil } ... if !config.Stream \u0026\u0026 noStreamFirstFrame { // prime the cpu stats so they aren't 0 in the final output noStreamFirstFrame = false continue } if err := enc.Encode(statsJSON); err != nil { return err } if !config.Stream { return nil } case \u003c-ctx.Done(): return nil } } } 可以看到最终在ContainerStats中调用了subscribe并在此函数退出后调用defer里的unsubscribe。直接看代码可能看不懂，先介绍下docker stats的api，此api用来获取容器资源使用详情，包括cpu，memory，network等信息，支持两种方式，流和非流的方式，流是利用http的chunked属性实现的，非流的方式是直接返回。 chan****的产生 每次调用docker stats {container} 或者 docker stats的api的时候，都会进入到ContainerStats函数，这里使用了一个chan来达到异步的效果，即并不是每次调用stats都去实时的统计相关数据，而是有个后台goroutine在定时的stats（下面会介绍），并把数据推送到chan，每次调用api时只是去chan中获取数据而已，此chan对应的就是上面for select中的updates，即daemon.subscribeToContainerStats(container)的返回结果，也就是最终SubscribeTopic里new的chan。 chan****的消亡 在ContainerStats函数结束后会调用unsubscribeToContainerStats，这里会关闭创建出来的chan并从map中删除，释放内存。 知道了chan的产生和消亡，可以排除以stream形式调用api导致的，因为这种方式不会一直调用api，也就不会导致chan一直新建。那就只剩下一种形式，即外部有程序定时的以非stream的形式调用docker stats的api。但是上面也看到了函数结束后会在defer里释放掉申请的chan，为什么还会导致内存泄露呢？稍微注意一下就可以看到释放chan的函数是在defer里调用的，而且函数里还有个for循环，所以很可能是因为函数的for循环一直没有退出，导致defer一直得不到执行，chan也就一直释放不了，而且外部还定时的调用api，会导致一直会有新的chan的创建且旧的chan加入缓存后无法被删除，最终导致占用的内存越来越大。那就看下for循环内的代码，可以看到只有一个select，两个case，分别对应从updates chan中读数据和从ctx.Done()中读数据，后者是外部程序取消此次api调用后会得到执行的，即结束此次调用，前者是从updates chan中读stats数据，所以如果想要函数不退出，那么两个case都无法满足即可，即外部程序没有主动cancel request且updates chan中始终没有数据，先不管外部程序，因为也不知道外部程序是谁，代码怎么写的，但是可以肯定的是外部程序在调用docker api时的处理有问题，没有设置超时或者超时了也没有去cancel request。重点关注下为什么updates chan中一直没有数据，那就要看下写数据相关代码，如下 func (s *statsCollector) run() { type publishersPair struct { container *container.Container publisher *pubsub.Publisher } // we cannot determine the capacity here. // it will grow enough in first iteration var pairs []publishersPair // s.interval是1s，硬编码的 for range time.Tick(s.interval) { // it does not make sense in the f","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:3","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"总结 docker使用下来给人的感觉就是存在太多的问题了，后面还会有多篇有关docker资源泄露、目录umount失败、读写pipe失败等各式各样的问题。 ","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:4","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查","性能优化"],"content":"create pod slowly","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"背景 线上kubernetes集群从创建sts到创建pod需要时间很长，分钟级别，但是调度却很快。偶尔还会出现导致kube-odin任务失败（超过300s）的情况 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:1","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"排查过程 分析可能的原因： watch到sts的变化有延迟 sts从入队列到出队列耗时长 处理sts耗时长 kube-controller-manager中sts相关源码中有一些日志，需要把loglevel设置为4，即调试级别才会打印，里面就包括处理单个sts的耗时。首先把kube-controller-manager日志级别调到4，日志如下图, 最后显示的时间是从队列中拿到sts到处理完sts的整个过程的耗时，可以看到耗时并不长，在毫秒级别，那就可以排除掉处理单个sts耗时长的可能性了。 还剩下两种可能，不过细想的话，第一种可能也不大，因为watch是通用的，没道理同一个集群kube-controller-manager里的watch就慢，kube-scheuler的watch就快。那就很有可能是从watch到变化后把sts入队列到从队列中拿到sts这个阶段耗时太长了。源码中并没有这一部分的耗时统计，但是从源码中可以看到整个处理过程是同步到的，即watch的所有sts按顺序入队列，消费者在顺序的从队列中拿到，每消费完一个，再去拿另一个，串行执行，那问题就来了，虽然单个sts执行耗时在毫秒级，但是整个集群的sts数量在2000+，按平均每个sts耗时40ms计算，粗略估算一下处理完一轮的话也需要40ms*2000=80s的时间，到这里已经离真相不远了，但还有一个问题，那就是kube-controller-manager在初始化的时候是会把所有的sts加载一遍放入队列中的，处理完一遍哪怕耗时2分钟，但是处理完一遍之后只watch变化的sts，数量就会少很多了，所以处理完初始化时加载的所有sts后，按道理再有sts变化应该是可以及时处理的，因为此时队列中基本没有sts了。带着问题再去看源码，发现了一个神奇的地方，如下 setInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: ssc.enqueueStatefulSet, UpdateFunc: func(old, cur interface{}) { oldPS := old.(*apps.StatefulSet) curPS := cur.(*apps.StatefulSet) if oldPS.Status.Replicas != curPS.Status.Replicas { glog.V(4).Infof(\"Observed updated replica count for StatefulSet: %v, %d-\u003e%d\", curPS.Name, oldPS.Status.Replicas, curPS.Status.Replicas) } ssc.enqueueStatefulSet(cur) }, DeleteFunc: ssc.enqueueStatefulSet, }, statefulSetResyncPeriod, // 30s ) 这就对了，上面这段代码意思是只要watch到sts的变化，就会把对应的sts放入队列，且每隔30s会把全部sts重新入一遍队列，再加上刚才的估算，80s才能处理完所有sts，在未处理完之前（处理了30s时）就又会把所有的sts重新加入到队列中（并不是简单粗暴的把所有sts入队列，中间还会做一些处理，过滤掉一些不需要重复入队列的sts），这就会导致sts的待处理队列中始终有2000+个元素，新watch到的变化会加到队尾，从而导致sts创建后过了很久Pod才创建，因为sts controller一直在消费之前未处理完的其他sts了。 下面写了一个Demo来演示这个问题，代码很简单，如下 package main import ( \"fmt\" \"k8s.io/api/apps/v1\" \"k8s.io/client-go/informers\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/cache\" \"k8s.io/client-go/util/workqueue\" \"log\" \"math/rand\" \"time\" \"unsafe\" ) var queue workqueue.RateLimitingInterface func main() { defer queue.ShutDown() clientset, _ := kubernetes.NewForConfig(\u0026rest.Config{ Host: \"http://10.80.101.22:8080\", }) factor := rand.Float64() + 1 syncPeriod := time.Duration(float64(time.Duration(12*time.Hour).Nanoseconds()) * factor) informerFactory := informers.NewSharedInformerFactory(clientset, syncPeriod) stopChan := make(chan struct{}) informerFactory.Start(stopChan) informerFactory.WaitForCacheSync(stopChan) stsInformer := informerFactory.Apps().V1().StatefulSets().Informer() // 和sts controller一直，30s同步一遍所有sts stsInformer.AddEventHandlerWithResyncPeriod(cache.ResourceEventHandlerFuncs{ AddFunc: onAdd, UpdateFunc: onUpdate, DeleteFunc: onDelete, }, 30*time.Second) go consume() stsInformer.Run(stopChan) } func init() { queue = workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"statefulset\") } func consume() { for { func() { key, quit := queue.Get() if quit { fmt.Println(\"done\") return } defer queue.Done(key) sts := key.(*v1.StatefulSet) // 模拟处理单个sts耗时40ms time.Sleep(40 * time.Millisecond) log.Printf(\"cosumed %s, queue count: %d\", sts.Name, queue.Len()) }() } } func onAdd(obj interface{}) { sts := obj.(*v1.StatefulSet) if sts.Name == \"test-delay-sf-60f09\" { log.Printf(\"add address: %d\", unsafe.Pointer(sts)) } queue.Add(sts) } func onUpdate(old, new interface{}) { sts := old.(*v1.StatefulSet) // 测试用的sts，观察全量同步时的地址变化，用来确定是不通批次的同步，针对同一个sts来说是否地址相同 // 因为queue中用到sts作为map的key，所以此处打印地址，验证一下 if sts.Name == \"test-delay-sf-60f09\" { log.Printf(\"update address: %d\", unsafe.Pointer(sts)) } queue.Add(sts) } func onDelete(obj interface{}) { sts := obj.(*v1.StatefulSet) if sts.Name == \"test-delay-sf-60f09\" { log.Printf(\"update address: %d\", unsafe.Pointer(sts)) } queue.Add(sts) } 最终的输出结果 可以看到queue的长度从一开始的2000+一直降到1525，此时同步了一遍全量的sts，即2000+，queue中元素数量又生了上去。同时在未修改sts的情况，指定sts同步后的地址和同步前的地址相同824689074368。验证了之前的猜想，问题就出在了这里。 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:2","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"解决方案 想到两种优化方案 去掉定期（30s）全量同步的机制，目前看其他controller，如ReplicationController，ServiceController，EndpointsController等都没有设置定期全量同步 保留定期同步，添加已处理的sts的缓存，每次从queue中拿到一个新的sts时，比较已处理缓存中是否存在相同的sts(resourceversion相同)，存在则忽略此sts，否则进行处理 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:3","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"社区 上述问题已反馈社区，修复方式就是方案1，直接去掉了30s的同步机制。见https://github.com/kubernetes/kubernetes/pull/75622 这里需要注意一点：30s的同步机制并不是从kube-apiserver拉取全量数据，而是把Informer本地缓存的数据（位于Indexer中）全量同步一遍，目的是为了防止出现在事件处理函数中与外部组件交互时出错的情况，参考这个issue：https://github.com/kubernetes/kubernetes/issues/75495，但是sts控制器本身没有依赖任何外部（k8s以外）组件，所以就不需要30s同步了。但是我们以Operator实现的自定义Controller就需要根据实际情况激进型设置了，后面会专门有一个系列详细讲Informer的源码，敬请期待。 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:4","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查"],"content":"etcd watch内存泄露","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"背景 通过监控看到弹性云用户平台后端程序kube-odin的内存使用量在稳定增加，每次上线完又会恢复，可以判断出kube-odin中存在内存泄漏问题 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/:0:1","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"排查过程 golang程序的问题排查，无论CPU还是Memory问题都可以用官方提供的pprof工具，最简单的办法就是在kube-odin代码加入如下包net/http/pprof，上线到了测试环境，然后通过go tool pprof httpaddress的方式查看kube-odin内存消耗，网上也有很多pprof使用方法的文章，可以自行搜索，知道怎么用了之后看下图 由于是测试环境，对接测试集群，本身数据量就不大，程序刚启动时占用内存也就百十来M，现在已经用了1G多，占用内存最多的是newWatcherGrpcStream函数，还有一些其他的函数，占用的内存也在逐步增加，先看newWatcherGrpcStream函数，可以通过list查看其具体内存使用情况，如下图 fmt.Sprintf居然都占用了86.01M内存，还有几个chan占用的也比较多，但是基本都是无缓存的chan，正常不会占用这么多的。一般内存泄露可能是流未关闭，这种情况一般文件描述符也会泄露，另外就是用到缓存时也容易造成泄露，如果缓存的内容得不到释放且一直增加内容，内存就会越来越高。去看etcd相关代码，在代码中找问题，发现了一处很可疑的代码，去掉了无关内容，且增加了fmt.Println相关函数，方便观察每次运行到此处的缓存的结果，如下 // Watch posts a watch request to run() and waits for a new watcher channel func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan { ... ctxKey := fmt.Sprintf(\"%v\", ctx) // find or allocate appropriate grpc watch stream w.mu.Lock() if w.streams == nil { // closed w.mu.Unlock() ch := make(chan WatchResponse) close(ch) return ch } fmt.Println(ctxKey) // 打印缓存的key fmt.Println(len(w.streams)) //打印缓存数量 wgs := w.streams[ctxKey] if wgs == nil { fmt.Println(\"new watcher stream\") //缓存里没有对应的key wgs = w.newWatcherGrpcStream(ctx) w.streams[ctxKey] = wgs }else{ fmt.Println(\"use exist watcher stream\") //缓存里有key，复用缓存 } ... } // watcher implements the Watcher interface type watcher struct { remote pb.WatchClient // mu protects the grpc streams map mu sync.RWMutex // streams holds all the active grpc streams keyed by ctx value. streams map[string]*watchGrpcStream } 这里出现了上面的fmt.Sprintf、newWatcherGrpcStream等函数，而且出现了缓存，即w.streams，每次watch时都是先调用fmt.Sprintf获取到key，再从缓存中取，如果有则复用，没有则新建，问题很有可能出现在这里，然后再找一下缓存删除数据的逻辑，如下 func (w *watcher) Close() (err error) { w.mu.Lock() fmt.Println(\"begin close watcher\") streams := w.streams w.streams = nil w.mu.Unlock() for _, wgs := range streams { if werr := wgs.Close(); werr != nil { err = werr } } return err } func (w *watcher) closeStream(wgs *watchGrpcStream) { w.mu.Lock() fmt.Println(\"delete watch stream\") //开始删除缓存 close(wgs.donec) wgs.cancel() if w.streams != nil { fmt.Println(\"before delete:\",len(w.streams)) //删除前缓存数量 fmt.Println(wgs.ctxKey) if _,ok:=w.streams[wgs.ctxKey];ok{ fmt.Println(\"delete key exist\") //删除的key在缓存里存在 } else{ fmt.Println(\"delete key NOT exist\") //删除的key在缓存里不存在 } delete(w.streams, wgs.ctxKey) fmt.Println(\"after delete:\",len(w.streams)) //删除后缓存的数量 } w.mu.Unlock() } 和删除缓存相关的函数有两个，第一个Close函数只有在etcdclient的关闭链接时才会调用，而我们在不断的lock，unlock时其实用的是同一份etcdclient，所以不会是第一个函数。还剩一个closeStream函数，这里我也加了一些打印信息，用来查看缓存相关信息，closeStream调用如下 func (w *watcher) newWatcherGrpcStream(inctx context.Context) *watchGrpcStream { ... go wgs.run() return wgs } // run is the root of the goroutines for managing a watcher client func (w *watchGrpcStream) run() { ... defer func() { ... w.owner.closeStream(w) }() ... } 整个过程从插入缓存到删除缓存看起来都没有问题，只能写个demo测试一下了，demo大致如下 func main() { client := instance.GetEtcdClient() locker := lock.New(client, lock.WithTTL(1*time.Second)) go foo(locker) http.HandleFunc(\"/gc\", func(writer http.ResponseWriter, request *http.Request) { runtime.GC() }) http.ListenAndServe(\":8080\", nil) } func foo(locker lock.Locker) { ticker := time.NewTicker(1 * time.Second) ids := []string{\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"} for range ticker.C { for _, v := range ids { go func(i string) { unlock, _, err := locker.Trylock(context.TODO(), fmt.Sprintf(\"%s/%s\", \"/kaku/test/etcd/lock\", i)) if err != nil { if err != context.DeadlineExceeded { fmt.Println(\"lock task failed:%s\", err) } return } //fmt.Println(\"task has been locked\") defer func() { time.Sleep(time.Second) unlock() //fmt.Println(\"task has been unlocked\") }() }(v) } } } 特别简单，就是不断的去lock，unlock，结合之前增加的一些缓存打印信息，运行demo，结果如下 context.TODO.WithCancel.WithDeadline(2019-01-27 12:03:04.293267 +0800 CST m=+3.078016721 [750.274218ms]).WithCancel 0 new watcher stream context.TODO.WithCancel.WithDeadline(2019-01-27 12:03:04.293375 +0800 CST m=+3.078124824 [704.968531ms]).WithCancel 1 new watcher stream context.TODO.WithCancel.WithDeadline(2019-01-27 12:03:04.293271 +0800 CST m=+3.078020","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/:0:2","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"解决方案 升级etcd包版本至少到3.2.20 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/:0:3","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"etcd分布式锁加锁失败","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"现象 线上程序一直报错，错误信息：lock failed: context deadline exceeded, retry ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:1","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"排查过程 异常对应代码位置 很明显的是获取锁超时了，由于用的etcd的分布式锁，就怀疑是etcd出问题了，此时看到大量etcd日志，rejected connection from “ip:port” (error “tls: first record does not look like a TLS handshake”, ServerName “\")，怀疑是不是这个问题导致的，经过查询报错的IP，均为线上容器IP，登陆容器内看发现都是管理员平台的代码，里面也用到了py03的etcd，但是不是导致超时的原因。在排除各种可能之后，最后去etcd查看锁对应的key的情况，发现有两个key /notifier/locker/{leaseid} /notifier/locker/rwl/{leaseid} 其中第一个key是notifier自己添加的，第二个key在代码中搜不到，但是看起来像是redis whitelist的简写，先把第一个key删了，然后看notifier日志，仍然获取不到锁，所以怀疑是第二个key已经获得了锁，虽然key不一样。于是删除了第二个key，再看notifier日志，终于获得了锁，开始正常工作，于是得出猜想，etcd****的分布式锁，在子目录下加了锁之后，父目录会加锁失败。然后用etcdctl lock来验证了下，确实如此，/a/b下加了锁，/a再加锁就会失败，但是/a下加了锁，/a/b再加锁会成功。基本上可以验证上面的猜想，剩下的就是从etcd源码中找到对应处理的代码了。 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:2","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"etcd源码部分 在查询源码之前，第一反应就是这肯定是在服务端实现的，于是开始了从etcd服务端找相关源码的过程，从etcdctl命令开始追溯到所涉及的服务端，一直没有发现问题。又在网上搜了相关etcd服务端源码实现的文章，结合本地代码均没有想找的代码，于是反过来从client找起。 首先从etcdctl lock命令开始，挑主要函数展示 // 代码位置go.etcd.io/etcd/etcdctl/ctlv3/command/lock_command.go func lockUntilSignal(c *clientv3.Client, lockname string, cmdArgs []string) error { ... if err := m.Lock(ctx); err != nil { return err } ... } 接下来进入到Lock函数，这是个关键函数，etcd的分布式锁就是在这里实现的 func (m *Mutex) Lock(ctx context.Context) error { s := m.s client := m.s.Client() // 这里的pfx就是prefix，就是传进来的前缀，后面的s.Lease()会返回一个租约，是一个int64的整数，和session有关 m.myKey = fmt.Sprintf(\"%s%x\", m.pfx, s.Lease()) // 这里比较上面prefix/lease的createrevision是否为0，为0表示目前不存在该key，需要执行Put操作，下面可以看到 // 不为0表示已经有对应的key了，只需要执行Get就行 // createrevision是自增的 cmp := v3.Compare(v3.CreateRevision(m.myKey), \"=\", 0) // put self in lock waiters via myKey; oldest waiter holds lock put := v3.OpPut(m.myKey, \"\", v3.WithLease(s.Lease())) // reuse key in case this session already holds the lock get := v3.OpGet(m.myKey) // 获取所得持有者 getOwner := v3.OpGet(m.pfx, v3.WithFirstCreate()...) resp, err := client.Txn(ctx).If(cmp).Then(put, getOwner).Else(get, getOwner).Commit() if err != nil { return err } m.myRev = resp.Header.Revision if !resp.Succeeded { m.myRev = resp.Responses[0].GetResponseRange().Kvs[0].CreateRevision } // if no key on prefix / the minimum rev is key, already hold the lock ownerKey := resp.Responses[1].GetResponseRange().Kvs // 比较如果当前没有人获得锁或者锁的owner的createrevision等于当前的kv的revision，则表示已获得锁，就可以退出了 if len(ownerKey) == 0 || ownerKey[0].CreateRevision == m.myRev { m.hdr = resp.Header return nil } // 为了验证自己加的打印信息 //fmt.Printf(\"ownerKey: %s\\n\", ownerKey) // 走到这里代表没有获得锁，需要等待之前的锁被释放，即revision小于当前revision的kv被删除 hdr, werr := waitDeletes(ctx, client, m.pfx, m.myRev-1) // release lock key if wait failed if werr != nil { m.Unlock(client.Ctx()) } else { m.hdr = hdr } return werr } // waitDeletes 等待所有当前比当前key的revision小的key被删除后，锁释放后才返回 func waitDeletes(ctx context.Context, client *v3.Client, pfx string, maxCreateRev int64) (*pb.ResponseHeader, error) { getOpts := append(v3.WithLastCreate(), v3.WithMaxCreateRev(maxCreateRev)) for { resp, err := client.Get(ctx, pfx, getOpts...) if err != nil { return nil, err } if len(resp.Kvs) == 0 { return resp.Header, nil } lastKey := string(resp.Kvs[0].Key) // 为了调试自己加的这句 fmt.Printf(\"wait for %s to delete\\n\", lastKey) if err = waitDelete(ctx, client, lastKey, resp.Header.Revision); err != nil { return nil, err } } } func waitDelete(ctx context.Context, client *v3.Client, key string, rev int64) error { cctx, cancel := context.WithCancel(ctx) defer cancel() var wr v3.WatchResponse // wch是个channel，key被删除后会往这个chan发数据 wch := client.Watch(cctx, key, v3.WithRev(rev)) for wr = range wch { for _, ev := range wr.Events { if ev.Type == mvccpb.DELETE { return nil } } } if err := wr.Err(); err != nil { return err } if err := ctx.Err(); err != nil { return err } return fmt.Errorf(\"lost watcher waiting for delete\") } 看完上面的代码基本知道了etcd分布式锁的实现机制了，但是还没看到哪里和前缀Prefix相关了。其实答案就藏在getOwner里，看上述代码，不管是执行Put还是Get，最终都有个getOwner的过程，看一下这个getOwner，options模式里有个v3.WithFirstCreate函数调用，看下这个函数 // WithFirstCreate gets the key with the oldest creation revision in the request range. func WithFirstCreate() []OpOption { return withTop(SortByCreateRevision, SortAscend) } // withTop gets the first key over the get's prefix given a sort order func withTop(target SortTarget, order SortOrder) []OpOption { return []OpOption{WithPrefix(), WithSort(target, order), WithLimit(1)} } // WithPrefix enables 'Get', 'Delete', or 'Watch' requests to operate // on the keys with matching prefix. For example, 'Get(foo, WithPrefix())' // can return 'foo1', 'foo2', and so on. func WithPrefix() OpOption { return func(op *Op) { if len(op.key) == 0 { op.key, op.end = []byte{0}, []byte{0} return } op.end = getPrefix(op.key) } } 看到上面的是三个函数后，大致就找到了对应的源码的感觉，因为看到了WithPrefix函数，和上面的猜测正好匹配。所以getOwner的具体执行效果是会把所有以lockkey开头的kv都拿到，且按照createrevision升序排列，取第一个值，这个意思就很明白了，就是要拿到当前以lockkey为prefix的且createrevision最小的那个key，就是","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:3","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"总结 通过分析问题，看源码，可以了解到etcd锁的实现原理，以及可能存在的小坑。etcd居然把锁的实现放在了client端，也是出乎我的意料，这样的话，可以直接修改client端代码来修改其锁的实现，就可能出现虽然共用一个服务端，但是etcd行为却不一致的问题，不知道为何要这么设计，个人感觉还是要放到服务端更好些。 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:4","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["使用说明"],"content":"docker image p2p","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"测试环境 10.0.13.19 部署harbor，单点，docker-compose的方式部署 4核8G 10.0.13.22 dragonfly的supernode节点 16核64G docker方式部署 10.0.13.31 dragonfly的supernode节点 16核64G docker方式部署 kubernetes 集群 20个节点 ，docker storage-driver overlay 部署了dragonfly的daemon和dfget等程序 以上均为虚机，在同一个网段内，centos7.4系统 涉及到的ansible脚本在这里 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:1:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"dragonfly ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"特性 基于P2P文件分发 支持各种容器化技术 主机级别限速策略 利用CDN机制避免远程重复下载 强一致性 磁盘保护,高效的IO处理 高性能 异常自动隔离 降低文件来源服务器压力 支持标准的Http Header 使用简单 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:1","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"结构介绍 分发普通文件 注: 其中cluster manager即超级节点(supernode) 超级节点充当CDN，同时调度每个对等者(peer)在他们之间传输文件块。dfget是P2P客户端，也称为对等者(peer)，主要用于下载和共享文件块。 分发容器镜像 图中镜像仓库(registry)类似于文件服务器。dfget proxy也称为dfdaemon，它拦截来自docker pull和docker push的HTTP请求，然后将那些跟镜像分层相关的请求使用dfget来处理。 文件分块是怎么下载的 注: 其中cluster manager即超级节点(supernode) 每个文件会被分成多个块在对等者(peer)间进行传输。一个peer就是一个P2P客户端。 超级节点会判断文件是否存在本地，如果不存在，则会将其从文件服务器下载到本地。 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:2","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"流程解析 1.当执行docker pull操作时,dfget-proxy会拦截docker pull请求。将请求转发给CM(cluster manager)。 cm的地址已经在client主机的/etc/dragonfly.conf文件中配置好了。另外上文中提到的dfget-proxy其实就是df-daemon。Dragonfly中有三个项目,client端:getter(python)、daemon(golang),docker pull时,df-daemon拦截到请求并通过dfget进行文件拉取,server端:supernode(java)。 2.df-daemon启动的时候带了registry参数,并且通过dfget传给服务端supernode。supernode解析参数到对应的镜像仓库获取镜像并以block的形式返回给客户端。如果再次拉取镜像时,supernode就会检测哪一个client存在和镜像文件对应的block,如果存在直接从该client下载,如果不存在就通过server端到镜像仓库拉取镜像。 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:3","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"安装部署 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"安装服务端 Dragonfly官方支持基于Docker和Physical Machine两种方案部署server，这里为了方便直接使用docker方式部署。 docker run -d -p 8001:8001 -p 8002:8002 --restart=always registry.cn-hangzhou.aliyuncs.com/alidragonfly/supernode:0.2.0 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:1","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"安装客户端 #下载 此链接为0.1.0版本 wget https://github.com/alibaba/Dragonfly/raw/master/package/df-client.linux-amd64.tar.gz #解压 tar -zxvf df-client.linux-amd64.tar.gz #设置Env vim ~/.bashrc #将下面的设置添加到~/.bashrc文件末 PATH=$PATH:/root/df-client #退出vim并执行以下命令 source ~/.bashrc ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:2","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"Harbor搭建 基于在线方式的安装, 本文采用http的方式配置Harbor。Harbor版本为1.2.2 1.下载 wget https://storage.googleapis.com/harbor-releases/harbor-online-installer-v1.5.2.tgz tar -zxvf harbor-online-installer-v1.5.2.tgz 2.修改配置 cd harbor vim harbor.cfg hostname=10.0.13.19 //设置为当前主机ip 3.安装并启动harbor服务 服务启动以后,如果需要管理Harbor服务的生命周期,可以直接通过docker-compose来管理 sh install.sh ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:3","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"使用指南 1.在client主机上通过配置文件指定CM(cluster manager)节点 vi /etc/dragonfly.conf 内容: [node] address=10.0.13.22,10.0.13.31 2.**由于当前Dragonfly暂不支持harbor认证。如果按照官网配置\"configure daemon mirror\"来拉取镜像会提示授权失败。**为了绕过这个问题可以采用docker proxy的方式来解决。具体步骤如下: （1）vi /etc/systemd/system/docker.service.d/http-proxy.conf,没有该文件就直接创建该文件。通过添加proxy，在拉取镜像时将会通过下面的配置地址转发到目标机。 [Service] Environment=\"HTTP_PROXY=http://127.0.0.1:65001\" （2）更新变更 systemctl daemon-reload 3.在client机上添加harbor的insecure地址，在/etc/docker/daemon.json的insecure-registries中添加10.0.13.19 {\"disable-legacy-registry\":false,\"graph\":\"/data/docker\",\"insecure-registries\":[\"10.0.13.19\"]} 4.启动client服务 df-daemon --registry http://10.0.13.19 5.重启docker systemctl restart docker 6.docker登录 docker login --username=admin 10.0.13.19 提示登录成功说明上述配置正确 7.验证 #有数据经过65001端口则配置正确 tcpdump -i lo port 65001 #另开一个shell执行 docker pull 10.0.13.19/kaku/bigimage:v1.0 #拉取镜像 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:4","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"测试结果 2个supernode，20个节点并行的拉镜像 镜像大小（压缩） native cost dragonfly cost native harbor流量 dragonfly harbor流量 1.28G 2m多 稳定在1m30s左右 20*1.28 2*1.28 3.48G 10+m 稳定在5m30s左右 20*3.48 2*3.48 使用df之后，对harbor的压力明显减小，在20个节点时测试结果为 约33%的流量是通过p2p的方式获得的，随着节点数的增多，此值还会继续增大； 各节点镜像拉取时间稳定，比不使用代理时好很多，但是在单节点拉镜像时，使用代理时的耗时是要比原生docker pull耗时长的 目前部署的df supernode为0.2.0版本，client为0.0.1版本，尝试用0.1.0、0.1.1版本的client均失败，见这里 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:4:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"参考 dragonfly与harbor组建支持P2P的镜像服务 https://github.com/alibaba/Dragonfly/issues/17 https://github.com/alibaba/Dragonfly/issues/20 https://github.com/alibaba/Dragonfly/issues/50#issuecomment-382286474 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:5:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"kubernetes启用GPU","date":"2018-09-06","objectID":"https://www.likakuli.com/posts/kubernetes-gpu/","tags":["kubernetes"],"title":"Kubernetes 启用GPU","uri":"https://www.likakuli.com/posts/kubernetes-gpu/"},{"categories":["使用说明"],"content":"kubernetes设置 k8s 1.10之前需要在kube-apiserver、kube-controller-manager、kube-scheduler、kubelet中开启如下feature，如果不是首次部署的话，重启以上所有组件： –feature-gates=“DevicePlugins=true” 安装 NVIDIA Driver~=361.93 安装nvidia-docker2，由于目前使用的docker版本提示信息里包含自定义字样，只能使用如下方式安装： 所有安装方式参考这里 # If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f sudo yum remove nvidia-docker # Add the package repositories distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \\ sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo # Install the nvidia runtime hook sudo yum install -y nvidia-container-runtime-hook sudo mkdir -p /usr/libexec/oci/hooks.d echo -e '#!/bin/sh\\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" exec nvidia-container-runtime-hook \"$@\"' | \\ sudo tee /usr/libexec/oci/hooks.d/nvidia sudo chmod +x /usr/libexec/oci/hooks.d/nvidia # Test nvidia-smi with the latest official CUDA image # You can't use `--runtime=nvidia` with this setup. docker run --rm nvidia/cuda nvidia-smi 安装nvidia-container-runtime，在上一步中已经安装了对应的yum repo，这里直接执行如下命令即可： 因为使用了上一步的安装方式，所以需要进行这一步的安装，如果是通过yum直接安装的nvidia-docker2，则不需要进行此步。 # install runtime yum install nvidia-container-runtime update docker daemon，在docker daemon中添加如下配置 daemon.json中添加如下配置，可选配置为\"default-runtime\": “nvidia”，如果不设置默认runtime，则默认使用runc，启动容器是需要指定–runtime=nvidia \"default-runtime\": \"nvidia\"， \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } 安装NVIDIA device plugin，插件以daemonset方式部署，如果集群中既有CPU也有GPU节点，可以通过label筛选出GPU节点，无GPU的节点无需部署此程序 # For Kubernetes v1.8 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml # For Kubernetes v1.9 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml ","date":"2018-09-06","objectID":"https://www.likakuli.com/posts/kubernetes-gpu/:1:0","tags":["kubernetes"],"title":"Kubernetes 启用GPU","uri":"https://www.likakuli.com/posts/kubernetes-gpu/"},{"categories":["使用说明"],"content":"测试 apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: GPU: \"true\" //测试时自己给对应GPU节点加了GPU=true的label kubectl create -f test.yaml 执行结果： [root@vm10-0-13-17 ~]# kubectl get pods -a NAME READY STATUS RESTARTS AGE cuda-vector-add 0/1 Completed 0 3h nvidia-device-plugin-daemonset-pv6z8 1/1 Running 0 4h [root@vm10-0-13-17 ~]# kubectl logs cuda-vector-add [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done ","date":"2018-09-06","objectID":"https://www.likakuli.com/posts/kubernetes-gpu/:2:0","tags":["kubernetes"],"title":"Kubernetes 启用GPU","uri":"https://www.likakuli.com/posts/kubernetes-gpu/"},{"categories":["使用说明"],"content":"golang监控","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"看了一篇文章，里面涉及到了一些golang程序监控的问题，回过头总结了一下实现方式，简单介绍一下 ","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:0:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"expvar go自带的runtime包拥有各种功能，包括goroutine数量，设置逻辑线程数量，当前go版本，当前系统类型等等。前两天发现了go标准库还有一个更好用的可以监控服务运行各项指标和状态的包—-expvar。 expvar包为监控变量提供了一个标准化的接口，它以 JSON 格式通过 /debug/vars 接口以 HTTP 的方式公开这些监控变量以及我自定义的变量。通过它，再加上metricBeat，ES和Kibana，可以很轻松的对服务进行监控。我这里是用gin把接口暴露出来，其实用别的web框架也都可以。下面我们来看一下如何使用它（示例代码使用GIN HTTP web framework）： package main import ( \"expvar\" \"github.com/gin-gonic/gin\" \"net/http\" \"net/http/pprof\" \"time\" ) func main() { router := gin.Default() router.GET(\"/debug/vars\", monitor.GetCurrentRunningStats) s := \u0026http.Server{ Addr: \":9090\", Handler: router, ReadTimeout: 5 * time.Second, WriteTimeout: 5 * time.Second, MaxHeaderBytes: 1 \u003c\u003c 20, } s.ListenAndServe() } 对应的handler package monitor import ( \"encoding/json\" \"expvar\" \"fmt\" \"github.com/gin-gonic/gin\" \"net/http\" \"runtime\" \"time\" ) // 开始时间 var start = time.Now() // calculateUptime 计算运行时间 func calculateUptime() interface{} { return time.Since(start).String() } // currentGoVersion 当前 Golang 版本 func currentGoVersion() interface{} { return runtime.Version() } // getNumCPUs 获取 CPU 核心数量 func getNumCPUs() interface{} { return runtime.NumCPU() } // getGoOS 当前系统类型 func getGoOS() interface{} { return runtime.GOOS } // getNumGoroutins 当前 goroutine 数量 func getNumGoroutins() interface{} { return runtime.NumGoroutine() } // getNumCgoCall CGo 调用次数 func getNumCgoCall() interface{} { return runtime.NumCgoCall() } var lastPause uint32 // getLastGCPauseTime 获取上次 GC 的暂停时间 func getLastGCPauseTime() interface{} { var gcPause uint64 ms := new(runtime.MemStats) statString := expvar.Get(\"memstats\").String() if statString != \"\" { json.Unmarshal([]byte(statString), ms) if lastPause == 0 || lastPause != ms.NumGC { gcPause = ms.PauseNs[(ms.NumGC+255)%256] lastPause = ms.NumGC } } return gcPause } // GetCurrentRunningStats 返回当前运行信息 func GetCurrentRunningStats(c *gin.Context) { c.Writer.Header().Set(\"Content-Type\", \"application/json; charset=utf-8\") first := true report := func(key string, value interface{}) { if !first { fmt.Fprintf(c.Writer, \",\\n\") } first = false if str, ok := value.(string); ok { fmt.Fprintf(c.Writer, \"%q: %q\", key, str) } else { fmt.Fprintf(c.Writer, \"%q: %v\", key, value) } } fmt.Fprintf(c.Writer, \"{\\n\") expvar.Do(func(kv expvar.KeyValue) { report(kv.Key, kv.Value) }) fmt.Fprintf(c.Writer, \"\\n}\\n\") c.String(http.StatusOK, \"\") } func init() { //这些都是我自定义的变量，发布到expvar中，每次请求接口，expvar会自动去获取这些变量，并返回给我 expvar.Publish(\"运行时间\", expvar.Func(calculateUptime)) expvar.Publish(\"version\", expvar.Func(currentGoVersion)) expvar.Publish(\"cores\", expvar.Func(getNumCPUs)) expvar.Publish(\"os\", expvar.Func(getGoOS)) expvar.Publish(\"cgo\", expvar.Func(getNumCgoCall)) expvar.Publish(\"goroutine\", expvar.Func(getNumGoroutins)) expvar.Publish(\"gcpause\", expvar.Func(getLastGCPauseTime)) } 运行程序，访问http://localhost:9090/debug/vars，如下 可以看到，expvar返回给了我我之前自定义的数据，以及它本身要默认返回的数据，比如memstats。这个memstats是干嘛的呢，其实看到这些字段名就可以知道，是各种内存堆栈以及GC的一些信息，具体可以看源码注释： type MemStats struct { // General statistics. // Alloc is bytes of allocated heap objects. // // This is the same as HeapAlloc (see below). Alloc uint64 // TotalAlloc is cumulative bytes allocated for heap objects. // // TotalAlloc increases as heap objects are allocated, but // unlike Alloc and HeapAlloc, it does not decrease when // objects are freed. TotalAlloc uint64 // Sys is the total bytes of memory obtained from the OS. // // Sys is the sum of the XSys fields below. Sys measures the // virtual address space reserved by the Go runtime for the // heap, stacks, and other internal data structures. It's // likely that not all of the virtual address space is backed // by physical memory at any given moment, though in general // it all was at some point. Sys uint64 // Lookups is the number of pointer lookups performed by the // runtime. // // This is primarily useful for debugging runtime internals. Lookups uint64 // Mallocs is the cumulative count of heap objects allocated. // The number of live objects is Mallocs - Frees. Mallocs uint64 // Frees is the cumulative count of heap objects f","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:1:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"pprof 有关pprof的基本介绍和使用，可以参考这里，最简单的使用方式如下 package main import ( \"net/http\" _ \"net/http/pprof\" ) func main() { http.ListenAndServe(\":9090\", nil) } 运行程序，访问http://localhost:9090/debug/pprof可以看到如下结果 配合go tool pprof一起使用，例如查看cpu详情，可以通过如下命令实现 root@kaku-Inspiron-7537:~/blog# go tool pprof localhost:9090/debug/pprof/profile Fetching profile over HTTP from http://localhost:9090/debug/pprof/profile Saved profile in /root/pprof/pprof.___go_build_main_go__1_.samples.cpu.002.pb.gz File: ___go_build_main_go__1_ Type: cpu Time: Jul 2, 2018 at 12:44am (CST) Duration: 30s, Total samples = 0 Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) web 执行完之后，如果本地安装了graphviz和chrome，则自动打开浏览器，可以在其中看到如下图片 因为示例程序没有任何其他代码，之暴露了http服务，所以看到的结果比较单调 ","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:2:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"Prometheus Prometheus client内置了golang metrics暴露的handler，只需要简单调用即可实现，如下 package main import ( \"github.com/prometheus/client_golang/prometheus/promhttp\" \"net/http\" ) func main() { http.Handle(\"/metrics\", promhttp.Handler()) panic(http.ListenAndServe(\":9090\", nil)) } 运行程序，访问http://localhost:9090/metrics 即可。 同时可以通过Prometheus来采集此Endpoint暴露出来的数据，也可以进行自定义数据的采集，参考这里 ","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:3:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"Prometheus","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"公司采用Prometheus来采集Kubernetes集群的性能指标数据，之前对性能数据采集这方面没有关注过，但是实际生产环境下有很多此类需求，因此重点学习了一下Prometheus采集数据的原理以及如何部署，接下来分别介绍。 Prometheus版本 2.3.0 AlertManager版本 1.4.0 因为两者的配置对旧版本的兼容不是很好，在按照网上搜索的资料进行部署时遇到了不少坑，所以当你看到这篇文章，根据文章进行部署时，可能我现在使用的版本已经很旧，您可能需要按需修改配置 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:0:0","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"原理 这张图大家应该不陌生，没错，就是从官网偷来的图片，下面特征和组件也是偷来的，哈哈。 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:1:0","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"特征 Prometheus的主要特点是： 一个具有由metric名称和键/值对标识的多维时间序列的数据模型 一种灵活的查询语言 来利用这种维度 不依赖分布式存储; 单个服务器节点是自治的（这点真不能认同，没有HA让人很难受） 时间序列收集通过HTTP上的拉式模型进行 推送时间序列通过中间网关支持 通过服务发现或静态配置来发现目标 多种模式的图形和仪表盘支持 AlertManager的主要特点： Grouping 分组 Inhibition 抑制 Silences HA (配置方式有限制，只能罗列出所有的实例，不能通过负载均衡方式配置) ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:1:1","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"组件 普罗米修斯生态系统由多个组件组成，其中许多组件是可选的： Prometheus服务器用来收集和存储时间序列数据 用于检测应用程序代码的客户端库 支持Short-lived job的Pushgateway 专门用于HAProxy，StatsD，Graphite等服务的exporter 一个警报处理器alertmanager 各种支持工具 大多数Prometheus组件都是用Go编写的，因此它们很容易构建和部署为静态二进制文件。 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:1:2","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"部署 这里主要通过Kubernetes部署Prometheus和AlertManager，实例数都只开1，原因也很好理解，Prometheus不支持HA，实例开多了也没有用，而且每个实例存的数据因为采集时间的原因会不一致，AlertManager只开一个实例的原因是虽然支持HA，但是只能在配置Prometheus时罗列出所有的实例，显然通过Kubernetes部署时无法获取到所有AlertManager的Pod实例，只能获得对应的Service，但是service就属于负载均衡了，如果AlertManager选在在集群外通过Docker或者二进制文件直接部署的话，可以开启多个实例，这里暂时在集群内部署，下面将列出所用到的Yaml文件。 首先，给出Prometheus相关的文件 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:0","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"prometheus.config.yaml apiVersion:v2kind:ConfigMapmetadata:name:prometheus-confignamespace:kube-systemdata:prometheus.yml:|global: scrape_interval: 15s evaluation_interval: 15s rule_files: - /etc/prometheus-rules/*.rules alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+)target_label:__metrics_path__replacement:/api/v1/nodes/${1}/proxy/metrics/cadvisor- job_name:'kubernetes-service-endpoints'kubernetes_sd_configs:- role:endpointsrelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scheme]action:replacetarget_label:__scheme__regex:(https?)- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__, __meta_kubernetes_service_annotation_prometheus_io_port]action:replacetarget_label:__address__regex:([^:]+)(?::\\d+)?;(\\d+)replacement:$1:$2- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]action:replacetarget_label:kubernetes_name- job_name:'kubernetes-services'kubernetes_sd_configs:- role:servicemetrics_path:/probeparams:module:[http_2xx]relabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__address__]target_label:__param_target- target_label:__address__replacement:blackbox-exporter.example.com:9115- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]target_label:kubernetes_name- job_name:'kubernetes-ingresses'kubernetes_sd_configs:- role:ingressrelabel_configs:- source_labels:[__meta_kubernetes_ingress_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]regex:(.+);(.+);(.+)replacement:${1}://${2}${3}target_label:__param_target- target_label:__address__replacement:blackbox-exporter.example.com:9115- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_ingress_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_ingress_name]target_label:kubernetes_name- job_name:'kubernetes-pods'kubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:1","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"Prometheus.deploy.yaml ---apiVersion:apps/v1beta2 kind:Deployment metadata:labels:name:prometheus-deployment name:prometheus namespace:kube-system spec:replicas:1selector:matchLabels:app:prometheus template:metadata:labels:app:prometheus spec:containers:- image:quay.io/prometheus/prometheus name:prometheus command:- \"/bin/prometheus\"args:- \"--config.file=/etc/prometheus/prometheus.yml\"- \"--storage.tsdb.path=/prometheus\"- \"--storage.tsdb.retention=24h\"ports:- containerPort:9090protocol:TCP volumeMounts:- mountPath:\"/prometheus\"name:data - mountPath:\"/etc/prometheus\"name:config-volume - name:rules-volume mountPath:/etc/prometheus-rules resources:requests:cpu:100m memory:100Mi limits:cpu:500m memory:2500Mi serviceAccountName:prometheus volumes:- name:data emptyDir:{}- name:config-volume configMap:name:prometheus-config - name:rules-volume configMap:name:prometheus-rules--kind:ServiceapiVersion:v1metadata:labels:app:prometheusname:prometheusnamespace:kube-systemspec:type:NodePortports:- port:9090targetPort:9090nodePort:30003selector:app:prometheus 成功部署后，通过http://nodeIP:30003就可以访问Prometheus的页面了，如下图 scrape_configs中配置的kubernetes-pods、kubernetes-services等Job，可能在target界面看不到，因为需要添加对应的Annotation，例如Pod添加如下的Annotation后，就会在上面的界面中出现，注意这里是在Pod的annotation里添加，而不是在Deploy的annotation prometheus.io/scrape:\"true\" service的话按照上述配置中写的，则需要添加如下的Annotation prometheus.io/probe:\"true\" 我们可以在Graph界面进行查询，可以以Console或者Graph的形式展示查询的结果，Prometheus提供了强大的查询语法，参考这里，下图为查询每个Node的CPU使用率 此时还没有进行AlertManager的相关配置，在Alerts页面看不到任何内容，接下来我们部署AlertManager ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:2","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"alertmanager.config.yaml kind:ConfigMapapiVersion:v1metadata:name:alertmanagernamespace:kube-systemdata:config.yml:|-global: resolve_timeout: 5m smtp_smarthost: 'mailhost:port' smtp_from: 'sender@example.com' smtp_auth_username: 'sender@example.com' smtp_auth_password: 'password' slack_api_url: 'https://hooks.slack.com/services/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' templates: - '/etc/alertmanager-templates/*.tmpl' route: group_by: ['alertname', 'cluster', 'service'] group_wait: 30s group_interval: 5m repeat_interval: 15m receiver: slack-notifications receiver: email_alert routes: - match: severity: email receiver: email_alert receivers: - name: 'email_alert' email_configs: - to: 'to@example.com' - name: 'slack-notifications' slack_configs: - channel: '#alert' send_resolved: true 类似Prometheus，我们把AlertManager的配置同样保存为ConfigMap，部署时修改邮箱、slack等对应配置为自己的值，也可以添加自己的接收途径，具体参考这里，上述配置的意思是告诉AlertManager按照alertname、cluster、service对收到的通知进行分组，每隔大约5+15分钟发送一次报警信息，报警信息默认通过发送到slack，如果信息中包含severity: email的label，那么警报还会同时再发送到接收邮箱。 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:3","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"alertmanager.deploy.yaml apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:alertmanagernamespace:kube-systemspec:replicas:1selector:matchLabels:app:alertmanagertemplate:metadata:name:alertmanagerlabels:app:alertmanagerspec:containers:- name:alertmanagerimage:prom/alertmanager:latestargs:- '--config.file=/etc/alertmanager/config.yml'- '--storage.path=/alertmanager'ports:- name:alertmanagercontainerPort:9093volumeMounts:- name:config-volumemountPath:/etc/alertmanager- name:templates-volumemountPath:/etc/alertmanager-templates- name:alertmanagermountPath:/alertmanagervolumes:- name:config-volumeconfigMap:name:alertmanager- name:templates-volumeconfigMap:name:alertmanager-templates- name:alertmanageremptyDir:{}---apiVersion:v1kind:Servicemetadata:annotations:prometheus.io/scrape:'true'labels:name:alertmanagername:alertmanagernamespace:kube-systemspec:selector:app:alertmanagerports:- name:alertmanagerprotocol:TCPport:9093targetPort:9093 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:4","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"prometheus-rules node-cpu-usage.rules groups:- name:node-cpu-usagerules:- alert:HighRateErrorexpr:(100 - (avg by (instance) (irate(node_cpu{name=\"node-exporter\",mode=\"idle\"}[5m])) * 100)) \u003e 75for:2mlabels:severity:pageannotations:summary:\"{{$labels.instance}}: High CPU usage detected\"description:\"{{$labels.instance}}: CPU usage is above 75% (current value is: {{ $value }})\" prometheus-rules为文件夹，所有的通知规则都放在此文件夹下面，有关rule文件配置，可以参考这里 通过如下命令创建rule对应的ConfigMap kubectl create cm prometheus-rules --from-file=prometheus-rules -o yaml --dry-run | kubectl create -n kube-system -f - ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:5","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"alertmanager-templates default.tmpl {{ define \"__alertmanager\" }}AlertManager{{ end }} {{ define \"__alertmanagerURL\" }}{{ .ExternalURL }}/#/alerts?receiver={{ .Receiver }}{{ end }} {{ define \"__subject\" }}[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join \" \" }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join \" \" }}{{ end }}){{ end }}{{ end }} {{ define \"__description\" }}{{ end }} {{ define \"__text_alert_list\" }}{{ range . }}Labels: {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }} {{ end }}Annotations: {{ range .Annotations.SortedPairs }} - {{ .Name }} = {{ .Value }} {{ end }}Source: {{ .GeneratorURL }} {{ end }}{{ end }} {{ define \"slack.default.title\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"slack.default.username\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"slack.default.fallback\" }}{{ template \"slack.default.title\" . }} | {{ template \"slack.default.titlelink\" . }}{{ end }} {{ define \"slack.default.pretext\" }}{{ end }} {{ define \"slack.default.titlelink\" }}{{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"slack.default.iconemoji\" }}{{ end }} {{ define \"slack.default.iconurl\" }}{{ end }} {{ define \"slack.default.text\" }}{{ end }} {{ define \"hipchat.default.from\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"hipchat.default.message\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"pagerduty.default.description\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"pagerduty.default.client\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"pagerduty.default.clientURL\" }}{{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"pagerduty.default.instances\" }}{{ template \"__text_alert_list\" . }}{{ end }} {{ define \"opsgenie.default.message\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"opsgenie.default.description\" }}{{ .CommonAnnotations.SortedPairs.Values | join \" \" }} {{ if gt (len .Alerts.Firing) 0 -}} Alerts Firing: {{ template \"__text_alert_list\" .Alerts.Firing }} {{- end }} {{ if gt (len .Alerts.Resolved) 0 -}} Alerts Resolved: {{ template \"__text_alert_list\" .Alerts.Resolved }} {{- end }} {{- end }} {{ define \"opsgenie.default.source\" }}{{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"victorops.default.message\" }}{{ template \"__subject\" . }} | {{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"victorops.default.from\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"email.default.subject\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"email.default.html\" }} \u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"\u003e \u003c!-- Style and HTML derived from https://github.com/mailgun/transactional-email-templates The MIT License (MIT) Copyright (c) 2014 Mailgun Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --\u003e \u003chtml xmlns=\"http://www.w3.org/1999/xhtml\" xmlns=\"http://www.w3.org/1999","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:6","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["源码分析"],"content":"golang netpoll 源码分析","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"简介 go针对不同的操作系统，其网络io模型不同，可以从go源码目录结构和对应内容清楚的看到各平台的io模型，如针对linux系统实现的epoll，针对windows操作系统实现的iocp等，这里主要看针对linux系统的实现，涉及到的文件大体如下： runtime/netpoll.go runtime/netpoll_epoll.go runtime/proc.go net/fd_unix.go internal/poll/fd_poll_runtime.go internal/poll/fd_unix.go 在开始正式看源码之前需要具备一些基础知识，如同步、异步、阻塞、非阻塞、io多路复用、go调度模型等，不了解的话可以参考下面的链接： 同步异步阻塞非阻塞 io多路复用select poll epoll goroutine实现原理 也可以网上自行搜索，文章很多 ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:1:0","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"源码分析（v1.10.2） golang通过对epoll的封装来取得使用同步编程达异步执行的效果。总结来说，所有的网络操作都以网络描述符netFD为中心实现，netFD通过将Sysfd与pollDesc结构绑定，当在一个netFD上读写遇到EAGAIN错误时，就将当前goroutine存储到这个netFD对应的pollDesc中，同时将goroutine给park住，直到这个netFD上再次发生读写事件，才将此goroutine设置为ready放入待运行队列等待重新运行，在底层通知goroutine再次发生读写等事件的方式就是靠的epoll事件驱动机制。 ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:0","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"netFD 服务端通过Listen方法返回的Listener接口的实现和通过listener的Accept方法返回的Conn接口的实现都包含一个网络文件描述符netFD，netFD中包含一个poll.FD数据结构，而poll.FD中包含两个重要的数据结构Sysfd和pollDesc，前者是真正的系统文件描述符，后者对是底层事件驱动的封装，所有的读写超时等操作都是通过调用后者的对应方法实现的。 服务端的netFD在listen时会创建epoll的实例，并将listenFD加入epoll的事件队列 netFD在accept时将返回的connFD也加入epoll的事件队列 netFD在读写时出现syscall.EAGAIN错误，通过pollDesc将当前的goroutine park住，直到ready，从pollDesc的waitRead中返回 涉及到的一些结构： type TCPListener struct { fd *netFD } type conn struct { fd *netFD } // Network file descriptor. type netFD struct { pfd poll.FD // immutable until Close family int sotype int isConnected bool net string laddr Addr raddr Addr } // FD is a file descriptor. The net and os packages use this type as a // field of a larger type representing a network connection or OS file. type FD struct { // Lock sysfd and serialize access to Read and Write methods. fdmu fdMutex // System file descriptor. Immutable until Close. // 系统文件描述符 Sysfd int // I/O poller. pd pollDesc // Writev cache. iovecs *[]syscall.Iovec // Semaphore signaled when file is closed. csema uint32 // Whether this is a streaming descriptor, as opposed to a // packet-based descriptor like a UDP socket. Immutable. IsStream bool // Whether a zero byte read indicates EOF. This is false for a // message based socket connection. ZeroReadIsEOF bool // Whether this is a file rather than a network socket. isFile bool // Whether this file has been set to blocking mode. isBlocking bool } ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:1","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"pollDesc 上面提到的net.conn的读写等操作实际上就是调用的poll.FD对应的方法，poll.FD中包含一个重要结构poll.pollDesc，其定义如下： type pollDesc struct { runtimeCtx uintptr } 可以看到其中只包含一个指针，这个指针具体代表的其实是另一个同名不同包的结构runtime.pollDesc，定义如下： // Network poller descriptor. // // No heap pointers. // //go:notinheap type pollDesc struct { link *pollDesc // in pollcache, protected by pollcache.lock // The lock protects pollOpen, pollSetDeadline, pollUnblock and deadlineimpl operations. // This fully covers seq, rt and wt variables. fd is constant throughout the PollDesc lifetime. // pollReset, pollWait, pollWaitCanceled and runtime·netpollready (IO readiness notification) // proceed w/o taking the lock. So closing, rg, rd, wg and wd are manipulated // in a lock-free way by all operations. // NOTE(dvyukov): the following code uses uintptr to store *g (rg/wg), // that will blow up when GC starts moving objects. lock mutex // protects the following fields fd uintptr closing bool seq uintptr // protects from stale timers and ready notifications rg uintptr // pdReady, pdWait, G waiting for read or nil rt timer // read deadline timer (set if rt.f != nil) rd int64 // read deadline wg uintptr // pdReady, pdWait, G waiting for write or nil wt timer // write deadline timer wd int64 // write deadline user uint32 // user settable cookie } runtime.pollDesc包含自身类型的一个指针，用来保存下一个runtime.pollDesc的地址，go中有很多类似的实现，用来实现链表，可以减少数据结构的大小，所有的runtime.pollDesc保存在runtime.pollCache结构中，定义如下： type pollCache struct { lock mutex first *pollDesc // PollDesc objects must be type-stable, // because we can get ready notification from epoll/kqueue // after the descriptor is closed/reused. // Stale notifications are detected using seq variable, // seq is incremented when deadlines are changed or descriptor is reused. } 以tcp连接为例，分析一下Listen和Accept调用过程： ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:2","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"Listen func Listen(network, address string) (Listener, error) { addrs, err := DefaultResolver.resolveAddrList(context.Background(), \"listen\", network, address, nil) if err != nil { return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: nil, Err: err} } var l Listener switch la := addrs.first(isIPv4).(type) { case *TCPAddr: l, err = ListenTCP(network, la) case *UnixAddr: l, err = ListenUnix(network, la) default: return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: la, Err: \u0026AddrError{Err: \"unexpected address type\", Addr: address}} } if err != nil { return nil, err // l is non-nil interface containing nil pointer } return l, nil } // ListenTCP acts like Listen for TCP networks. // // The network must be a TCP network name; see func Dial for details. // // If the IP field of laddr is nil or an unspecified IP address, // ListenTCP listens on all available unicast and anycast IP addresses // of the local system. // If the Port field of laddr is 0, a port number is automatically // chosen. func ListenTCP(network string, laddr *TCPAddr) (*TCPListener, error) { switch network { case \"tcp\", \"tcp4\", \"tcp6\": default: return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: laddr.opAddr(), Err: UnknownNetworkError(network)} } if laddr == nil { laddr = \u0026TCPAddr{} } ln, err := listenTCP(context.Background(), network, laddr) if err != nil { return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: laddr.opAddr(), Err: err} } return ln, nil } 当我们调用net.Listen(“tcp”,addr)时，根据address类型会命中ListenTCP函数去执行，ListenTCP函数很简单，基本处理逻辑都在listenTCP函数中，往下看 func listenTCP(ctx context.Context, network string, laddr *TCPAddr) (*TCPListener, error) { fd, err := internetSocket(ctx, network, laddr, nil, syscall.SOCK_STREAM, 0, \"listen\") if err != nil { return nil, err } return \u0026TCPListener{fd}, nil } func internetSocket(ctx context.Context, net string, laddr, raddr sockaddr, sotype, proto int, mode string) (fd *netFD, err error) { if (runtime.GOOS == \"windows\" || runtime.GOOS == \"openbsd\" || runtime.GOOS == \"nacl\") \u0026\u0026 mode == \"dial\" \u0026\u0026 raddr.isWildcard() { raddr = raddr.toLocal(net) } family, ipv6only := favoriteAddrFamily(net, laddr, raddr, mode) return socket(ctx, net, family, sotype, proto, ipv6only, laddr, raddr) } listenTCP函数，在此函数中终于见到了期望看到的fd字眼，跳到internetSocket函数，可以看到最终的fd是由socket函数产生的，继续 // socket returns a network file descriptor that is ready for // asynchronous I/O using the network poller. func socket(ctx context.Context, net string, family, sotype, proto int, ipv6only bool, laddr, raddr sockaddr) (fd *netFD, err error) { s, err := sysSocket(family, sotype, proto) if err != nil { return nil, err } if err = setDefaultSockopts(s, family, sotype, ipv6only); err != nil { poll.CloseFunc(s) return nil, err } if fd, err = newFD(s, family, sotype, net); err != nil { poll.CloseFunc(s) return nil, err } // This function makes a network file descriptor for the // following applications: // // - An endpoint holder that opens a passive stream // connection, known as a stream listener // // - An endpoint holder that opens a destination-unspecific // datagram connection, known as a datagram listener // // - An endpoint holder that opens an active stream or a // destination-specific datagram connection, known as a // dialer // // - An endpoint holder that opens the other connection, such // as talking to the protocol stack inside the kernel // // For stream and datagram listeners, they will only require // named sockets, so we can assume that it's just a request // from stream or datagram listeners when laddr is not nil but // raddr is nil. Otherwise we assume it's just for dialers or // the other connection holders. if laddr != nil \u0026\u0026 raddr == nil { switch sotype { case syscall.SOCK_STREAM, syscall.SOCK_SEQPACKET: if err := fd.listenStream(laddr, listenerBacklog); err != nil { fd.Close() return nil, err } return fd, nil case syscall.SOCK_DGRAM: if err := fd.listenDatagram(laddr); err != nil { fd.Close() return nil, err } ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:3","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"Accept TCPListner有两个暴露出来的Accept相关的函数，分别为Accept和AcceptTCP，这里主要从AcceptTCP分析，因为后者被tcpKeepAliveListener的Accept函数调用，而tcpKeepAliveListener的Accept方法就是常用的建立web项目时，http.ListenAndServe中会用到的Accept方法，如下： func (l *TCPListener) AcceptTCP() (*TCPConn, error) { if !l.ok() { return nil, syscall.EINVAL } c, err := l.accept() if err != nil { return nil, \u0026OpError{Op: \"accept\", Net: l.fd.net, Source: nil, Addr: l.fd.laddr, Err: err} } return c, nil } 可以看到实现逻辑基本都在accept方法内 func (ln *TCPListener) accept() (*TCPConn, error) { fd, err := ln.fd.accept() if err != nil { return nil, err } return newTCPConn(fd), nil } func (fd *netFD) accept() (netfd *netFD, err error) { d, rsa, errcall, err := fd.pfd.Accept() if err != nil { if errcall != \"\" { err = wrapSyscallError(errcall, err) } return nil, err } if netfd, err = newFD(d, fd.family, fd.sotype, fd.net); err != nil { poll.CloseFunc(d) return nil, err } if err = netfd.init(); err != nil { fd.Close() return nil, err } lsa, _ := syscall.Getsockname(netfd.pfd.Sysfd) netfd.setAddr(netfd.addrFunc()(lsa), netfd.addrFunc()(rsa)) return netfd, nil } // Accept wraps the accept network call. func (fd *FD) Accept() (int, syscall.Sockaddr, string, error) { if err := fd.readLock(); err != nil { return -1, nil, \"\", err } defer fd.readUnlock() if err := fd.pd.prepareRead(fd.isFile); err != nil { return -1, nil, \"\", err } for { s, rsa, errcall, err := accept(fd.Sysfd) if err == nil { return s, rsa, \"\", err } switch err { case syscall.EAGAIN: if fd.pd.pollable() { if err = fd.pd.waitRead(fd.isFile); err == nil { continue } } case syscall.ECONNABORTED: // This means that a socket on the listen // queue was closed before we Accept()ed it; // it's a silly error, so try again. continue } return -1, nil, errcall, err } } 这里有两个函数比较重要，一个是accept，一个是fd.pd.waitRead，首先看accept的实现，最终还是会通过汇编进行系统调用。 // Wrapper around the accept system call that marks the returned file // descriptor as nonblocking and close-on-exec. func accept(s int) (int, syscall.Sockaddr, string, error) { ns, sa, err := Accept4Func(s, syscall.SOCK_NONBLOCK|syscall.SOCK_CLOEXEC) // On Linux the accept4 system call was introduced in 2.6.28 // kernel and on FreeBSD it was introduced in 10 kernel. If we // get an ENOSYS error on both Linux and FreeBSD, or EINVAL // error on Linux, fall back to using accept. switch err { case nil: return ns, sa, \"\", nil default: // errors other than the ones listed return -1, sa, \"accept4\", err case syscall.ENOSYS: // syscall missing case syscall.EINVAL: // some Linux use this instead of ENOSYS case syscall.EACCES: // some Linux use this instead of ENOSYS case syscall.EFAULT: // some Linux use this instead of ENOSYS } // See ../syscall/exec_unix.go for description of ForkLock. // It is probably okay to hold the lock across syscall.Accept // because we have put fd.sysfd into non-blocking mode. // However, a call to the File method will put it back into // blocking mode. We can't take that risk, so no use of ForkLock here. ns, sa, err = AcceptFunc(s) if err == nil { syscall.CloseOnExec(ns) } if err != nil { return -1, nil, \"accept\", err } if err = syscall.SetNonblock(ns, true); err != nil { CloseFunc(ns) return -1, nil, \"setnonblock\", err } return ns, sa, \"\", nil } // Accept4Func is used to hook the accept4 call. var Accept4Func func(int, int) (int, syscall.Sockaddr, error) = syscall.Accept4 func Accept4(fd int, flags int) (nfd int, sa Sockaddr, err error) { var rsa RawSockaddrAny var len _Socklen = SizeofSockaddrAny nfd, err = accept4(fd, \u0026rsa, \u0026len, flags) if err != nil { return } if len \u003e SizeofSockaddrAny { panic(\"RawSockaddrAny too small\") } sa, err = anyToSockaddr(\u0026rsa) if err != nil { Close(nfd) nfd = 0 } return } // THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT func accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error) { r0, _, e1 := Syscall6(SYS_ACCEPT4, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), uintptr(flags), 0, 0) fd = int(r0) if e1 != 0 { err = err","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:4","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"总结 通过上面的简单分析，我们大致可以理解整个流程，里面还涉及到很多具体细节以及go调度器相关的内容无法一一介绍，有兴趣的话可以直接查看源码。剩下的read，write大致相同，这里不再分析，最终都是通过netpoll的相关函数实现的，可以说整个核心实现都在netpoll.go这个文件中，外面只是进行了一些封装和状态的处理，至于G状态的变化的相关代码，可以自行搜索go调度器，已经有相当多博客进行过讲解了。 其实我们可以看到，知识就是个圈，缺少哪一块都串不起来，让我们一起努力，填补我们所缺失的部分吧，加油！ ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:3:0","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["问题排查"],"content":"kubernetes's bug because of the resourceversion","date":"2018-05-26","objectID":"https://www.likakuli.com/posts/kubernetes-ep-bug/","tags":["kubernetes"],"title":"Kubernetes惊天地泣鬼神之大bug","uri":"https://www.likakuli.com/posts/kubernetes-ep-bug/"},{"categories":["问题排查"],"content":"最近docker one的交流群里发出了一篇文章，Kubernetes 惊天地泣鬼神之大Bug ，估计很多人看完文章的反应和我一样，心中万马奔腾，自己的集群会不会也有这个问题 ？？？ 文中对于bug的影响和产生的原因已经描述的很清楚了，但看完之后我有一点疑问，文中所说的复现条件( 陆续创建、删除、创建 Kubernetes service 对象，然后\"kubectl delete svc xxx\"删掉创建时间靠前的 service，也就是往 service event list 末尾插入了一条 resourceVersion 比较小的记录，这将使得 controller-manager 去从已经爬过的 service event list 位置重新爬取重放，然后就重放了 service 的 ADDED、DELETED event，于是 controller-manager 内存里缓存的 service 对象被删除，导致 EndpointController 删除了“不存在的”service 对应的 endpoints。)在日常操作中其实会经常出现，那岂不是很多集群都会存在这个问题，而且这个bug影响这么严重，为什么现在才被报出来？基于上述疑问，本文主要是到源码中一探究竟，源码版本1.9.2，按照文中指引可以快速定位到问题代码所在位置，话不多说，直接上代码，如下： // watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh \u003c-chan struct{}) error { start := r.clock.Now() eventCount := 0 // Stopping the watcher should be idempotent and if we return from this function there's no way // we're coming back in with the same watch interface. defer w.Stop() // update metrics defer func() { r.metrics.numberOfItemsInWatch.Observe(float64(eventCount)) r.metrics.watchDuration.Observe(time.Since(start).Seconds()) }() loop: for { select { case \u003c-stopCh: return errorStopRequested case err := \u003c-errc: return err case event, ok := \u003c-w.ResultChan(): if !ok { break loop } if event.Type == watch.Error { return apierrs.FromObject(event.Object) } if e, a := r.expectedType, reflect.TypeOf(event.Object); e != nil \u0026\u0026 e != a { utilruntime.HandleError(fmt.Errorf(\"%s: expected type %v, but watch event object had type %v\", r.name, e, a)) continue } meta, err := meta.Accessor(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) continue } newResourceVersion := meta.GetResourceVersion() switch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) eventCount++ } } watchDuration := r.clock.Now().Sub(start) if watchDuration \u003c 1*time.Second \u0026\u0026 eventCount == 0 { r.metrics.numberOfShortWatches.Inc() return fmt.Errorf(\"very short watch: %s: Unexpected watch close - watch lasted less than a second and no items received\", r.name) } glog.V(4).Infof(\"%s: Watch close - %v total %v items received\", r.name, r.expectedType, eventCount) return nil } 这是文中指出的具体BUG所在，用错误的newResourceVersion去给resourceVersion和lastSyncResourceVersion赋了值，所以接下来就是要找这两个属性在什么地方被用到了，经过逐步查找代码引用，最后确定lastSyncResourceVersion和这个bug无关，那问题就出在了resourceVersion上，可以看到上面的方法中resourceVersion是通过指针传进来的，也就是说会影响到外面使用到这个属性的地方，继续查看watchHandler被调用的代码，如下： // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { glog.V(3).Infof(\"Listing and watching %v from %s\", r.expectedType, r.name) var resourceVersion string // Explicitly set \"0\" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1.ListOptions{Resou","date":"2018-05-26","objectID":"https://www.likakuli.com/posts/kubernetes-ep-bug/:0:0","tags":["kubernetes"],"title":"Kubernetes惊天地泣鬼神之大bug","uri":"https://www.likakuli.com/posts/kubernetes-ep-bug/"},{"categories":["使用说明"],"content":"kubernetes deploy with CA","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"简介 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书，操作系统CentOS7 amd64； 集群节点 10.202.43.132 master \u0026 etcd 10.202.43 133 master \u0026 node \u0026 etcd 10.202.43.134 node \u0026 etcd 认证请求 ca-csr.json etcd-csr.json admin-csr.json kubernetes-csr.json kube-proxy-csr.json 生成的 CA 证书和秘钥文件如下： ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem etcd.pem etcd-key.pem 证书使用情况如下： etcd：使用 ca.pem、etcd-key.pem、etcd.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem； kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:1:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"安装 CFSSL 这里采用二进制源码包安装，当然也可以使用go命令安装 cd /usr/local/bin wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:2:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"创建 CA (Certificate Authority) 创建 CA 配置文件 mkdir /opt/ssl cd /opt/ssl # ca-config.json 文件 vim ca-config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } 字段说明 ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书，生成的 ca.pem 证书中 CA=TRUE； server auth：表示client可以用该 CA 对server提供的证书进行验证； client auth：表示server可以用该 CA 对client提供的证书进行验证； 创建 CA 证书签名请求 # ca-csr.json 文件 vim ca-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 字段说明 “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成 CA 证书和私钥 cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls ca.csr ca-key.pem ca.pem ca-csr.json ca-config.json 分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 10.202.43.133:/etc/kubernetes/ssl/ scp *.csr 10.202.43.133:/etc/kubernetes/ssl/ scp *.pem 10.202.43.134:/etc/kubernetes/ssl/ scp *.csr 10.202.43.134:/etc/kubernetes/ssl/ ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:3:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"创建 etcd 证书 etcd 证书这里，我做测试时用的etcd地址如下 10.202.43.132 10.202.43.133 10.202.43.134 实际配置的 IP 还请根据实际情况修改，也可以多预留几个IP，以备后续添加能通过认证，不需要重新签发 vim etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"10.202.43.132\", \"10.202.43.133\", \"10.202.43.134\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 生成 etcd 证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 10.202.43.133:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 10.202.43.134:/etc/kubernetes/ssl/ 修改 etcd 配置 # etcd-1 vim /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.202.43.132:2380 \\ --listen-peer-urls=https://10.202.43.132:2380 \\ --listen-client-urls=https://10.202.43.132:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.202.43.132:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.202.43.132:2380,etcd2=https://10.202.43.133:2380,etcd3=https://10.202.43.134:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 另外两个节点上的配置和上面大致相同，只需要把对应IP换成自己的就可以了 证书相关的配置 --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem --peer-cert-file=/etc/kubernetes/ssl/etcd.pem --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem --trusted-ca-file=/etc/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem 分别启动 etcd 集群 systemctl daemon-reload systemctl start etcd # 配置开机启动 systemctl enable etcd # 查看etcd状态 systemctl status etcd 验证 etcd 集群状态 etcdctl --endpoints=https://10.202.43.132:2379,https://10.202.43.133:2379,https://10.202.43.134:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:4:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"创建 kubernetes 证书 创建 admin 证书和私钥 # 创建证书签名请求 vim admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin # 查看生成 ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 10.202.43.133:/etc/kubernetes/ssl/ 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权； kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限； OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限； ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kubectl 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes 创建 kubernetes 证书和私钥 # 创建证书签名请求 vim kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.202.43.132\", \"10.202.43.133\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes # 查看生成 ls kubernetes* kubernetes.csr kubernetes-key.pem kubernetes.pem kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 10.202.43.133:/etc/kubernetes/ssl/ 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 10.202.43.132 和 10.202.43.133 为 Master 的 IP ，多个Master需要写多个，也可以多写几个，方便以后扩展master节点； 10.254.0.1 为 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个 IP，如 10.254.0.1 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:1","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。有关 TLS Bootstrapping 参考 这里 # 生成 token head -c 16 /dev/urandom | od -An -t x | tr -d ' ' f6280a3754345875d392258bd340ef7e # 创建 token.csv 文件 cd /opt/ssl vim token.csv f6280a3754345875d392258bd340ef7e,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 10.202.43.133:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 10.202.43.133:/etc/kubernetes/ vim /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=10.202.43.132 \\ --allow-privileged=true \\ --apiserver-count=2 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://10.202.43.132:2379,https://10.202.43.133:2379,https://10.202.43.134:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=10000-60000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 证书相关配置 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction # 1.8开始需要添加 NodeRestriction --audit-policy-file=/etc/kubernetes/audit-policy.yaml --authorization-mode=Node,RBAC #1.8 开始需要添加 Node --client-ca-file=/etc/kubernetes/ssl/ca.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/etcd.pem --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem --kubelet-https=true --insecure-bind-address=127.0.0.1 #不是0.0.0.0 仅供kube-controller-manager 和 kube-scheduler 使用 --insecure-port=8080 --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --enable-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:2","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vim /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 证书相关配置 --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:3","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vim /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:4","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap 配置 kubelet.kubeconfig # 配置集群，server参数为apiserver地址，可以使用haproxy或nginx来代替直接使用ip,达到k8s多主的目的 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.202.43.132:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=f6280a3754345875d392258bd340ef7e \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ 配置 kubelet.service # 创建 kubelet 目录 mkdir /var/lib/kubelet vim /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --pod-infra-container-image=repository.gridsum.com:8443/kaku/pause-adm64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.210.250 \\ --cluster_domain=cluster.local. \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target 证书相关配置 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl 配置 TLS 认证 # 查看 csr 的名称 kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-*********************************** 1m kubelet-bootstrap Pending # 增加 认证 kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 这里注意如果 csr 被删除了，请删除如下文件，并重启 kubelet 服务 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:5","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-proxy 创建 kube-proxy 证书和私钥 vim kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy* /etc/kubernetes/ssl/ scp kube-proxy* 10.202.43.133:/etc/kubernetes/ssl/ scp kube-proxy* 10.202.43.134:/etc/kubernetes/ssl/ CN 指定该证书的 User 为 system:kube-proxy； kube-apiserver 预定义的 RoleBindings cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； 配置 kube-proxy.kubeconfig # 配置集群, server参数为apiserver地址，可以使用haproxy或nginx来代替直接使用ip,达到k8s多主的目的 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.202.43.132:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 10.202.43.133:/etc/kubernetes/ scp kube-proxy.kubeconfig 10.202.43.134:/etc/kubernetes/ 配置 kube-proxy.service 1.9 官方 ipvs 已经 beta , 尝试开启 ipvs 测试一下, 官方 –feature-gates=SupportIPVSProxyMode=false 默认是 false 的， 需要设置 –-feature-gates=SupportIPVSProxyMode=true –-masquerade-all。执行 yum install ipvsadm -y 安装 ipvs，ipvs 和 calico 不兼容，原因之一是 calico 必须不能设置 –-masquerade-all # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vim /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=10.202.43.133 \\ --hostname-override=10.202.43.133 \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 配置说明 # --bind-address 和 --hostname-override 按需设置， 其中 --bind-address 为 kube-proxy 所在节点的 IP 证书相关配置 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:6","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 CoreDNS 配置 coredns.yaml # 下载配置文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vim coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/16 { pods insecure upstream /etc/resolv.conf fallthrough in-addr.arpa ip6.arpa } ... image: repository.gridsum.com:8443/library/coredns ... clusterIP: 10.254.210.250 # 创建 coredns kubectl apply -f coredns.yaml ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:7","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"dashboard 官方 dashboard dashboard都从1.7之后只支持https访问，按照官网使用kubectl proxy启动本地代理来访问。此处必须为本地代理，即必须通过http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/访问，如果要在windows上访问的话，可以自己编译一个kubectl对应的windows客户端，通过令牌登陆的时候，能看到的内容与token的权限有关。 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:6:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"apiserver访问 客户端在集群外 # 在任一 master 节点执行瑞安命令便可在集群外通过http://masterip:8001访问api server kubectl proxy --address 0.0.0.0 --accept-hosts '.*' --port=8001 集群内访问 # kubectl get secret NAME TYPE DATA AGE default-token-z6lq7 kubernetes.io/service-account-token 3 15d # kubectl describe secret default-token-z6lq7 Name: default-token-z6lq7 Namespace: default Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name=default kubernetes.io/service-account.uid=c246a42f-3d73-11e8-aa36-00155d010a3a Type: kubernetes.io/service-account-token Data ==== ca.crt: 1310 bytes namespace: 7 bytes token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tejZscTciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyNDZhNDJmLTNkNzMtMTFlOC1hYTM2LTAwMTU1ZDAxMGEzYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Qfi9881mVeX8Z4ophf6_l5e5jGcILFobS5mKTQgdoz6NF0rJ4buRASmQiqu4N5ErolOstSIJhhK1yVzHbkBYsYReip6ffTnOwF2cWU5EJAhP7_o2zGWK5b11amlp5qLU0rWucfYe34ZfGxAcxoekmgKwJ6Hu58JlgD3ae5lu-_J6yVT_O-klC6FUXCY-r3FbtwwYz7WbrSxhuu5nCbegmEy5gPy9aEeVZcz6v5ZIyTU62mvbO_M1xYQfPyaHPWgjbkh9H540j2LUa7Y7RQw_LLEp_NbpzVfN58sFLY8cndzUfr5v_KjtFXyPVqmUX0qxoIRWL2opB7Qr2lL7qyTg5w #上述secret会挂载到所有的pod上，容器内对应路径为/var/run/secrets/kubernetes.io/serviceaccount #集群内的Pod通过在http请求的header中添加Authorization: Bearer ${KUBE_BEARER_TOKEN} 进行访问 # kubectl exec -it crawler-bgapi-new-9bjc7 sh # cd /var/run/secrets/kubernetes.io/serviceaccount # ls ca.crt namespace token # cat token eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tejZscTciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyNDZhNDJmLTNkNzMtMTFlOC1hYTM2LTAwMTU1ZDAxMGEzYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Qfi9881mVeX8Z4ophf6_l5e5jGcILFobS5mKTQgdoz6NF0rJ4buRASmQiqu4N5ErolOstSIJhhK1yVzHbkBYsYReip6ffTnOwF2cWU5EJAhP7_o2zGWK5b11amlp5qLU0rWucfYe34ZfGxAcxoekmgKwJ6Hu58JlgD3ae5lu-_J6yVT_O-klC6FUXCY-r3FbtwwYz7WbrSxhuu5nCbegmEy5gPy9aEeVZcz6v5ZIyTU62mvbO_M1xYQfPyaHPWgjbkh9H540j2LUa7Y7RQw_LLEp_NbpzVfN58sFLY8cndzUfr5v_KjtFXyPVqmUX0qxoIRWL2opB7Qr2lL7qyTg5 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:7:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"参考 Generate self-signed certificates Setting up a Certificate Authority and Creating TLS Certificates Client Certificates V/s Server Certificates 数字证书及 CA 的扫盲介绍 kubernetes 1.9.1 kubernetes安装之证书认证 管理集群中的TLS ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:8:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"}]