[{"categories":["问题排查"],"content":"问题描述 开启特权模式（--privileged）的容器，在使用nvidia GPU时，无法通过cAdvisor获取GPU相关的metrics信息。Google大法可以搜到相关的Issue，于2018年提出，至今仍处于Open状态（给cAdvisor贡献代码的机会），由于涉及到的内容较多，分为三篇来讲。 接上一篇，在上一篇中我们已经清楚cAdvisor是如何获取容器所使用的GPU卡信息的，也清楚了为什么在容器开启特权模式时cAdvisor无法获取其所使用的的GPU卡信息。但距离给出有效且优雅的解决方案还有很多待探索的，例如devices.list内容是如何来的？如何知道应该为容器绑定哪些GPU卡等，这些问题都将会在接下来的内容中得到答案，让我们一步一步进行分析。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:1:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"抽丝剥茧 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"devics.lists内容从何而来 上面提到过三个device cgroup的重要文件，其中devices.allow和devices.deny为只写文件，devices.list为只读文件。device子系统是通过device whitelist实现的，此处涉及到内核的知识，想深入了解其实现的可以翻一下内核的代码。简单理解就是通过前面两个只写的文件对whitelist做设置，往devices.allow中添加条目相当于添加白名单，往devices.deny中添加条目相当于删除白名单，最后通过devices.list获取白名单内容。那接下来的问题就是devices.allow和devices.deny内容是谁根据什么规则写入的？ ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:1","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"devices.allow和devices.deny内容谁负责写入 nvidia-container-runtime-webhook对应的github项目为 nvidia-container-toolkit。 这里内容比较多，涉及到kubelet、device-plugin、 containerd、 runC、nvidia-container-runtime、nvidia-container-runtime-hook等。在继续之前先回顾一下docker的进程模型，了解了进程模型及各组件的作用之后定位问题会更有针对性，更容易理解下面的内容。 进程模型回顾 相信上图大家已经非常熟悉了，其中Docker Engine通过rpc与containerd通信，containerd通过rpc与containerd-shim 通信，最终containerd-shim通过runC来控制容器的生命周期。提到runC就不得不提OCI（Open Container Initiative）。OCI 的来源这里不多说，感兴趣的可以自行搜索。其主要包含了两种规范，即runtime-spec和image-spce，而 runC就是对runtime-spec的一种实现。既然是规范，那就会存在多种实现，nvidia-container-runtime就是为了支持容器使用GPU而做的另一种runtime-spec的实现，原理如下 右侧从runC开始的一整块就是nvidia-container-runtime。 runC实现原理 runC容器创建原理 一个容器启动主要分为三大部分： create: 主要是为了解析、组装容器启动的配置和与子进程的消息通道等； init : 主要根据容器配置启动容器整个运行环境，包括熟知ns，cgroups, seccomp, apparmor, caps等; start : 主要是为了通知init 进程启动容器； 大致流程如下图 nvidia-container-runtime原理 nvidia-container-runtime原理是利用runtime-spec规范中提到的PreStart Hook机制在执行完runc start之后，在真正的用户进程启动之前，执行nviadia-container-runtime-hook进行一些操作，其针对GPU设备的挂载和device cgroup的设置是通过一个由C和C++实现的叫做nvidia-container-cli（libnvidia-container）的程序实现的。 注意：prestart hook的生效时机有待确认，虽然OCI spec中提到是在start之后，但是经过看runc源码发现是在执行init的过程中，且尚未执行start nvidia-container-runtime-hook通过解析runtime-spec中为用户程序设置的**NVIDIA_VISIBLE_DEVICES**环境变量获取容器需要使用的GPU卡的ID，并调用nvidia-container-cli configure --device=ID(s) 来完成上述提到的操作。 至此可以对容器内的GPU设备挂载和device cgroup的设置过程和原理有个大概的了解，这里利用了OCI hook和spec，通过环境变量形式把需要的GPU设备ID设置到名为NVIDIA_VISIBLE_DEVICES的环境变量上，进行了比较巧妙的实现，那又是谁为此环境变量设置赋值的呢？继续往下看 NVIDIA_VISIBLE_DEVICES来自何处 Kubernetes 提供了一个 device plugin框架（beta from 1.10），可以用它来将系统硬件资源发布到Kubelet，再由Kubelet上报给Kube-apiserver，不必定制 Kubernetes 本身的代码。目标设备包括 GPU、高性能 NIC、FPGA、 InfiniBand 适配器以及其他类似的、可能需要特定于供应商的初始化和设置的计算资源。 工作流程如上图所示，device plugin先注册到Kubelet，Kubelet开始watchdevice plugin变化，在容器真正创建之前Kubelet调用device plugin提供的Allocate服务去为Pod申请所需资源。Kubelet在内部维护了一份Pod与扩展资源的映射数据，且通过checkpoint形式写到本地文件中，在后续Kubelet重启时会用到。同时在开启KubeletPodResource特性开关后，Kubulet还可以通过rpc的形式对外提供pod-resources信息，详情可以参考https://kubernetes.io/zh/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/。 这里大致可以猜到赋值的地方了，因为这是一个专门用于GPU的环境变量，所以不可能直接在Kubelet中定义，那就只剩下nidia对应的k8s-device-plugin了，我们可以在其源码中找到对应的实现逻辑，如下： // Constants to represent the various device list strategies const ( DeviceListStrategyEnvvar = \"envvar\" DeviceListStrategyVolumeMounts = \"volume-mounts\" ) // migStrategyNone func (s *migStrategyNone) GetPlugins() []*NvidiaDevicePlugin { return []*NvidiaDevicePlugin{ NewNvidiaDevicePlugin( \"nvidia.com/gpu\", NewGpuDeviceManager(false), // Enumerate device even if MIG enabled \"NVIDIA_VISIBLE_DEVICES\", gpuallocator.NewBestEffortPolicy(), pluginapi.DevicePluginPath+\"nvidia-gpu.sock\"), } } // Allocate which return list of devices. func (m *NvidiaDevicePlugin) Allocate(ctx context.Context, reqs *pluginapi.AllocateRequest) (*pluginapi.AllocateResponse, error) { responses := pluginapi.AllocateResponse{} for _, req := range reqs.ContainerRequests { for _, id := range req.DevicesIDs { if !m.deviceExists(id) { return nil, fmt.Errorf(\"invalid allocation request for '%s': unknown device: %s\", m.resourceName, id) } } response := pluginapi.ContainerAllocateResponse{} uuids := req.DevicesIDs deviceIDs := m.deviceIDsFromUUIDs(uuids) // 默认为DeviceListStrategyEnvvar，m.deviceListEnvvar的值就是NVIDIA_VISIBLE_DEVICES if deviceListStrategyFlag == DeviceListStrategyEnvvar { response.Envs = m.apiEnvs(m.deviceListEnvvar, deviceIDs) } if deviceListStrategyFlag == DeviceListStrategyVolumeMounts { response.Envs = m.apiEnvs(m.deviceListEnvvar, []string{deviceListAsVolumeMountsContainerPathRoot}) response.Mounts = m.apiMounts(deviceIDs) } if passDeviceSpecsFlag { response.Devices = m.apiDeviceSpecs(nvidiaDriverRootFlag, uuids) } responses.ContainerResponses = append(responses.ContainerResponses, \u0026response) } return \u0026responses, nil } 可以看到在具体是在Kubelet调用Allocate服务时，k8s-device-plugin对response.Env进行了设置，添加了对应的环境变量，值就是容器所对应的GPU设备，多个设备用逗号分隔。 为什么开启特权模式的容器devices.list是*:* 同样是设置了NVIDIA_VISIBLE_DEVICES，同样最终都是nvidia-container-cli进行device cgroup的设置，为什么特权模式的容","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:2","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"完整流程 至此我们清楚了容器在使用GPU时的整个流程，从容器创建到真正挂载GPU设备以及开启特权模式后无法获取GPU指标的原因。 好了，有关cAdvisor无法提供特权模式容器的GPU指标的原理及原因至此已经都搞清楚了，下一篇我们讲介绍解决方案，敬请期待~ ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display2/:2:3","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display2/"},{"categories":["问题排查"],"content":"问题描述 开启特权模式（--privileged）的容器，在使用nvidia GPU时，无法通过cAdvisor获取GPU相关的metrics信息。Google大法可以搜到相关的Issue，于2018年提出，至今仍处于Open状态（给cAdvisor贡献代码的机会），由于涉及到的内容较多，分为三篇来讲。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display/:1:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display/"},{"categories":["问题排查"],"content":"寻踪觅源 问题的最终表现是通过cAdvisor无法获取开启特权模式容器的gpu相关数据，即 curl localhost:4194/api/v1.3/docker/{containerID} 返回的结果中不包含任何gpu信息，而没有开特权模式的容器是可以正常返回gpu相关信息的。 未开启特权模式的容器返回结果类似如下： root@node1: # curl localhost:4194/api/v1.3/docker/a0f3c54d8c6beba5d9947723494e4c9b625f03ce4eb0c096e4cf46cae9e389e0 | jq [.[]][0].stats[0].accelerators [ { \"make\": \"nvidia\", \"model\": \"Tesla T4\", \"id\": \"GPU-a2d26dd1-5a8f-35e5-c325-2cd0756d5ed2\", \"memory_total\": 15843721216, \"memory_used\": 15127085056, \"duty_cycle\": 6 }, { \"make\": \"nvidia\", \"model\": \"Tesla T4\", \"id\": \"GPU-febe3f91-914e-cb7f-bf0a-d96ffeda2777\", \"memory_total\": 15843721216, \"memory_used\": 11367940096, \"duty_cycle\": 44 } ] 可以看到容器用到了两块GPU卡。 开启特权模式的容器执行上述命令后返回空信息。 ","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display/:2:0","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display/"},{"categories":["问题排查"],"content":"cAdvisor 首先需要了解的就是cAdvisor获取gpu指标信息的原理，可以查看官方文档，简单翻译一下如下： cAdvisor可以对外暴露容器级别的硬件加速器的指标，且当前只支持英伟达GPU，不支持整机级别 cAdvisor只对在容器启动时显示设置了--device /dev/nvidia0:/dev/nvidia0信息的容器暴露指标，未显示指定的不暴露（对应容器启动时设置了--privileged参数） 通过介绍可以得出如下结论：无法获取开启特权模式容器的GPU指标是Feature而不是Bug 同时文档中最后提到如果cAdvisor容器化部署时如何设置参数，其中提到的三种方法，如下： cAdvisor容器以--privileged模式启动 Docker v17.04.0-ce 及以上版本的话，启动时设置--device-cgroup-rule 'c 195:* mrw'参数 启动时设置--device /dev/nvidiactl:/dev/nvidiactl /dev/nvidia0:/dev/nvidia0 /dev/nvidia1:/dev/nvidia1 \u003cand-so-on-for-all-nvidia-devices\u003e参数 现实是无论容器是否开启特权模式，我们都需要去获取容器级别的GPU使用率指标，那怎么办呢？大致有两种方法：修改cAdvisor使其支持（In-Tree）、添加其他组件来提供GPU使用率指标（Out-Of-Tree），无论哪种方式，我们都有必要先搞清楚如何采集GPU使用率指标，可以从cAdvisor下手，看他是如何采集的。 采集机制 在看具体实现之前，首先介绍一下cAdvisor运行原理，如下图 cAdvisor在采集过程中主要分两种数据： 容器数据 指标数据 容器数据来源 通过watch cgroup下文件目录的变化进行对应容器的处理，此处只获取到容器ID，在获取到新增容器时，通过containerHandler根据容器ID获取容器详情，例如判断出来watch到的容器是通过docker创建的，则会调用docker API获取指定ID的容器详情，其中就包含了容器启动时需要的Env信息，后面的解决方案中会用到这个属性。 指标数据来源 每个容器都有与之对应的collector来进行指标采集，其中nvidia指标由对应的nvidiaCollector负责采集。 源码分析 明白了原理之后，就可以有针对性的去源码中看其具体实现逻辑了。代码位置很好找，在项目根目录下有个accelerators文件夹，源码位于nvidia.go文件内。 // GetCollector returns a collector that can fetch NVIDIA gpu metrics for NVIDIA devices // present in the devices.list file in the given devicesCgroupPath. func (nm *nvidiaManager) GetCollector(devicesCgroupPath string) (stats.Collector, error) { nc := \u0026nvidiaCollector{} if !nm.devicesPresent { return \u0026stats.NoopCollector{}, nil } // Makes sure that we don't call initializeNVML() concurrently and // that we only call initializeNVML() when it's not initialized. nm.Lock() if !nm.nvmlInitialized { err := initializeNVML(nm) if err != nil { nm.Unlock() return \u0026stats.NoopCollector{}, err } } nm.Unlock() if len(nm.nvidiaDevices) == 0 { return \u0026stats.NoopCollector{}, nil } // 从cgroup中获取需要容器使用的nvidia设备信息，返回的是gpu序号数组 nvidiaMinorNumbers, err := parseDevicesCgroup(devicesCgroupPath) if err != nil { return \u0026stats.NoopCollector{}, err } for _, minor := range nvidiaMinorNumbers { device, ok := nm.nvidiaDevices[minor] if !ok { return \u0026stats.NoopCollector{}, fmt.Errorf(\"NVIDIA device minor number %d not found in cached devices\", minor) } nc.devices = append(nc.devices, device) } return nc, nil } 逻辑比较简单，先初始化NVML（基于C实现的API，用来监控和管理nvidia设备的状态信息，通过nvidia-smi对外暴露其各种能力。），接着调用parseDevicesCgroup函数获取容器自身所使用的GPU的序号，最后通过返回的设备序号找到使用的设备信息返回。 // parseDevicesCgroup解析device cgroup下的devices.list文件来获取允许被容器访问的GPU设备的minor号 // 如果容器可以访问所有设备或者所有nvidia设备的话，但这些设备并未在devices.list中，则返回一个空数组 var parseDevicesCgroup = func(devicesCgroupPath string) ([]int, error) { // Always return a non-nil slice nvidiaMinorNumbers := []int{} devicesList := filepath.Join(devicesCgroupPath, \"devices.list\") f, err := os.Open(devicesList) if err != nil { return nvidiaMinorNumbers, fmt.Errorf(\"error while opening devices cgroup file %q: %v\", devicesList, err) } defer f.Close() s := bufio.NewScanner(f) // See https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt for the file format for s.Scan() { text := s.Text() fields := strings.Fields(text) if len(fields) != 3 { return nvidiaMinorNumbers, fmt.Errorf(\"invalid devices cgroup entry %q: must contain three whitespace-separated fields\", text) } // Split the second field to find out major:minor numbers majorMinor := strings.Split(fields[1], \":\") if len(majorMinor) != 2 { return nvidiaMinorNumbers, fmt.Errorf(\"invalid devices cgroup entry %q: second field should have one colon\", text) } // NVIDIA graphics devices are character devices with major number 195. // https://github.com/torvalds/linux/blob/v4.13/Documentation/admin-guide/devices.txt#L2583 if fields[0] == \"c\" \u0026\u0026 majorMinor[0] == \"195\" { minorNumber, err := strconv.Atoi(majorMinor[1]) if err != nil { return nvidiaMinorNumbers, fmt.Errorf(\"invalid devices cgroup entry %q: minor number is not integer\", text) } // We don't want devices like nvidiactl (195:255) and nvidia-modeset (195:254) if minorNumber \u003c 128 { nvidiaMinorNumbers = append(nvidiaMinorNumbers, minorNumber) } // We are ignoring the \"195:*\" case","date":"2021-05-30","objectID":"https://www.likakuli.com/posts/gpu-metrics-not-display/:2:1","tags":["docker","gpu","cadvisor"],"title":"容器开启特权模式后无法通过cadvisor获取GPU metrics指标","uri":"https://www.likakuli.com/posts/gpu-metrics-not-display/"},{"categories":["使用说明"],"content":"通用Restful API项目模板 欢迎使用，这是一个用Go编写的简单通用的Restful API项目，遵循SOLID原则。 部分灵感来自于 service-pattern-go ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:0","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"依赖 Gin Gorm Testify (Test \u0026 Mock framework) Mockery (Mock generator) Hystrix-Go (Circuit Breaker) ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:1","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"开始 安装 介绍 目录结构 Mocking Tesing 能力支持 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:2","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"安装 克隆项目代码 git clone https://github.com/likakuli/generic-project-template.git 启动mysql服务并初始化数据库 docker run --name=mysql -it -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d registry.cn-beijing.aliyuncs.com/likakuli/mysql 注意：如果是在MacOS上使用Docker for Mac启动的容器，则需要安装 docker-connector ，否则无法在本机通过容器IP访问容器，原因参考这里。安装命令如下 # 安装 docker-connector brew install wenjunxiao/brew/docker-connector # 把 docker 的所有 bridge 网络都添加到路由中 docker network ls --filter driver=bridge --format \"{{.ID}}\" | xargs docker network inspect --format \"route {{range .IPAM.Config}}{{.Subnet}}{{end}}\" \u003e\u003e /usr/local/etc/docker-connector.conf # 启动服务 sudo brew services start docker-connector # 在 docker 端运行 wenjunxiao/mac-docker-connector，需要使用 host 网络，并且允许 NET_ADMIN docker run -it -d --restart always --net host --cap-add NET_ADMIN --name connector wenjunxiao/mac-docker-connector 镜像涉及到的Dockerfile与sql放置在docker文件夹下 运行单元测试 make test 编译程序 make build 运行程序 # 替换配置文件MySQL connectionString ./docker/replace_ip.sh # 启动程序 ./generic-project-template --config=./conf/config.toml --log_dir=./logs --v=1 访问程序 curl http://localhost:8080/api/v1/score/Lucy/vs/Lily ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:3","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"介绍 这是一个简单通用的Restful API项目，内置依赖注入、Mocking等功能，旨在方便快速的编写安全可靠的Restful API代码。不同的数据结构之间通过接口来访问，避免直接引用具体的实现，这样就可以实现依赖注入及采用Mock结构进行单元测试的效果。 举例来说： IPlayerServie --\u003e IPlayerRepository type PlayerController struct { service interfaces.IPlayerService } type playerService struct { repo interfaces.IPlayerRepository } ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:4","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"目录结构 /cmd /- apiserver /conf /pkg /- config /- controllers /- interfaces /- models /- repositories /- server /- middlewares |- container.go |- router.go |- server.go /- services /- viewmodels controllers 控制器文件夹下包含所有Gin Route Handler，里面只包含处理Request和Response的逻辑，不包含任何业务逻辑和数据访问逻辑。仅依赖于interfaces下的IService接口，不依赖于具体实现。 interafces 接口文件夹下存放所有IService和IRepository接口定义及通过Mockery自动生成的用于单元测试的文件，不包含具体接口实现。 models 模型文件下下存放所有与数据库映射的实体模型对应的Go Struct，只包含数据结构，不包含数据访问逻辑。可以由 gen 根据数据库表结构自动生成，详情参考这里 repositories 仓库文件夹下存放所有数据库访问逻辑，且实现了interfaces下定义的IRepository接口，主要用到models文件夹下定义的实体结构。 services 服务文件夹下存放所有实现了services下定义的IService接口的逻辑，供controllers直接使用。其中涉及到的数据库访问部分均通过调用interfaces下的IRepository接口实现，不依赖任何具体实现。 viewmodels 视图模型文件夹下存放所有需要与API交互的实体，主要包含从API获取到的结构和返回值的结构。与models的区别在于前者对应api层，后者对应数据库层。 router 路由文件夹下包含了所有可以对外提供服务的Restful API的路由注册逻辑。 container 容器文件下包含了所有依赖注入需要的Provider的逻辑，且在此选择具体使用的接口实现类型。 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:5","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"Mocking 为方便进行单元测试，使用Mockery自动interfaces下接口实现，例如生成IPlayerService的实现，只需要进入interfaces文件夹下执行如下命令即可，最后会在interfaces下自动创建mocks文件夹来存放自动生成的文件。 mockery -name=IPlayerService 需要提前安装mokery工具 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:6","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"Testing 有了依赖注入和Mock功能后，就可以针对任意接口实现编写单元测试了，示例中添加了针对serviceshecontrollers的单测，供参考。 ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:7","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["使用说明"],"content":"能力支持 Tracing PProf Prometheus Metrics Health Check Mock Testing Circuit Breaker Rate Limit Common go-utils … ","date":"2021-05-11","objectID":"https://www.likakuli.com/posts/generic-project-template/:1:8","tags":["Golang","Resetful API"],"title":"通用Restful API项目模板","uri":"https://www.likakuli.com/posts/generic-project-template/"},{"categories":["开源项目介绍"],"content":"CRUD还能这么玩？ 作为一名有多年经验的CRUD工程师，在做某项目时，需要在搭建完apiserver框架以及dao部分，三下五除二的就把apiserver部分搞定了，然后到了dao部分。用屁股想了想网上肯定有现成的工具可以自动生成go struct，于是在网上找了找，果不其然，很多，但基本都是根据table来自动生成go struct的，于是搞定了创建table的sql，创建完table之后顺利的用工具生成了go struct。你以为这就完了吗。当然没有，因为在此过程中我发现了一个BUG级的项目：https://github.com/smallnest/gen，点开作者主页一看，原来又是一位自己关注已久的博主：鸟窝，肃然起敬。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:1:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"项目介绍 这个项目能干什么呢？简单来说就是他可以根据你的数据库自动生成go struct及dao层、api层，直接生成一套可运行的restful api项目，而且还搭配了swagger文档。抛开最终生成的代码不说，单就这个思想就已经够我们学习了。 接下来简单介绍下这个项目，为啥简单介绍呢，因为使用起来确实很简单，而且其文档说明比较完善。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"安装 项目采用golang编写，生成的也是golang代码，安装方式自然也是golang项目的安装方式 go get -u github.com/smallnest/gen ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:1","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"使用 在项目代码的example文件夹下存在一个sample.db文件，可以直接根据这个文件来生成完成的项目，当然也支持自定义配置来生成自己需要的部分。 ## 根据sample.db生成项目代码 $ gen --sqltype=sqlite3 \\ --connstr \"./sample.db\" \\ --database main \\ --json \\ --gorm \\ --guregu \\ --rest \\ --out ./example \\ --module example.com/rest/example \\ --mod \\ --server \\ --makefile \\ --json-fmt=snake \\ --generate-dao \\ --generate-proj \\ --overwrite ## 编译 (使用make命令进行编译，packr2会被自动安装) $ cd ./example $ make example ## 二级制位置./bin/example $ cp ../../sample.db . $ ./example ## 打开浏览器访问这里 http://127.0.0.1:8080/swagger/index.html ## 同样可以使用命令行工具进行访问 curl http://localhost:8080/artists 可能会报错，提示找不到某些文件或者包，安装即可，部分文件判断方式是直接检查GOPATH下是否存在，所以会出现使用go mod模式时get下来之后仍然报错的情况，可以直接到GOPATH下下载对应的项目即可。 是不是so easy，只需要提供数据库即可生成完整项目，但是他的功能不止于此。因为每个人有自己的习惯的编码风格，自动生成的代码不可能满足所有人的需求，那怎么办呢？ ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:2","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"高级功能 gen支持自定义模板，可以把原模板导出，基于其修改，或者干脆按照自己的风格制作一套模板，通过--templateDir=参数指定自己的模板的路径即可 如果只想进行简单改动，可以尝试--exec=参数，允许开发者自定义代码生成规则，在项目自带的custom文件夹下有sample.gen文件，可以直接指定--exec=sample.gen看执行效果，如果使用的是最新的master，则执行会报错，看代码可以发现在2020/8/4的一次修改，改变了某些方法的形参，而sample.gen没有进行对应修改，按需修改即可。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:2:3","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"原理 项目用到的主要的技术就是golang的template功能，包括template的常用能力及定制函数等功能。另外包括GIN框架、GORM框架等crud工程师们常用的包。里面还有一个平时不怎么用的包packr，其主要功能就是把静态文件嵌入到最终生成的二进制文件中，程序就可以在任何位置直接运行而不需要受制于额外静态文件的位置等因素。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:3:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["开源项目介绍"],"content":"结束语 被CRUD折磨的人啊，是否有考虑过将其自动化呢，虽然业务逻辑是在变化的，但项目框架部分是基本保持不变的，搞一套自己习惯的开箱即用框架岂不是更美，毕竟从头写的话也需要一点时间，懒果然是人类进步的阶梯，赶紧去下载项目试一试吧。 ","date":"2021-04-29","objectID":"https://www.likakuli.com/posts/crud/:4:0","tags":["gen"],"title":"CRUD工程师的福音","uri":"https://www.likakuli.com/posts/crud/"},{"categories":["思考感悟"],"content":"近期有个小需求，在不重建Container的前提下修改Pod结构中的Request值，限制仅可以调小。本以为很简单的一个需求，但实际花费了一天的时间才搞完，代码改动只有几行，但是在改完测试的过程中发现很多超出预期或者认知的现象，为了搞懂为什么会这样，又重新捋了捋kubelet源码。 在这件事结束之后也进行了反思，主要是有关源码阅读的，于是把这个过程和自己的感触以及后续的一些改进方法和计划记录下来。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:0:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"过程 先看下这件事的过程，我们先忽略这个需求的合理性，直接分析技术实现。 首先，kubernetes本身并不支持修改Pod的资源属性，无论Request还是Limit，可以通过修改apiserver中的校验逻辑来放开此限制； 其次，如何保证在Request改变之后容器不重启？我们知道，kubelet会为每个container都计算出一个hash值，其中用到了container的所有属性，在调用docker api进行容器创建的时候会把这个值设置到容器的Label中，后续如果kubelet检测到新计算出的hash值与在运行的容器的hash值不同，则会进行容器的原地重启操作，这也是为什么修改container的Image会出发容器原地重启的原因。很明显，如果放开Request的修改，Request值变了之后也会导致新的hash值变化从而导致容器重建，与我们的期望不符。也有办法来解决：记录container创建时的Request值，计算的时候还是使用创建时的值，此值只有在container创建时会记录，后续不再更新。 测试场景是创建了Qos类型为Guaranteed 类型的Pod，放开了kube-apiserver对Request修改的限制，kubelet保持原生不动，调小其Request值，大家可以先尝试自己思考一下会发生什么？ 那么在测试的时候遇到了什么问题呢？ 首先，发现放开Request修改之后，如果改了Request的值，容器重启了（这一步符合预期），但是重启次数加2（这里其实是之前的一个盲点） 接着，继续修改Request值，容器依然重启（符合预期），但是此次重启次数只加了1 最后，通过查看Pod Cgroup目录，确认修改后的Request已经在Pod级别和Container级别分别生效，但是同时存在两个Pod的目录，类似如下 # 修改之前的Pod对应的cgroup目录 /sys/fs/cgroup/cpu/kubepods/guaranteed/pod{uid} # 修改之后的Pod对应的cgroup目录 /sys/fs/cgroup/cpu/kubepods/burstable/pod{uid} 为什么同一个Pod会存在两个cgroup目录呢？ 容器重启了，重启次数应该只加1，那为什么在第一步中加了2？ 你可以在继续阅读之前先自己思考一下可能的原因。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:1:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"问题分析 首先看为什么会有两个cgroup目录，需要先搞清楚cgroup目录是如何创建、如何删除的。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:2:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"Cgorup创建 我们采用CgroupPerQos的方式进行管理，以cpu子系统为例，层级类似如下所示 /sys/fs/cgroup/cpu kubepods guaranteed pod{uid} {containerid} {containerid} pod{uid} {containerid} {containerid} burstable bestaffort 从创建者的角度分两种：kubelet创建的、docker创建的。其中container层由docker创建，container以上的pod层、qos层和root（kubepods）都是由kubelet创建的。那docker又是怎么知道容器的cgroup parent目录是谁呢？其实是kubelet在调用docker api时传给docker的一个参数，告诉了其cgroup parent路径，可以通过执行docker inspect {containerid} | grep -i cgroup来查看每个container的cgroup parent路径。 那为什么会在两个qos目录下分别存在一个Pod目录呢？因为我们修改了Pod的Qos类型，触发了syncPod逻辑，里面会去根据Pod的qos类型进行cgroup目录判断，如果qos改变，则会把原Pod下的所有container全部杀掉，然后创建新的cgroup目录，再启动容器。这也就可以解释为什么在第一次修改Request之后Pod重启次数增加了2，因为pause容器也发生了重建。为什么要重建容器呢，因为整个pod的qos发生了变化，Pod内的所有容器需要在新的qos目录下重建其目录，但是kubelet没有去更新container的cgroup设置，而是采用重建的方式来实现。 为什么kubelet不直接去更新cgroup目录，而是重建容器呢？首先修改Request不仅影响cgroup，容器的oomscore也将受到影响，docker虽然提供了api来修改资源大小，但并没有提供相关的api去进行cgroup目录及oomscore等属性的修改，其次cgroup迁移是一个比较复杂的工作，迁移过程会出现部分历史数据丢失等问题，所以kubele直接采用重建的方式来解决这个问题。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:2:1","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"Cgroup删除 经过分析Cgroup创建过程，重启两次的问题已经找到了答案。但为什么新的Pod cgroup目录创建出来之后，原有的目录没有被删除呢？这就需要搞清楚Pod Cgroup目录什么时候删除的，容器级别的cgroup目录是在容器被删除的时候删除的，这个很好理解，Pod级别的Cgroup目录是否也是在Pod删除时删除的呢？经过看代码发现并不是，Pod资源清理是一个异步的过程，定时监测Pod是否已经设置了deletionTimestamp属性和容器的运行状态，只有设置了此属性的Pod才有可能被清理，清理的过程中包含挂在卷、Cgroup等资源，会一并清理。因为修改Request的请求是不会去给Pod设置deletionTimestamp属性的，这就导致Pod级别的旧目录不会被删除，又因为新目录的创建，导致同时存在两个Pod级别的目录。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:2:2","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"反思 综合看下来，这两个点都没有那么难，而且之前也做过kubelet定制开发，syncPod部分代码更是看过数次。那为什么花费了这么久的时间呢？ ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:0","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"源码阅读的目的性 此前阅读源码的目的有几种，查问题、验证某些想法、探寻系统运行原理，还有一些人通过看源码来写blog、或者写源码分析之类的书。此前大部分场景是为了查问题、验证想法以及某些小功能的定制开发，可以聚焦到某些点或流程上，做完之后会对相关点印象比较深刻，但是相关性不大的地方就会很容易遗忘，或者说根本不会去刻意关注相关性不大的地方。偶尔会有想法去主动阅读源码，探求系统运行原理，实现方式，写blog等，但最终发现效果很差，因为在此过程中我们的目的性并不是真正的去理解系统，缺乏针对性。而且对于在还不了解系统运行原理的情况下想通过看源码去了解其原理就是本末倒置的事情。比如接手一个新项目或者其他人的项目的时候，如果没有相关背景，而且也没有项目文档，没有代码注释的情况，直接想通过代码去了解业务和系统运行原理是一件非常痛苦的事情。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:1","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"思考的必要性 无论处于什么目的去看代码，需要有自己的思考，可以假设系统由自己设计，那会设计成什么样子，代码由自己实现，会写成什么样子。在设计过程中可能会遇到一些问题，带着问题再去看代码，去验证别人是如何设计并实现的，尤其是遇到和自己预期设计不一致的地方，可以进行对比，分析那种方案好，或者他这么设计是处于什么考虑，为什么这么实现。以这样的方式看代码要比没有目的性的走马观花式的浏览代码收获更多，印象更深刻。这里推荐一本书《思考，快与慢》，解释了人的大脑是如何工作的，可以通过本书了解到思考是一个怎样的过程。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:2","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":["思考感悟"],"content":"后续计划 源码还是要去读的，后面会进行一些尝试，根据上面提到的阅读方式开始进行，即 思考系统运行方式 ==》自己设计系统实现 ==》带着问题读源码（验证想法） ==》思考与总结，试运行一段时间看看效果。 ","date":"2021-04-18","objectID":"https://www.likakuli.com/posts/sourcecodereading/:3:3","tags":["思考"],"title":"关于阅读源码的一些思考","uri":"https://www.likakuli.com/posts/sourcecodereading/"},{"categories":[],"content":"Hi，我是 Kaku 李鹤，目前就职于快手。幼儿园小班工程师，终身学习者。 2016年开始从事容器云PAAS平台相关的研发工作，从0到1搭建了公司级容器容器云平台。 2018年开始主要研究大规模集群调度和管理，2019年开始负责滴滴弹性云平台基础架构方向的工作，包括大规模集群支撑、管理运维、调度优化等工作，同时对线上问题排查与处理也有一定的踩坑经验。 目前主要关注领域在大规模容器集群管理与调度、利用ebpf技术实现运行状态的可视化。 欢迎扫描下方二维码关注微信公众号IT散修进行订阅。也可以通过微信公众号留言同作者进行交流或者直接通过微信公众号加作者微信进行交流。 ","date":"2021-04-15","objectID":"https://www.likakuli.com/about/:0:0","tags":[],"title":"关于作者","uri":"https://www.likakuli.com/about/"},{"categories":["源码分析"],"content":"开局一张图，剩下全靠编 详细内容参考： Informer DeltaFIFO Informer LocalStore 未完待续 ","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer/:0:0","tags":["informer","kubernetes"],"title":"Kubernetes Informer","uri":"https://www.likakuli.com/posts/kubernetes-informer/"},{"categories":["源码分析"],"content":"按照惯例，先上图，cache对应Informer中的Local Store。 直接撸源码（基于1.12版本，有删减，不影响主要逻辑） // 代码源自client-go/tools/cache/store.go type Store interface { Add(obj interface{}) error Update(obj interface{}) error Delete(obj interface{}) error List() []interface{} ListKeys() []string Get(obj interface{}) (item interface{}, exists bool, err error) GetByKey(key string) (item interface{}, exists bool, err error) // Replace will delete the contents of the store, using instead the // given list. Store takes ownership of the list, you should not reference // it after calling this function. // 用传入的内容替换原有内容 Replace([]interface{}, string) error Resync() error } Store为最基本的存储接口，提供增删改查基本功能，要求对象有唯一键，键的计算方式由接口的具体实现决定，很好理解。 // 代码源自client-go/tools/cache/index.go // Indexer is a storage interface that lets you list objects using multiple indexing functions type Indexer interface { Store // Retrieve list of objects that match on the named indexing function Index(indexName string, obj interface{}) ([]interface{}, error) // IndexKeys returns the set of keys that match on the named indexing function. IndexKeys(indexName, indexKey string) ([]string, error) // ListIndexFuncValues returns the list of generated values of an Index func ListIndexFuncValues(indexName string) []string // ByIndex lists object that match on the named indexing function with the exact key ByIndex(indexNtame, indexKey string) ([]interface{}, error) // GetIndexer return the indexers GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error } Indexer在Store基础上扩展了索引能力，就好比给数据库添加的索引，以便查询更快，那么肯定需要有个结构来保存索引 // 代码源自client-go/tools/cache/index.go // IndexFunc knows how to provide an indexed value for an object. type IndexFunc func(obj interface{}) ([]string, error) // Index maps the indexed value to a set of keys in the store that match on that value type Index map[string]sets.String //sets.String为map[string]struct{}类型，减少内存占用 // Indexers maps a name to a IndexFunc type Indexers map[string]IndexFunc // Indices maps a name to an Index type Indices map[string]Index 到这里是不是一脸懵逼？？！！，光看注释根本不知道这是要干啥。经过搜索发现client-go里只用到了一个IndexFunc，即MetaNamespaceIndexFunc，对应的IndexName为namespace，所以，一般情况下Indexers总是这个值{“namespace”:MetaNamespaceIndexFunc}，举个栗子： 假如我要从Store中获取Key为default/test-app的Pod信息，那么上面的三个map中的内容分别为 Indexers: {“namespace”:MetaNamespaceIndexFunc} Index:{“default”:{“default/test-app”:}} Indices: {“namespace”:{“default”:{“default/test-app”:}}} 如果还是不懂，就先往下看，看Indexer的具体实现 // 代码源自client-go/tools/cache/store.go type cache struct { // cacheStorage bears the burden of thread safety for the cache cacheStorage ThreadSafeStore // keyFunc is used to make the key for objects stored in and retrieved from items, and // should be deterministic. keyFunc KeyFunc } // KeyFunc knows how to make a key from an object. Implementations should be deterministic. type KeyFunc func(obj interface{}) (string, error) // 代码源自client-go/tools/cache/thread_safe_store.go type ThreadSafeStore interface { Add(key string, obj interface{}) Update(key string, obj interface{}) Delete(key string) Get(key string) (item interface{}, exists bool) List() []interface{} ListKeys() []string Replace(map[string]interface{}, string) Index(indexName string, obj interface{}) ([]interface{}, error) IndexKeys(indexName, indexKey string) ([]string, error) ListIndexFuncValues(name string) []string ByIndex(indexName, indexKey string) ([]interface{}, error) GetIndexers() Indexers // AddIndexers adds more indexers to this store. If you call this after you already have data // in the store, the results are undefined. AddIndexers(newIndexers Indexers) error Resync() error } // threadSafeMap implements ThreadSafeStore type threadSafeMap struct { lock sync.RWMutex items map[string]interface{} // indexers maps a name to an IndexFunc indexers Indexers // indices maps a name to an Index indices Indices } 上面代码很好理解，cache主要由线程安全的map[string]interface{}实现，Go的","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer-localstore/:0:0","tags":["informer","kubernetes"],"title":"Informer LocalStore源码解析","uri":"https://www.likakuli.com/posts/kubernetes-informer-localstore/"},{"categories":["源码分析"],"content":"总结 会不会有人有疑问，所有的对象都存在一个索引键下面，这样的效率岂不是太低了?其实client-go为每类对象都创建了Informer(Informer内有Indexer)，所以存储在相同索引建下的对象都是同一类，这个问题自然也就没有了。cache就类似数据库的Table，而且带了索引功能，可以试想一下自己想要实现一个带索引功能的缓存的话，怎么设计数据结构 对象如何存储？每个对象都有唯一键，通过KeyFunc计算出来，不同对象的键不重复，可以用map实现，对应上面的ThreadSafeMap 索引策略有哪些？用map[string]Func实现，key代表该索引策略名字，Func为策略，传入对象，返回对象在该策略下的索引值，比如key为namespace，Func(obj)的返回值就是该obj的namespace，如default，这里要注意，Func应该返回数组，例如如果策略为Label，一个对象的Label是有多个的，索引值就会有多个，对应上面的Indexers 索引值和对象的关系如何存储？一个索引值可能回应多个对象，如按照Label索引，需要一个map来存储，key为索引的值，值为对象集合，但是因为对象有唯一键，所以值可以用对象的唯一键的集合来减少内存消耗，但是这种数据结构在查询某个对象是否满足指定的索引值时需要遍历值的集合，很简单吗，值也用map来寸就可以了，key为对象的键，值为空结构体，即值得类型为map[string]struct{}，对应sets.String，整个数据结构对应上面的Index 索引策略和索引值的关系如何存储？很简单，一个map就搞定了，key为策略名字，value为Index，对应上面的Indices 上面基本可以实现功能了，还有一个可以考虑的问题：计算对象唯一键的方法、索引相关的三个数据结构放在哪里，k8s的实现里，KeyFunc是调用者和cache都知道具体的算法，索引相关的数据结构都放在了threadSafeMap里。 ","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer-localstore/:1:0","tags":["informer","kubernetes"],"title":"Informer LocalStore源码解析","uri":"https://www.likakuli.com/posts/kubernetes-informer-localstore/"},{"categories":["源码分析"],"content":"DeltaFIFO位于Reflector和LocalStore之间，看名字知道是个先进先出的队列，作用就是缓存数据变化，直接看代码 // 代码源自client-go/tools/cache/delta_fifo.go type DeltaFIFO struct { // lock/cond protects access to 'items' and 'queue'. // 读写锁，性能要比Mutex好 lock sync.RWMutex // 供pop函数使用，没有对象的时候可以阻塞 cond sync.Cond // We depend on the property that items in the set are in // the queue and vice versa, and that all Deltas in this // map have at least one Delta. // 注意map值类型为Deltas，DeltaFIFO生产者和消费者是异步的，如果同一个目标的频繁操作，前面操作还缓存在队列中的时候，那么队列就要缓冲对象的所有操作 // queue里存的是map的keys，用map实现高速查询，用slice保证有序 items map[string]Deltas queue []string // 下面两个属性用来判断是否已完成同步， // populated is true if the first batch of items inserted by Replace() has been populated // or Delete/Add/Update was called first. populated bool // initialPopulationCount is the number of items inserted by the first call of Replace() initialPopulationCount int // keyFunc is used to make the key used for queued item // insertion and retrieval, and should be deterministic. keyFunc KeyFunc // knownObjects list keys that are \"known\", for the // purpose of figuring out which items have been deleted // when Replace() or Delete() is called. // 这里其实就是Indexer knownObjects KeyListerGetter // Indication the queue is closed. // Used to indicate a queue is closed so a control loop can exit when a queue is empty. // Currently, not used to gate any of CRED operations. closed bool closedLock sync.Mutex } // 代码源自client-go/tools/cache/delta_fifo.go type Delta struct { Type DeltaType Object interface{} // 对象，也就是k8s里的资源 } type DeltaType string // Delta的类型用字符串表达 const ( Added DeltaType = \"Added\" // 增加 Updated DeltaType = \"Updated\" // 更新 Deleted DeltaType = \"Deleted\" // 删除 Sync DeltaType = \"Sync\" // 同步 ) type Deltas []Delta // Delta数组 Delta就是数据的变化，用字典存储达到更快的检索速度，而且Deltas是要求有序的，而字典的遍历是无序的，所以用slice保存key的集合，以达到先进先出效果。结合上图来看，DeltaFIFO就是一个用来存储kubernetes资源变化的先进先出的队列。DeltaFIFO队列功能是通过实现如下接口实现的 // 代码源自client-go/tools/cache/fifo.go type Queue interface { // 存储接口，再Indexer分析过了 Store // Pop blocks until it has something to process. // It returns the object that was process and the result of processing. // The PopProcessFunc may return an ErrRequeue{...} to indicate the item // should be requeued before releasing the lock on the queue. // pop在对列为空的时候会阻塞，就是通过上面提到的sync.Cond实现的 // 具体弹出的对象是通过传入的PopProcessFunc处理的，处理完返回错误后，可能会再入对列 Pop(PopProcessFunc) (interface{}, error) // AddIfNotPresent adds a value previously // returned by Pop back into the queue as long // as nothing else (presumably more recent) // has since been added. AddIfNotPresent(interface{}) error // HasSynced returns true if the first batch of items has been popped HasSynced() bool // Close queue Close() } 从这个接口可以看出，其实Queue也是一个Store，只不过添加了Pop的操作，可以将对象有序弹出，对比Indexer，Indexer是在Store基础上增加了索引，所以一个当做对列用，一个当做存储用。 接着看DeltaFIFO具体实现，还是比较简单的 // 代码源自client-go/tools/cache/delta_fifo.go // 添加对象接口 func (f *DeltaFIFO) Add(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true // 队列第一次写入操作都要设置标记 return f.queueActionLocked(Added, obj) } // 更新对象接口 func (f *DeltaFIFO) Update(obj interface{}) error { f.lock.Lock() defer f.lock.Unlock() f.populated = true // 队列第一次写入操作都要设置标记 return f.queueActionLocked(Updated, obj) } // 删除对象接口 func (f *DeltaFIFO) Delete(obj interface{}) error { id, err := f.KeyOf(obj) if err != nil { return KeyError{obj, err} } f.lock.Lock() defer f.lock.Unlock() f.populated = true // 队列第一次写入操作都要设置标记 // 此处是需要注意的，knownObjects就是Indexer，里面存有已知全部的对象 if f.knownObjects == nil { // 在没有Indexer的条件下只能通过自己存储的对象查一下 if _, exists := f.items[id]; !exists { return nil } } else { // 自己和Indexer里面有任何一个有这个对象多算存在 _, exists, err := f.knownObjects.GetByKey(id) _, itemsExist := f.items[id] if err == nil \u0026\u0026 !exists \u0026\u0026 !itemsExist { return nil } } return f.queueActionLocked(Deleted, obj) } // 列举对象键接口 func (f *DeltaFIFO) ListKeys() []string { f.lock.RLock() defer f.lock.RUnlock() list := make([]string, 0, len(f.items)) for key := range f.items { list = append(list, key) } return list } // 列举对象接口 func (f *DeltaFIFO) List() []","date":"2021-03-18","objectID":"https://www.likakuli.com/posts/kubernetes-informer-deltafifo/:0:0","tags":["informer","kubernetes"],"title":"Informer DeltaFIFO源码解析","uri":"https://www.likakuli.com/posts/kubernetes-informer-deltafifo/"},{"categories":["问题排查"],"content":"cgroup mount destination: unknown","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"问题 线上k8s集群在进行容器创建时报如下错误 Failed create pod sandbox: rpc error: code = Unknown desc = failed to start sandbox container for pod “xxx-sf-32c80-0”: Error response from daemon: cgroups: cannot find cgroup mount destination: unknown 之前遇到过cgroup相关问题，但是这个问题还是头一次见，网上搜索了关键字，社区有类似报错的issue，如cgroups: cannot found cgroup mount destination: unknown，联系最近做过的线上变更及问题，怀疑跟某自定义组件有关，详细背景参考这篇。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/:0:1","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"排查过程 光看问题云里雾里的，只知道和cgroup有关，登陆宿主查看此错误是kubelet请求docker时docker返回的，docker 18.06版本，没有更详细的日志了，但是开源的一个好处在于查问题的时候有源码，这大大降低了查问题的难度，直接去docker项目中搜索关键词，最终发现是在containerd的源码中，相关代码如下 // PidPath will return the correct cgroup paths for an existing process running inside a cgroup // This is commonly used for the Load function to restore an existing container func PidPath(pid int) Path { p := fmt.Sprintf(\"/proc/%d/cgroup\", pid) paths, err := parseCgroupFile(p) if err != nil { return errorPath(errors.Wrapf(err, \"parse cgroup file %s\", p)) } return existingPath(paths, \"\") } func existingPath(paths map[string]string, suffix string) Path { // localize the paths based on the root mount dest for nested cgroups for n, p := range paths { dest, err := getCgroupDestination(string(n)) if err != nil { return errorPath(err) } rel, err := filepath.Rel(dest, p) if err != nil { return errorPath(err) } if rel == \".\" { rel = dest } paths[n] = filepath.Join(\"/\", rel) } return func(name Name) (string, error) { root, ok := paths[string(name)] if !ok { if root, ok = paths[fmt.Sprintf(\"name=%s\", name)]; !ok { return \"\", fmt.Errorf(\"unable to find %q in controller set\", name) } } if suffix != \"\" { return filepath.Join(root, suffix), nil } return root, nil } } func getCgroupDestination(subsystem string) (string, error) { f, err := os.Open(\"/proc/self/mountinfo\") if err != nil { return \"\", err } defer f.Close() s := bufio.NewScanner(f) for s.Scan() { fields := strings.Split(s.Text(), \" \") if len(fields) \u003c 10 { // broken mountinfo? continue } if fields[len(fields)-3] != \"cgroup\" { continue } for _, opt := range strings.Split(fields[len(fields)-1], \",\") { if opt == subsystem { return fields[3], nil } } } if err := s.Err(); err != nil { return \"\", err } return \"\", ErrNoCgroupMountDestination } func parseCgroupFile(path string) (map[string]string, error) { f, err := os.Open(path) if err != nil { return nil, err } defer f.Close() return parseCgroupFromReader(f) } func parseCgroupFromReader(r io.Reader) (map[string]string, error) { var ( cgroups = make(map[string]string) s = bufio.NewScanner(r) ) for s.Scan() { if err := s.Err(); err != nil { return nil, err } var ( text = s.Text() parts = strings.SplitN(text, \":\", 3) ) if len(parts) \u003c 3 { return nil, fmt.Errorf(\"invalid cgroup entry: %q\", text) } for _, subs := range strings.Split(parts[1], \",\") { if subs != \"\" { cgroups[subs] = parts[2] } } } return cgroups, nil } 逻辑比较清晰，先从/proc/id/cgroup中解析得到所有的subsystem，对应上面parseCgroupFromReader函数，/proc/id/cgroup内容如下 先按冒号分隔每行字符串，然后取第2列，再根据逗号分隔得到所有的子系统，最终返回所有子系统。然后调用existingPath检查是否所有子系统都存在，内部又调用getCgroupDestination，最终的报错就是在这个函数里报出来的。 getCgroupDestination的逻辑是读取/proc/id/mountinfo信息，判断是否传入的子系统存在 先根据空格分隔，找到所有cgroup类型的目录，然后再根据逗号分隔遍历所有的子系统是否是传入的子系统。找不到的话就会报错，但是不得不吐槽的就是这个报错报的太没有诚意了，要是直接把找不到的子系统报出来，问题会直观很多。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/:0:2","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"结论 到此可以明白是agent隔离程序先mount了自定义目录cpu_mirror到cgroup目录下，然后影响到了java程序去获取正确的核数，为了修复特意执行了umount的操作，但是umount之后/proc/id/cgroup还是存在cpu_mirror相关信息而/proc/id/mountinfo中已经不存在了，在容器重新创建的时候进行检查进而报错。 对比线上其他docker版本，比如1.13.1中就没有此问题，因为1.13.1用的containerd中并没有上面提到的检验逻辑 通过这个问题也暴露出来我们在测试、灰度过程中的问题，由于线上环境复杂，系统版本众多、组件版本也不统一，在上线一个功能或者执行线上操作的时候，测试用例需要充分覆盖所有场景，灰度时也需要所有类型的机器至少都覆盖到了之后才可以放量继续靠扩大灰度范围，否则很容易出现类似的问题。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-cgroup-unknown/:0:3","tags":["linux","cgroup","docker"],"title":"cgroup mount destination: unknown","uri":"https://www.likakuli.com/posts/docker-cgroup-unknown/"},{"categories":["问题排查"],"content":"背景 容器原生设计为单进程模型，但公司线上运行的服务以多进程的方式运行，而且里面包含了很多的agent，例如日志采集、监控采集、数据配送等，耦合在了一个Container中，经过对线上资源使用率分析发现很大一部分资源消耗是在agent部分，而且与业务进程同时争抢业务容器申请的资源，彼此影响。虽然增量的容器部分agent迁移到了sidecar里面，解决了这些问题，但存量问题也需要解决，为此专门搞了一个项目用来优化这些问题。思想就是把agent进程从业务进程所在的cgroup中迁移出去，以不同cgroup层级存在，就可以避免相互影响，也可以限制各自资源大小，但是在灰度过程中发现部分Java容器服务开始出现毛刺。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-java-cpu/:0:1","tags":["docker","上云"],"title":"Java服务突现毛刺","uri":"https://www.likakuli.com/posts/docker-java-cpu/"},{"categories":["问题排查"],"content":"排查过程 java服务毛刺问题在最早上云的时候就出现过，当时是因为jdk版本太低，在容器内运行时无法正确获取容器申请的cpu大小，导致创建过多的线程，从而导致容器内的进程内部争抢过高，业务开始出现毛刺。在某个版本（jdk8u191）之后，开发者完全不需要关注程序是运行在物理机还是容器环境下。 对比有问题的容器内的业务进程使用的jdk版本，结果（202 \u003e 191）居然是没问题的版本，也就是已经可以自动识别cpu核数了，但为什么还是出现问题了呢？ 首先需要确定下容器内java服务获取到的真正的cpu核数，可以执行如下命令 java -XX:+UnlockDiagnosticVMOptions -XX:+PrintContainerInfo -version 输出结果中可以看到获取到的核数确实不是我们申请的核数，也就是说虽然采用了没问题的jdk版本，但还是获取到了错误的核数。 接下来就是看下为什么会获取到错误的核数信息，可以使用strace来分析java服务启动过程中的函数调用信息，其中在获取cpu核数的时候比较奇怪，正常是从cpu子系统获取，但是结果却显示从cpu_mirror获取，而这个目录下没有保存cpu核数的文件。后经过确定，这个目录就是前面提到的做agent隔离的时候自定义的挂在目录，容器内位于/sys/fs/cgroup下面。到此基本可以猜测是因为我们自己mount了一个目录到cgroup目录下，导致jdk获取到了错误目录，参数也获取错误。 最后，为什么jdk获取到了错误的路径呢？参考 Container Support doesn’t work for some Join Controllers combinations，代码比较好理解，其实就是有问题的版本在获取路径时采用的是字符串截取，而不是精确匹配，导致cpu_mirror这种目录会被误认为是正确目录。 老版处理/prof/self/mountinfo直接遍历每行，用strstr判断是否包含cpu、memory等，包含就是找到了，处理/proc/self/cgroup的时候根据 冒号 分隔，去掉第一列序号，然后剩下的再取第一列，然后用strstr比较是不是包含cpu、memory等 新版针对/proc/self/mountinfo读一条数据，最后一列根据 逗号 分隔，然后用strcmp比较是否和cpu、memory相等，处理 /proc/self/cgroup的时候根据 冒号 分隔，去掉第一列序号，然后剩下的再取第一列 ，然后根据逗号分隔，遍历结果用strcmp比较是否是cpu、memory等 总结就是老版本 直接if contains(str, “cpu”) then set cpu subsystme 新版本 for subStr in split(str, “,\") if subStr equals “cpu” then set cpu subsystem ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-java-cpu/:0:2","tags":["docker","上云"],"title":"Java服务突现毛刺","uri":"https://www.likakuli.com/posts/docker-java-cpu/"},{"categories":["问题排查"],"content":"解决方案 让业务升级jdk版本无法快速解决此问题，最终决定把挂载的cpu_mirror等问题目录umount掉，容器内看不到的话也就不会出问题了，经过测试发现可行，最终又在宿主上批量执行了umount的操作（为后面一个线上问题埋下了伏笔，详见这篇），长期方案是负责隔离的agent把问题目录改名，避免字符串匹配时匹配到。 ","date":"2021-02-10","objectID":"https://www.likakuli.com/posts/docker-java-cpu/:0:3","tags":["docker","上云"],"title":"Java服务突现毛刺","uri":"https://www.likakuli.com/posts/docker-java-cpu/"},{"categories":["问题排查"],"content":"现象 线上k8s集群报警，宿主fd利用率超过80%，登陆查看dockerd内存使用26G ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:1:0","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"排查思路 由于之前已经遇到过多次dockerd资源泄露的问题，先看是否是已知原因导致的，参考前面两篇 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:0","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"fd的对端是谁？ 执行ss -anp | grep dockerd，结果如下图，可以看到和之前遇到的问题不同，第8列显示为0，与之前遇到的的情况不符，无法找到对端。 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:1","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"内存为什么泄露？ 为了可以使用pprof分析内存泄露位置，首先为dockerd打开debug模式，需要修改service文件，添加如下两句 ExecReload=/bin/kill -s HUP $MAINPID KillMode=process 同时在/etc/docker/daemon.json文件中添加 “debug”: true的配置，修改完之后执行systemctl daemon-reload重新加载docker服务配置，然后执行systemctl reload docker，重新加载docker配置，开启debug模式 dockerd默认使用uds对外提供服务，为了方便我们调试，可以使用socat对docker进行端口转发，如下 sudo socat -d -d TCP-LISTEN:8080,fork,bind=0.0.0.0 UNIX:/var/run/docker.sock，意思是外部可以通过访问宿主机的8080端口来调用docker api，至此一切就绪 在本地执行go tool pprof http://ip:8080/debug/pprof/heap查看内存使用情况，如下图 可以看到占用多的地方在golang自带的bufio NewWriterSize和NewReaderSize处，每次http调用都会都这里，也看出来有什么问题。 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:2","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"Goroutine也泄露？ 泄露位置 通过内存还是无法知道具体出问题的位置，问题不大，再看看goroutine的情况，直接在浏览器访问http://ip:8080/debug/pprof/goroutine?debug=1，如下图 一共1572822个goroutine，两个大头各占一半，各有786212个。看到这里基本就可以沿着文件行数去源码中查看了，这里我们用的docker 18.09.2版本，把源码切换到对应版本下，通过查看源码可以知道这两大类的goroutine泄露的原因，dockerd与containerd相关处理流程如下图 对应上图的话，goroutine泄露是由上面最后docker kill时的wait chan close导致的，wait的时候会启动另一个goroutine，每次docker kill都会造成这两个goroutine的泄露。对应代码如下 // Kill forcefully terminates a container. func (daemon *Daemon) Kill(container *containerpkg.Container) error { if !container.IsRunning() { return errNotRunning(container.ID) } // 1. Send SIGKILL if err := daemon.killPossiblyDeadProcess(container, int(syscall.SIGKILL)); err != nil { // While normally we might \"return err\" here we're not going to // because if we can't stop the container by this point then // it's probably because it's already stopped. Meaning, between // the time of the IsRunning() call above and now it stopped. // Also, since the err return will be environment specific we can't // look for any particular (common) error that would indicate // that the process is already dead vs something else going wrong. // So, instead we'll give it up to 2 more seconds to complete and if // by that time the container is still running, then the error // we got is probably valid and so we return it to the caller. if isErrNoSuchProcess(err) { return nil } ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second) defer cancel() if status := \u003c-container.Wait(ctx, containerpkg.WaitConditionNotRunning); status.Err() != nil { return err } } // 2. Wait for the process to die, in last resort, try to kill the process directly if err := killProcessDirectly(container); err != nil { if isErrNoSuchProcess(err) { return nil } return err } // Wait for exit with no timeout. // Ignore returned status. \u003c-container.Wait(context.Background(), containerpkg.WaitConditionNotRunning) return nil } // Wait waits until the container is in a certain state indicated by the given // condition. A context must be used for cancelling the request, controlling // timeouts, and avoiding goroutine leaks. Wait must be called without holding // the state lock. Returns a channel from which the caller will receive the // result. If the container exited on its own, the result's Err() method will // be nil and its ExitCode() method will return the container's exit code, // otherwise, the results Err() method will return an error indicating why the // wait operation failed. func (s *State) Wait(ctx context.Context, condition WaitCondition) \u003c-chan StateStatus { s.Lock() defer s.Unlock() if condition == WaitConditionNotRunning \u0026\u0026 !s.Running { // Buffer so we can put it in the channel now. resultC := make(chan StateStatus, 1) // Send the current status. resultC \u003c- StateStatus{ exitCode: s.ExitCode(), err: s.Err(), } return resultC } // If we are waiting only for removal, the waitStop channel should // remain nil and block forever. var waitStop chan struct{} if condition \u003c WaitConditionRemoved { waitStop = s.waitStop } // Always wait for removal, just in case the container gets removed // while it is still in a \"created\" state, in which case it is never // actually stopped. waitRemove := s.waitRemove resultC := make(chan StateStatus) go func() { select { case \u003c-ctx.Done(): // Context timeout or cancellation. resultC \u003c- StateStatus{ exitCode: -1, err: ctx.Err(), } return case \u003c-waitStop: case \u003c-waitRemove: } s.Lock() result := StateStatus{ exitCode: s.ExitCode(), err: s.Err(), } s.Unlock() resultC \u003c- result }() return resultC } 对照goroutine的图片，两个goroutine分别走到了Kill最后一次的container.Wait处、Wait的select处，正因为Wait方法的select一直不返回，导致resultC无数据，外面也就无法从container.Wait返回的chan中读到数据，从而导致每次docker stop调用阻塞两个goroutine。 为什么泄露？ 为什么select一直不返回呢？可以看到select在等三个chan，任意一个有数据或者关闭都会返回 ctx.Done()：不返回是因为最后一次调用Wait的时候传入的是context.Background()。这里其实也是dockerd对请求的处理方式，既然客户端要删除容器，那我就等着容器删除，什么时间删除什么时间退出，只要容器没删，就一直有个goroutine在等待。 waitStop和waitRemove：不返回是因为没收到containerd发来的task exit的信号，可以对照上图看下，在收到task exit后才会关闭chan 为什么没收","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:2:3","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"总结 Kubelet为了保证最终一致性，发现宿主上还有不应该存在的容器就会一直不断的去尝试删除，每次删除都会调用docker stop的api，与dockerd建立一个uds连接，dockerd删除容器的时候会启动一个goroutine通过rpc形式调用containerd来删除容器并等待最终删除完毕才返回，等待的过程中会另起一个goroutine来获取结果，然而containerd在调用runc去真正执行删除的时候因为容器内D进程，无法删除容器，导致没有发出task exit信号，dockerd的两个相关的goroutine也就不会退出。整个过程不断重复，最终就导致fd、内存、goroutine一步步的泄露，系统逐渐走向不可用。 回过头来想想，其实kubelet本身的处理都没有问题，kubelet是为了确保一致性，要去删除不应该存在的容器，直到容器被彻底删除，每次调用docker api都设置了timeout。dockerd的逻辑有待商榷，至少可以做一些改进，因为客户端请求时带了timeout，且dockerd后端在接收到task exit事件后是会去做container remove操作的，即使当前没有docker stop请求。所以可以考虑把最后传入context.Background()的Wait函数调用去掉，当前面带超时的Wait返回后直接退出就可以，这样就不会造成资源泄露了。 ","date":"2020-12-24","objectID":"https://www.likakuli.com/posts/docker-leak3/:3:0","tags":["docker"],"title":"Dockerd资源泄露系列 - 3","uri":"https://www.likakuli.com/posts/docker-leak3/"},{"categories":["问题排查"],"content":"1. 背景 承接上文，近期我们排查弹性云线上几起故障时，故障由多个因素共同引起，列举如下： 弹性云在逐步灰度升级docker版本至 18.06.3-ce 由于历史原因，弹性云启用了docker服务的systemd配置选项 MountFlags=slave 为了避免dockerd重启引起业务容器重建，弹性云启用了 live-restore=true 配置，docker服务发生重启，dockerd与shim进程mnt ns不一致 在以上三个因素合力作用下，线上容器在重建与漂移场景下，出现删除失败的事件。 同样，文章最后也给出了两种解决方案： 长痛：修改代码，忽略错误 短痛：修改配置，一劳永逸 作为优秀的社会主义接班人，我们当然选择短痛了！依据官方提示 MountFlags=slave 与 live-restore=true 不能协同工作，那么我们只需关闭二者之一就能解决问题。 与我们而言，docker提供的 live-restore 能力是一个很关键的特性。docker重启的原因多种多样，既可能是人为调试因素，也可能是机器的非预期行为，当docker重启后，我们并不希望用户的容器也发生重建。似乎关闭 MountFlags=slave 成了我们唯一的选择。 等等，回想一下docker device busy问题解决方案，别人正是为了避免docker挂载泄漏而引起删除容器失败才开启的这个特性。 但是，这个17年的结论真的还具有普适性吗？是与不是，我们亲自验证即可。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:1","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"2. 对比实验 为了验证在关闭 MountFlags=slave 选项后，docker是否存在挂载点泄漏的问题，我们分别挑选了一台 1.13.1 与 18.06.3-ce 的宿主进行实验。实验步骤正如docker device busy问题解决方案所提示，在验证之前，环境准备如下： 删除docker服务的systemd配置项 MountFlags=slave 挑选启用systemd配置项 PrivateTmp=true 的任意服务，本文以 httpd 为例 下面开始验证： ////// docker 1.13.1 验证步骤及结果 // 1. 重新加载配置 [stupig@hostname2 ~]$ sudo systemctl daemon-reload // 2. 重启docker [stupig@hostname2 ~]$ sudo systemctl restart docker // 3. 创建容器 [stupig@hostname2 ~]$ sudo docker run -d nginx c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c // 4. 重启httpd [stupig@hostname2 ~]$ sudo systemctl restart httpd // 5. 停止容器 [stupig@hostname2 ~]$ sudo docker stop c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c // 6. 清理容器 [stupig@hostname2 ~]$ sudo docker rm c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c Error response from daemon: Driver overlay2 failed to remove root filesystem c89c2aeff6e3e6414dfc7f448b4a560b4aac96d69a82ba021b78ee576bf6771c: remove /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged: device or resource busy // 7. 定位挂载点 [stupig@hostname2 ~]$ grep -rwn /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged /proc/*/mountinfo /proc/19973/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19974/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19975/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19976/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19977/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/19978/mountinfo:40:231 227 0:40 / /home/docker_rt/overlay2/6c77cfb6c0c4b1e809c47af3c5ff6a4732a783cc14ff53270a7709c837c96346/merged rw,relatime shared:119 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // 8. 定位目标进程 [stupig@hostname2 ~]$ ps -ef|egrep '19973|19974|19975|19976|19977|19978' root 19973 1 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19974 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19975 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19976 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19977 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND apache 19978 19973 0 15:13 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND docker 1.13.1 版本的实验结果正如网文所料，容器读写层挂载点出现了泄漏，并且 docker rm 无法清理该容器（注意 docker rm -f 仍然可以清理，原因参考上文）。 弹性云启用docker配置 MountFlags=slave 也是为了避免该问题发生。 那么现在压力转移到 docker 18.06.3-ce 这边来了，新版本是否仍然存在这个问题呢？ ////// docker 18.06.3-ce 验证步骤及结果 [stupig@hostname ~]$ sudo systemctl daemon-reload [stupig@hostname ~]$ sudo systemctl restart docker [stupig@hostname ~]$ sudo docker run -d nginx 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f [stupig@hostname ~]$ sudo systemctl restart httpd [stupig@hostname ~]$ sudo docker stop 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f [stupig@hostname ~]$ sudo docker rm 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f 718114321d67a817c1498e530b943c2514ed4200f2d0d138880f8c345df7048f 针对docker 18.06.3-ce 的实验非常丝滑顺畅，不存在任何问题。回顾上文知识点，当容器读写层挂载点出现泄漏后，docker 18.06.3-","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:2","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"3. 蛛丝马迹 上一节对比实验的结果给了我们莫大的鼓励，本节我们探索两个版本的docker的表现差异，以期定位症结所在。 既然核心问题在于挂载点是否被泄漏，那么我们就以挂载点为切入点，深入分析两个版本docker的差异性。我们对比在两个环境下执行完 步骤4 后，不同进程内的挂载详情，结果如下： // docker 1.13.1 [stupig@hostname2 ~]$ sudo docker run -d nginx 0fe8d412f99a53229ea0df3ec44c93496e150a39f724ea304adb7f924910d61b [stupig@hostname2 ~]$ sudo docker inspect -f {{.GraphDriver.Data.MergedDir}} 0fe8d412f99a53229ea0df3ec44c93496e150a39f724ea304adb7f924910d61b /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged // 共享命名空间 [stupig@hostname2 ~]$ grep -rw /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged /proc/$$/mountinfo 223 1143 0:40 / /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX [stupig@hostname2 ~]$ sudo systemctl restart httpd [stupig@hostname2 ps -ef|grep httpd|head -n 1 root 16715 1 2 16:09 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND // httpd进程命名空间 [stupig@hostname2 ~]$ grep -rw /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged /proc/16715/mountinfo 257 235 0:40 / /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged rw,relatime shared:123 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // docker 18.06.3-ce [stupig@hostname ~]$ sudo docker run -d nginx ce75d4fdb6df6d13a7bf4270f71b3752ee2d3849df1f64d5d5d19a478ac7db8d [stupig@hostname ~]$ sudo docker inspect -f {{.GraphDriver.Data.MergedDir}} ce75d4fdb6df6d13a7bf4270f71b3752ee2d3849df1f64d5d5d19a478ac7db8d /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged // 共享命名空间 [stupig@hostname ~]$ grep -rw /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged /proc/$$/mountinfo 218 43 0:105 / /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged rw,relatime shared:109 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX [stupig@hostname ~]$ sudo systemctl restart httpd [stupig@hostname ~]$ ps -ef|grep httpd|head -n 1 root 63694 1 0 16:14 ? 00:00:00 /usr/sbin/httpd -DFOREGROUND // httpd进程命名空间 [stupig@hostname ~]$ grep -rw /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged /proc/63694/mountinfo 435 376 0:105 / /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged rw,relatime shared:122 master:109 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX 咋一看，好像没啥区别啊！睁大你们的火眼金睛，是否发现差异所在了？ 如果细心对比，还是很容易分辨出差异所在的： 共享命名空间中 docker 18.06.3-ce 版本创建的挂载点是shared的 而docker 1.13.1 版本创建的挂载点是private的 httpd进程命名空间中 docker 18.06.3-ce 创建的挂载点仍然是共享的，并且接收共享组109传递的挂载与卸载事件，注意：共享组109正好就是共享命名空间中对应的挂载点 而docker 1.13.1 版本创建的挂载点虽然也是共享的，但是却与共享命名空间中对应的挂载点没有关联关系 可能会有用户不禁要问：怎么分辨挂载点是什么类型？以及不同类型挂载点的传递属性呢？请参阅：mount命名空间说明文档。 问题已然明了，由于两个版本docker所创建的容器读写层挂载点具备不同的属性，导致它们之间的行为差异。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:3","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"4. 刨根问底 相信大家如果理解了上一节的内容，就已经了解了问题的本质。本节我们继续探索问题的根因。 为什么两个版本的docker行为表现不一致？不外乎两个主要原因： docker处理逻辑发生变动 宿主环境不一致，主要指内核 第二个因素很好排除，我们对比了两个测试环境的宿主内核版本，结果是一致的。所以，基本还是因docker代码升级而产生的行为不一致。理论上，我们只需逐个分析docker 1.13.1 与 docker 18.06.3-ce 两个版本间的所有提交记录，就一定能够定位到关键提交信息，大力总是会出现奇迹。 但是，我们还是希望能够从现场中发现有用信息，缩小检索范围。 仍然从挂载点切入，既然两个版本的docker所创建的挂载点在共享命名空间中就已经出现差异，我们顺藤摸瓜，找找容器读写层挂载点链路上是否存在差异： // docker 1.13.1 // 本挂载点 [stupig@hostname2 ~]$ grep -rw /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged /proc/$$/mountinfo 223 1143 0:40 / /home/docker_rt/overlay2/4e09fa6803feab9d96fe72a44fb83d757c1788812ff60071ac2e62a5cf14cd97/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // 定位本挂载点的父挂载点 [stupig@hostname2 ~]$ grep -rw 1143 /proc/$$/mountinfo 1143 44 8:4 /docker_rt/overlay2 /home/docker_rt/overlay2 rw,relatime - xfs /dev/sda4 rw,attr2,inode64,logbsize=256k,sunit=512,swidth=512,prjquota // 继续定位祖父挂载点 [stupig@hostname2 ~]$ grep -rw 44 /proc/$$/mountinfo 44 39 8:4 / /home rw,relatime shared:28 - xfs /dev/sda4 rw,attr2,inode64,logbsize=256k,sunit=512,swidth=512,prjquota // 继续往上 [stupig@hostname2 ~]$ grep -rw 39 /proc/$$/mountinfo 39 1 8:3 / / rw,relatime shared:1 - ext4 /dev/sda3 rw,stripe=64,data=ordered // docker 18.06.3-ce // 本挂载点 [stupig@hostname ~]$ grep -rw /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged /proc/$$/mountinfo 218 43 0:105 / /home/docker_rt/overlay2/a9823ed6b3c5a752eaa92072ff9d91dbe1467ceece3eedf613bf6ffaa5183b76/merged rw,relatime shared:109 - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX // 定位本挂在点的父挂载点 [stupig@hostname ~]$ grep -rw 43 /proc/$$/mountinfo 43 61 8:17 / /home rw,noatime shared:29 - xfs /dev/sdb1 rw,attr2,nobarrier,inode64,prjquota // 继续定位祖父挂载点 [stupig@hostname ~]$ grep -rw 61 /proc/$$/mountinfo 61 1 8:3 / / rw,relatime shared:1 - ext4 /dev/sda3 rw,data=ordered 两个版本的docker所创建的容器读写层挂载点链路上差异还是非常明显的： 容器读写层挂载点的父级挂载点不同 docker 18.06.3-ce 创建的容器读写层挂载点的父级挂载点是 /home/ ，并且是共享的 docker 1.13.1 创建的容器读写层挂载点的父级挂载点是 /home/docker_rt/overlay2 ，并且是私有的 这里补充一个背景，弹性云机器在初始化阶段，会将 /home 初始化为xfs文件系统类型，因此所有宿主上 /home 挂载点都具备相同属性。 那么，问题基本就是由 docker 1.13.1 中多出的一层挂载层 /home/docker_rt/overlay2 引起。 如何验证这个猜想呢？现在，其实我们已经具备了检索代码的关键目标，docker 1.13.1 会设置容器镜像层根目录的传递属性。拿着这个先验知识，我们直接查代码，检索过程基本没费什么功夫，直接展示相关代码： // filepath: daemon/graphdriver/overlay2/overlay.go func init() { graphdriver.Register(driverName, Init) } func Init(home string, options []string, uidMaps, gidMaps []idtools.IDMap) (graphdriver.Driver, error) { if err := mount.MakePrivate(home); err != nil { return nil, err } supportsDType, err := fsutils.SupportsDType(home) if err != nil { return nil, err } if !supportsDType { // not a fatal error until v1.16 (#27443) logrus.Warn(overlayutils.ErrDTypeNotSupported(\"overlay2\", backingFs)) } d := \u0026Driver{ home: home, uidMaps: uidMaps, gidMaps: gidMaps, ctr: graphdriver.NewRefCounter(graphdriver.NewFsChecker(graphdriver.FsMagicOverlay)), supportsDType: supportsDType, } d.naiveDiff = graphdriver.NewNaiveDiffDriver(d, uidMaps, gidMaps) if backingFs == \"xfs\" { // Try to enable project quota support over xfs. if d.quotaCtl, err = quota.NewControl(home); err == nil { projectQuotaSupported = true } } return d, nil } 很明显，问题就出在 mount.MakePrivate 函数调用上。 官方将 GraphDriver 根目录设置为 Private，本意是为了避免容器读写层挂载点泄漏。那为什么在高版本中去掉了这个逻辑呢？显然官方也意识到这么做并不能实现期望的目的，官方也在修复中给出了详细说明。 实际上，不设置 GraphDriver 根目录的传播属性，反而能避免绝大多数挂载点泄漏的问题。。。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:4","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":"5. 结语 现在，我们已经了解了问题的来龙去脉，我们总结下问题的解决方案： 针对 1.13.1 版本docker，存量宿主较多，我们可以忽略 device or resource busy 问题，基本也不会给线上服务带来什么影响 针对 18.06.3-ce 版本docker，存量宿主较少，我们删除docker服务的systemd配置项 MountFlags，通过故障自愈解决docker卡在问题 在容器创建后，卸载容器读写层挂载，如果不影响容器内文件访问。那么可以直接卸载所有挂载点，修改docker配置，并重启docker服务【本方案尚未验证】 针对增量宿主，全部删除docker服务的systemd配置项 MountFlags ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating2/:0:5","tags":["docker"],"title":"Pod terminating2","uri":"https://www.likakuli.com/posts/docker-pod-terminating2/"},{"categories":["问题排查"],"content":" 转载自组内同事stupig ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:0","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"1. 背景 近期，弹性云线上集群发生了几起特殊的容器漂移失败事件，其特殊之处在于容器处于Pod Terminating状态，而宿主则处于Ready状态。 宿主状态为Ready说明其能够正常处理Pod事件，但是Pod却卡在了退出阶段，说明此问题并非由kubelet引起，那么docker就是1号犯罪嫌疑人了。 下文将详细介绍问题的排查与分析全过程。 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:1","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"2. 抽丝剥茧 2.1 排除kubelet嫌疑 Pod状态如下： [stupig@master ~]$ kubectl get pod -owide pod-976a0-5 0/1 Terminating 0 112m 尽管kubelet的犯罪嫌疑已经很小，但是我们还是需要排查kubelet日志进一步确认。截取kubelet关键日志片段如下： I1014 10:56:46.492682 34976 kubelet_pods.go:1017] Pod \"pod-976a0-5_default(f1e03a3d-0dc7-11eb-b4b1-246e967c4efc)\" is terminated, but some containers have not been cleaned up: {ID:{Type:docker ID:41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef} Name:stupig State:exited CreatedAt:2020-10-14 10:49:57.859913657 +0800 CST StartedAt:2020-10-14 10:49:57.928654495 +0800 CST FinishedAt:2020-10-14 10:50:28.661263065 +0800 CST ExitCode:0 Hash:2101852810 HashWithoutResources:2673273670 RestartCount:0 Reason:Completed Message: Resources:map[CpuQuota:200000 Memory:2147483648 MemorySwap:2147483648]} E1014 10:56:46.709255 34976 remote_runtime.go:250] RemoveContainer \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\" from runtime service failed: rpc error: code = Unknown desc = failed to remove container \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\": Error response from daemon: container 41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef: driver \"overlay2\" failed to remove root filesystem: unlinkat /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged: device or resource busy E1014 10:56:46.709292 34976 kuberuntime_gc.go:126] Failed to remove container \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\": rpc error: code = Unknown desc = failed to remove container \"41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef\": Error response from daemon: container 41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef: driver \"overlay2\" failed to remove root filesystem: unlinkat /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged: device or resource busy 日志显示kubelet处于Pod Terminating状态的原因很清楚：清理容器失败。 kubelet清理容器的命令是 docker rm -f ，其失败的原因在于删除容器目录 xxx/merged 时报错，错误提示为 device or resource busy 。 除此之外，kubelet无法再提供其他关键信息。 登陆宿主，我们验证对应容器的状态： [stupig@hostname ~]$ sudo docker ps -a | grep pod-976a0-5 41020461ed4d Removal In Progress k8s_stupig_pod-976a0-5_default_f1e03a3d-0dc7-11eb-b4b1-246e967c4efc_0 f0a75e10b252 Exited (0) 2 minutes ago k8s_POD_pod-976a0-5_default_f1e03a3d-0dc7-11eb-b4b1-246e967c4efc_0 [stupig@hostname ~]$ sudo docker rm -f 41020461ed4d Error response from daemon: container 41020461ed4d801afa8d10847a16907e65f6e8ca34d1704edf15b0d0e72bf4ef: driver \"overlay2\" failed to remove root filesystem: unlinkat /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged: device or resource busy 问题已然清楚，现在我们有两种排查思路： 参考Google上解决 device or resource busy 问题的思路 结合现象分析代码 2.2 Google大法 有问题找Google！所以，我们首先咨询了Google，检索结果显示很多人都碰到了类似的问题。 而网络上主流的解决方案：配置docker服务MountFlags为slave，避免docker挂载点信息泄漏到其他mnt命名空间，详细原因请参阅：docker device busy问题解决方案。 这么简单？？？显然不能，检查发现docker服务当前已配置MountFlags为slave。网络银弹再次失去功效。 so，我们还是老老实实结合现场分析代码吧。 2.3 docker处理流程 在具体分析docker代码之前，先简单介绍下docker的处理流程，避免作为一只无头苍蝇处处碰壁。 清楚了docker的处理流程之后，我们再来分析现场。 2.4 提审docker 问题发生在docker清理阶段，docker清理容器读写层出错，报错信息为 device or resource busy，说明docker读写层并没有被正确卸载，或者是没有完全卸载。下面的命令可以验证这个结论： [stupig@hostname ~]$ grep -rwn '/home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged' /proc/*/mountinfo /proc/22283/mountinfo:50:386 542 0:92 / /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/22407/mountinfo:50:386 542 0:92 / /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX /proc/28454/mountinfo:50:386 542 0:92 / /home/docker_rt/overlay2/e5dab77be213d9f9cfc0b0b3281dbef9c2878fee3b8e406bc8ab97adc30ae4d5/merged rw,relatime - overlay overlay rw,lowerdir=XXX,upperdir=XXX,workdir=XXX","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:2","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"3. 问题影响 既然所有版本的docker都存在这个问题，那么其影响是什么呢？ 在高版本docker中，其影响是显式的，会引起容器清理失败，进而造成Pod删除失败。 而在低版本docker中，其影响是隐式的，造成挂载点泄漏，进而可能会造成的影响如下： inode被打满：由于挂载点泄漏，容器读写层不会被清理，长时间累计可能会造成inode耗尽问题，但是是小概率事件 容器ID复用：由于挂载点未被卸载，当docker复用了原来已经退出的容器ID时，在挂载容器init层与读写层时会失败。由于docker生成容器ID是随机的，因此也是小概率事件 ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:3","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":"4. 解决方案 问题已然明确，如何解决问题成了当务之急。思路有二： 治标：对标 1.13.1 版本的处理逻辑，修改 18.06.3-ce 处理代码 治本：既然官方也提及 MountFlags=slave 与 live-restore 不能同时使用，那么我们修改两个配置选项之一即可 考虑到 重启docker不重启容器 这样一个强需求的存在，似乎我们唯一的解决方案就是关闭 MountFlags=slave 配置。关闭该配置后，与之而来的疑问如下： 能够解决本问题？ 网传其他systemd托管服务启用PrivateTmp是否会造成挂载点泄漏？ 欲知后事如何，且听下回分解！ ","date":"2020-10-31","objectID":"https://www.likakuli.com/posts/docker-pod-terminating/:0:4","tags":["docker"],"title":"Pod terminating","uri":"https://www.likakuli.com/posts/docker-pod-terminating/"},{"categories":["问题排查"],"content":" 转载自组内同事 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:0","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"1. 背景 最近升级了一版kubelet，修复因kubelet删除Pod慢导致平台删除集群超时的问题。在灰度redis隔离集群的时候，发现升级kubelet并重启服务后，少量宿主状态变成了NotReady，并且回滚kubelet至之前版本，宿主状态仍然是NotReady。查看宿主状态时提示 ‘container runtime is down’ ，根据经验，此时一般就是容器运行时出了问题。弹性云使用的容器运行时是docker，我们就去检查docker的状态，检测结果如下： docker ps 查看所有容器状态，执行正常 docker inspect 查看某一容器详细状态，执行阻塞 典型的docker hang死行为。因为我们最近在升级docker版本，存量宿主docker的版本为1.13.1，并且在逐步升级至18.06.3，新宿主的docker版本都是18.06.3。docker hang死问题在1.13.1版本上表现得更彻底，在执行docker ps的时候就已经hang死了，一旦某个容器出了问题，docker就处于无响应状态；而docker 18.06.3做了一点小小的优化，在执行docker ps时去掉了针对容器级别的加锁操作，但是docker inspect依然会加容器锁，因此某一个容器出现问题，并不会造成docker服务不可响应，受影响的也仅仅是该容器，无法执行任何操作。 至于为什么以docker ps与docker inspect为指标检查docker状态，因为kubelet就是依赖这两个docker API获取容器状态。 所以，现在问题有二： docker hang死的根因是什么？ docker hang死时，为什么重启kubelet，会导致宿主状态变为NotReady？ ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:1","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"2.重启kubelet变更宿主状态 kubelet重启后宿主状态从Ready变为NotReady，这个问题相较docker hang死而言，没有那么复杂，所以我们先排查这个问题。 kubelet针对宿主会设置多个Condition，表明宿主当前所处的状态，比如宿主内存是否告急、线程数是否告急，以及宿主是否就绪。其中ReadyCondition表明宿主是否就绪，kubectl查看宿主状态时，展示的Statue信息就是ReadCondition的内容，常见的状态及其含义定义如下： Ready状态：表明当前宿主状态一切OK，能正常响应Pod事件 NotReady状态：表明宿主的kubelet仍在运行，但是此时已经无法处理Pod事件。NotReady绝大多数情况都是容器运行时出了问题 Unknown状态：表明宿主kubelet已停止运行 kubelet定义的ReadyCondition的判定条件如下： // defaultNodeStatusFuncs is a factory that generates the default set of // setNodeStatus funcs func (kl *Kubelet) defaultNodeStatusFuncs() []func(*v1.Node) error { ...... setters = append(setters, nodestatus.OutOfDiskCondition(kl.clock.Now, kl.recordNodeStatusEvent), nodestatus.MemoryPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderMemoryPressure, kl.recordNodeStatusEvent), nodestatus.DiskPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderDiskPressure, kl.recordNodeStatusEvent), nodestatus.PIDPressureCondition(kl.clock.Now, kl.evictionManager.IsUnderPIDPressure, kl.recordNodeStatusEvent), nodestatus.ReadyCondition(kl.clock.Now, kl.runtimeState.runtimeErrors, kl.runtimeState.networkErrors, validateHostFunc, kl.containerManager.Status, kl.recordNodeStatusEvent), nodestatus.VolumesInUse(kl.volumeManager.ReconcilerStatesHasBeenSynced, kl.volumeManager.GetVolumesInUse), // TODO(mtaufen): I decided not to move this setter for now, since all it does is send an event // and record state back to the Kubelet runtime object. In the future, I'd like to isolate // these side-effects by decoupling the decisions to send events and partial status recording // from the Node setters. kl.recordNodeSchedulableEvent, ) return setters } 深入nodestatus.ReadyCondition的实现可以发现，宿主是否Ready取决于很多条件，包含运行时判定、网络判定、基本资源判定等。这里我们只需关注运行时判定即可： func (s *runtimeState) runtimeErrors() []string { s.RLock() defer s.RUnlock() var ret []string if !s.lastBaseRuntimeSync.Add(s.baseRuntimeSyncThreshold).After(time.Now()) { // 1 ret = append(ret, \"container runtime is down\") } if s.internalError != nil { ret = append(ret, s.internalError.Error()) } for _, hc := range s.healthChecks { // 2 if ok, err := hc.fn(); !ok { ret = append(ret, fmt.Sprintf(\"%s is not healthy: %v\", hc.name, err)) } } return ret } 当出现如下两种状况之一时，则判定运行时检查不通过： 距最近一次运行时同步操作的时间间隔超过指定阈值（默认30s） 运行时健康检查未通过 那么，当时宿主的NotReady是由哪种状况引起的呢？结合kubelet日志分析，kubelet每隔5s就输出一条日志： ...... I0715 10:43:28.049240 16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down] I0715 10:43:33.049359 16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down] I0715 10:43:38.049492 16315 kubelet.go:1835] skipping pod synchronization - [container runtime is down] ...... 因此，状况1是宿主NotReady的元凶。 我们继续分析为什么kubelet没有按照预期设置lastBaseRuntimeSync。kubelet启动时会创建一个goroutine，并在该goroutine中循环设置lastBaseRuntimeSync，循环如下： func (kl *Kubelet) Run(updates \u003c-chan kubetypes.PodUpdate) { ...... go wait.Until(kl.updateRuntimeUp, 5*time.Second, wait.NeverStop) ...... } func (kl *Kubelet) updateRuntimeUp() { kl.updateRuntimeMux.Lock() defer kl.updateRuntimeMux.Unlock() ...... kl.oneTimeInitializer.Do(kl.initializeRuntimeDependentModules) kl.runtimeState.setRuntimeSync(kl.clock.Now()) } 正常情况下，kubelet每隔5s会将lastBaseRuntimeSync设置为当前时间，而宿主状态异常时，这个时间戳一直未被更新。也即updateRuntimeUp一直被阻塞在设置lastBaseRuntimeSync之前的某一步。我们只需逐个排查updateRuntimeUp内的函数调用即可，具体过程不再展示，最终的函数调用链路如下： initializeRuntimeDependentModules -\u003e kl.cadvisor.Start -\u003e cc.Manager.Start -\u003e self.createContainer -\u003e m.createContainerLocked -\u003e container.NewContainerHandler -\u003e factory.CanHandleAndAccept -\u003e self.client.ContainerInspect 由于某个容器状态异常，kubelet执行docker inspect操作也被hang死。 因此，重启kubelet引起宿主状态从Ready变为NotReady，其根因在于某个容器状态异常，执行docker inspect时被hang死。而如果docker inspect hang死发生在kubelet重启之后，则不会对宿主的Ready状态造成任何影响，因为oneTimeInitializer是sync.Once类型，也即仅仅会在kebelet启动时执行一次。那时kubelet仅仅是不能处理该Pod相关的任何事件，包含删除、变更等，但是仍然能够处理其他Pod的任意事件。 可能有人会问，为什么kubelet重启时访问docker inspect操作不加超时控制？确实，如果添加了超时控制，kubelet重启不会引起宿主状态变更。待详细挖掘后再来补充，我们先继续分析docker hang死的问题。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:2","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"3. docker hang死 我们对docker hang死并不陌生，因为已经发生了好多起。其发生时的现象也多种多样。以往针对docker 1.13.1版本的排查都发现了一些线索，但是并没有定位到根因，最终绝大多数也是通过重启docker解决。而这一次发生在docker 18.06.3版本的docker hang死行为，经过我们4人小分队接近一周的望闻问切，终于确定了其病因。注意，docker hang死的原因不止一种，因此本处方并非是个万能药。 现在，我们掌握的知识仅仅是docker异常了，无法响应特定容器的docker inspect操作，而对详细信息则一无所知。 3.1 链路跟踪 首先，我们希望对docker运行的全局状况有一个大致的了解，熟悉go语言开发的用户自然能联想到神器pprof。我们借助pprof描绘出了docker当时运行的蓝图： goroutine profile: total 722373 717594 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2132ef 0x7fe8bc238c1a 0x7fe8bd56f7fe 0x7fe8bd56f6bd 0x7fe8bcea8719 0x7fe8bcea938b 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711 # 0x7fe8bc2132ee sync.runtime_SemacquireMutex+0x3e /usr/local/go/src/runtime/sema.go:71 # 0x7fe8bc238c19 sync.(*Mutex).Lock+0x109 /usr/local/go/src/sync/mutex.go:134 # 0x7fe8bd56f7fd github.com/docker/docker/daemon.(*Daemon).ContainerInspectCurrent+0x8d /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:40 # 0x7fe8bd56f6bc github.com/docker/docker/daemon.(*Daemon).ContainerInspect+0x11c /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/daemon/inspect.go:29 # 0x7fe8bcea8718 github.com/docker/docker/api/server/router/container.(*containerRouter).getContainersByName+0x118 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/inspect.go:15 # 0x7fe8bcea938a github.com/docker/docker/api/server/router/container.(*containerRouter).(github.com/docker/docker/api/server/router/container.getContainersByName)-fm+0x6a /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router/container/container.go:39 # 0x7fe8bcb726c9 github.com/docker/docker/api/server/middleware.ExperimentalMiddleware.WrapHandler.func1+0xd9 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/experimental.go:26 # 0x7fe8bcb72b00 github.com/docker/docker/api/server/middleware.VersionMiddleware.WrapHandler.func1+0x400 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/middleware/version.go:62 # 0x7fe8bc71c26a github.com/docker/docker/pkg/authorization.(*Middleware).WrapHandler.func1+0x7aa /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/pkg/authorization/middleware.go:59 # 0x7fe8bcb85f49 github.com/docker/docker/api/server.(*Server).makeHTTPHandler.func1+0x199 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/server.go:141 # 0x7fe8bc4b9895 net/http.HandlerFunc.ServeHTTP+0x45 /usr/local/go/src/net/http/server.go:1947 # 0x7fe8bc72a437 github.com/docker/docker/vendor/github.com/gorilla/mux.(*Router).ServeHTTP+0x227 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/vendor/github.com/gorilla/mux/mux.go:103 # 0x7fe8bcb849e1 github.com/docker/docker/api/server.(*routerSwapper).ServeHTTP+0x71 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/api/server/router_swapper.go:29 # 0x7fe8bc4bc67d net/http.serverHandler.ServeHTTP+0xbd /usr/local/go/src/net/http/server.go:2694 # 0x7fe8bc4b88a2 net/http.(*conn).serve+0x652 /usr/local/go/src/net/http/server.go:1830 4175 @ 0x7fe8bc202980 0x7fe8bc202a40 0x7fe8bc2135d8 0x7fe8bc2132ef 0x7fe8bc238c1a 0x7fe8bcc2eccf 0x7fe8bd597af4 0x7fe8bcea2456 0x7fe8bcea956b 0x7fe8bcb73dff 0x7fe8bcb726ca 0x7fe8bcb72b01 0x7fe8bc71c26b 0x7fe8bcb85f4a 0x7fe8bc4b9896 0x7fe8bc72a438 0x7fe8bcb849e2 0x7fe8bc4bc67e 0x7fe8bc4b88a3 0x7fe8bc230711 # 0x7fe8bc2132ee sync.runtime_SemacquireMutex+0x3e /usr/local/go/src/runtime/sema.go:71 # 0x7fe8bc238c19 sync.(*Mutex).Lock+0x109 /usr/local/go/src/sync/mutex.go:134 # 0x7fe8bcc2ecce github.com/docker/docker/container.(*State).IsRunning+0x2e /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com/docker/docker/container/state.go:240 # 0x7fe8bd597af3 github.com/docker/docker/daemon.(*Daemon).ContainerStats+0xb3 /root/rpmbuild/BUILD/src/engine/.gopath/src/github.com","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:3","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"4. 解决方案 当大家了解了docker hang死的成因之后，我们可以针对性的提出如下解决办法。 4.1 最直观的办法 既然docker exec可能会引起docker hang死，那么我们禁用系统中所有的docker exec操作即可。最典型的是kubelet的probe，当前我们默认给所有Pod添加了ReadinessProbe，并且是以exec的形式进入容器内执行命令。我们调整kubelet的探测行为，修改为tcp或者http probe即可。 这里虽然改动不大，但是涉及业务容器的改造成本太大了，如何迁移存量集群是个大问题。 4.2 最根本的办法 既然当前containerd-shim读pipe需要等待runc exec执行完毕，如果我们将读pipe的操作提前至runc exec命令执行之前，理论上也可以避免死锁。 同样。这种方案的升级成本太高了，升级containerd-shim时需要重启存量的所有容器，这个方案基本不可能通过。 4.3 最简单的办法 既然runc init阻塞在写pipe，我们主动读取pipe内的内容，也能让runc init顺利推出。 再将本解决方案自动化的过程中，如何能够识别如docker hang死是由于写pipe导致的，是一个小小的挑战。但是相对于以上两种解决方案，我认为还是值得一试，毕竟影响面微乎其微。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:4","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"5. 后续 docker hang死的原因远非这一种，本次排查的结果也并非适用于所有场景。希望各位看官能够根据自己的现场排查问题。另外查看linux文档，pipe capacity是可以设置的，高版本中since 4.5也可以通过设置/proc/sys/fs/pipe-user-pages-soft来解决，但是如文档所说，在4.9之前/proc/sys/fs/pipe-user-pages-soft还是存在一些bug的，至于采用什么办法解决，还得根据自己情况来做选择。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/docker-hang/:0:5","tags":["docker"],"title":"docker hang问题排查","uri":"https://www.likakuli.com/posts/docker-hang/"},{"categories":["问题排查"],"content":"1. 揭开面纱 周一，接到RD反馈线上容器网络访问存在异常，具体线上描述如下： 上游服务driver-api所有容器访问下游服务duse-api某一容器TCP【telnet测试】连接不通，访问其余下游容器均正常 上游服务容器测试下游容器IP连通性【ping测试】正常 从以上两点现象可以得出一个结论： 容器的网络设备存在，IP地址连通，但是容器服务进程未启动，端口未启动 但是，当我们和业务RD确认之后，发现业务容器状态正常，业务进程也正运行着。嗯，问题不简单。 此外，飞哥这边排查还有一个结论： arp反向解析duse-api特殊容器IP时，不返回MAC地址信息 当telnet失败后，立即执行arp，会返回MAC地址信息 当我们拿着arp解析的MAC地址与容器当前的MAC地址作比较时，发现MAC地址不一致。唔，基本上确定问题所在了，net ns泄漏了。执行如下命令验证： sudo ip netns ls | while read ns; do sudo ip netns exec $ns ip addr; done | grep inet | grep -v 127 | awk '{print $2}' | sort | uniq -c 确实发现该容器对应的IP出现了两次，该容器IP对应了两个网络命名空间，也即该容器的网络命名空间出现了泄漏。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:1","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"2. 误入迷障 当确定了问题所在之后，我们立马调转排查方向，重新投入到net ns泄漏的排查事业当中。 既然net ns出现了泄漏，我们只需要排查被泄露的net ns的成因即可。在具体定位之前，首先补充一个背景： ip netns 命令默认扫描 /var/run/netns 目录，从该目录下的文件读取net ns的信息 默认情况下，kubelet调用docker创建容器时，docker会将net ns文件隐藏，如果不做特殊处理，我们执行 ip netns 命令将看不到任何数据 当前弹性云为了方便排查问题，做了一个特殊处理，将容器的网络命名空间mount到 /var/run/netns 目录 【注意，这里有个大坑】 有了弹性云当前的特殊处理，我们就可以知道所有net ns的创建时间，也即 /var/run/netns 目录下对应文件的创建时间。 我们查看该泄漏ns文件的创建时间为2020-04-17 11:34:07，排查范围进一步缩小，只需从该时间点附近排查即可。 接下来，我们分析了该附近时间段，容器究竟遭遇了什么： 2020-04-17 11:33:26 用户执行发布更新操作 2020-04-17 11:34:24 平台显示容器已启动 2020-04-17 11:34:28 平台显示容器启动脚本执行失败 2020-04-17 11:36:22 用户重新部署该容器 2020-04-17 11:36:31 平台显示容器已删除成功 既然是容器网络命名空间泄漏，则说明再删除容器的时候，没有执行ns的清理操作。【注：这里由于基础知识不足，导致问题****排查绕了地球一圈】 我们梳理kubelet在该时间段对该容器的清理日志，核心相关日志展示如下： I0417 11:36:30.974674 37736 kubelet_pods.go:1180] Killing unwanted pod \"duse-api-sf-xxxxx-0\" I0417 11:36:30.976803 37736 plugins.go:391] Calling network plugin cni to tear down pod \"duse-api-sf-80819-0_default\" I0417 11:36:30.983499 37736 kubelet_pods.go:1780] Orphaned pod \"4ae28778-805c-11ea-a54c-b4055d1e6372\" found, removing pod cgroups I0417 11:36:30.986360 37736 pod_container_manager_linux.go:167] Attempt to kill process with pid: 48892 I0417 11:36:30.986382 37736 pod_container_manager_linux.go:174] successfully killed all unwanted processes. 简单描述流程： I0417 11:36:30.974674 根据删除容器执行，执行杀死Pod操作 I0417 11:36:30.976803 调用cni插件清理网络命名空间 I0417 11:36:30.983499 常驻协程检测到Pod已终止运行，开始执行清理操作，包括清理目录、cgroup I0417 11:36:30.986360 清理cgroup时杀死容器中还未退出的进程 I0417 11:36:30.986382 显示所有容器进程都已被杀死 这里提示一点：正常情况下，容器退出时，容器内所有进程都已退出。而上面之所以出现清理cgroup时需要杀死容器内未退出进程，是由于常驻协程的检测机制导致的，常驻协程判定Pod已终止运行的条件是： // podIsTerminated returns true if pod is in the terminated state (\"Failed\" or \"Succeeded\"). func (kl *Kubelet) podIsTerminated(pod *v1.Pod) bool { // Check the cached pod status which was set after the last sync. status, ok := kl.statusManager.GetPodStatus(pod.UID) if !ok { // If there is no cached status, use the status from the // apiserver. This is useful if kubelet has recently been // restarted. status = pod.Status } return status.Phase == v1.PodFailed || status.Phase == v1.PodSucceeded || (pod.DeletionTimestamp != nil \u0026\u0026 notRunning(status.ContainerStatuses)) } 这个容器命中了第三个或条件：容器已被标记删除，并且所有业务容器都不在运行中（业务容器启动失败，根本就没运行起来过），但是Pod的sandbox容器可能仍然处于运行状态。 仅依据上面的kubelet日志，难以发现问题所在。我们接着又分析了cni插件的日志，截取cni在删除该Pod容器网络时的日志如下： [pid:98497] 2020/04/17 11:36:30.990707 main.go:89: ===== start cni process ===== [pid:98497] 2020/04/17 11:36:30.990761 main.go:90: os env: [CNI_COMMAND=DEL CNI_CONTAINERID=c2ef79f7596b6b558f0c01c0715bac46714eefd1e9966625a09414c7218e1013 CNI_NETNS=/proc/48892/ns/net CNI_ARGS=IgnoreUnknown=1;K8S_POD_NAMESPACE=default;K8S_POD_NAME=duse-api-sf-xxxxx-0;K8S_POD_INFRA_CONTAINER_ID=c2ef79f7596b6b558f0c01c0715bac46714eefd1e9966625a09414c7218e1013 CNI_IFNAME=eth0 CNI_PATH=/home/kaku/kakucloud/cni-plugins/bin LANG=en_US.UTF-8 PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin KUBE_LOGTOSTDERR=--logtostderr=false KUBE_LOG_LEVEL=--v=3 KUBE_ALLOW_PRIV=--allow-privileged=true KUBE_MASTER=--master=https://10.xxx.xxx.xxx:xxxx KUBELET_ADDRESS=--address=0.0.0.0 KUBELET_HOSTNAME=--hostname_override=10.xxx.xxx.xxx KUBELET_POD_INFRA_CONTAINER= KUBELET_ARGS=--network-plugin=cni --cni-bin-dir=/home/kaku/kakucloud/cni-plugins/bin --cni-conf-dir=/home/kaku/kakucloud/cni-plugins/conf --kubeconfig=/etc/kubernetes/kubeconfig/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl --log-dir=/var/log/kubernetes --stderrthreshold=3 --allowed-unsafe-sysctls=net.*,kernel.shm*,kernel.msg*,kernel.sem,fs.mqueue.* --pod-infra-container-image=registry.kaku.com/kakuk8s/pause:3.0 --eviction-hard= --image-gc-high-threshold=75 --image-gc-low-threshold=65 --feature-gates=KubeletPluginsWatcher=false --restart-count-limit=5 --last-upgrade-time=2019-07-01] [pid:98497] 2020/04/17 11:36:30.990771 main.go:91: stdin : {\"cniVersion\":\"0.3.0\",\"logDir\":\"/home/kaku/kakucloud/cni-plugins/acllogs\",\"name\":\"ddcloudcni\",\"type\":\"aclCni\"} [pid:98497] 2020/04/17 11:36:30.990790 main.go:181: failed to ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:2","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"3. 拨云现月 这个结论对我们来说，不是一个好消息。费力不小，不说南辕北辙，但是确实还未发现问题的根因。 为了进一步缩小问题排查范围，我们找内核组同学请教了一个基础知识： 在删除net ns时，如果该ns内仍有网络设备，系统自动先删除网络设备，然后再删除ns 掌握了这个基础知识，我们再来排查。既然原生k8s集群不存在net ns泄漏问题，那问题一定由我们定制的某个模块引起。由于net ns泄漏发生在node上，当前弹性云在node节点上部署的模块包含： kubelet cni plugins other tools 由于kubelet已经被排除嫌疑，那么罪魁祸首基本就是cni插件了。对比原生集群与弹性云线上集群的cni插件，发现一个极有可能会造成net ns泄漏的点： 定制的cni插件为了排查问题的方便，将容器的网络命名空间文件绑定挂载到了/var/run/netns 目录下 【参考上面的大坑】 我们赶紧着手验证元凶是否就是它。修改cni插件代码，删除绑定挂载操作，然后在测试环境验证。验证结果符合预期，net ns不在泄漏。至此，真相终于大白于天下了。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:3","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"4. 亡羊补牢 当初为net ns做一个绑定挂载，其目的就是为了方便我们排查问题，使得 ip netns 命令能够访问当前宿主上所有Pod的网络命名空间。 但其实一个简单的软链操作就能够实现这个目标。Pod退出时，如果这个软链文件未被清理，也不会引起net ns的泄漏，同时 ls -la /var/run/netns 命令可以清晰的看到哪些net ns仍有效，哪些已无效。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:4","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"5. 事后诸葛 为什么绑定挂载能够导致net ns泄漏呢？这是由linux 网络命名空间特性决定的： 只要该命名空间中仍有一个进程存活，或者存在绑定挂载的情况（可能还存在其他情况），该ns就不会被回收 而一旦所有进程都已退出，并且也无特殊状况，linux将自动回收该ns 最后，这个问题本身并不复杂，之所以问题存在如此之久，排查如此曲折，主要暴露了我们的基础知识有所欠缺。 好好学习，天天向上，方是王道！ ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/netns-leak/:0:5","tags":["cni"],"title":"netns泄露","uri":"https://www.likakuli.com/posts/netns-leak/"},{"categories":["问题排查"],"content":"问题描述 在修复cgroup泄漏问题时会现停掉kubelet，待修复完成后启动kubelet组件，重启后收到业务反馈，业务容器重启了。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/:0:1","tags":["kubernetes"],"title":"Kubelet重启导致容器重启","uri":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/"},{"categories":["问题排查"],"content":"问题排查 这个问题具体原因的排查还是花了一定时间的，下面会列一下大致的排查思路并结合源码（自定义的1.12.4分支）进行分析。 排查过程中涉及到了3个容器，如下 名称 集群 宿主 结果 重启次数 1 auto-srv-cwhttp-sf-30b71-0 py 10.86.98.42 重启 1 2 conf-master-sf-19cf6-0 us01 10.15.29.31 重启 1 3 opensource-sf-dc750-2 us01 10.15.29.31 未重启 1 容器启停相关的组件首先想到的就是kubelet，因此去查看kubelet日志，拿py的举例，重启时间为2020-03-12 10:42:27，所以只需要看这之前的一些日志 这里直接贴出来最后过滤后的日志，省略一些中间过程 root@ddcloud-underlay-kube-node065.py:~$ grep 0312 /var/log/kubernetes/kubelet.INFO | grep -E \"10:42:26|10:42:27\" | grep -E \"8a61fda8d43e4c28d4092a1bc8e5f372846d955ffffe0353a754c2e42f271b56|auto-srv-cwhttp-sf-30b71-0|bfde4b15-cb98-11e8-a3c8-6c92bf85beda\" | grep -v -i volume I0312 10:42:26.101676 2353235 kubelet.go:1887] SyncLoop (ADD, \"api\"): \"lifejuhe-pre-sf-d46e3-0_default(9c048a53-53b2-11ea-b6ae-6c92bf85be08), prime-manager-sf-71a2e-2_default(35bc54e5-4ef1-11ea-b6ae-6c92bf85be08), ai-business-sf-0cac5-19_default(b60c0a18-4d70-11ea-b6ae-6c92bf85be08), disconf-sf-92894-2_default(4ae4dfb8-fee4-11e9-b433-6c92bf85be08), am-base-api-new-sf-de855-4_default(e59f4da9-cd68-11e9-92ca-6c92bf85be08), auto-srv-menu-sf-96f75-1_default(eb6ef2a6-4c7c-11ea-b6ae-6c92bf85be08), pre-000_default(d8f7688a-cc39-11e8-a3c8-6c92bf85beda), auto-wechat-srv-sf-e680a-0_default(9a6e0fc8-051c-11ea-b433-6c92bf85be08), oracle-pre-sf-253a7-0_default(c0cef405-5fa8-11ea-b6ae-6c92bf85be08), auto-srv-cwhttp-sf-30b71-0_default(bfde4b15-cb98-11e8-a3c8-6c92bf85beda), auto-srv-chewu-sf-dc750-0_default(1d9eac73-620a-11ea-b6ae-6c92bf85be08), member-sf-747bb-64_default(4e761a14-634e-11ea-b6ae-6c92bf85be08), ep-arch-tmp-bh-sf-1eaad-4_default(7a2d3f79-001a-11e9-a95b-6c92bf85beda), nsky-htw-h5-extranet-sf-900e8-1_default(10ac4571-08c5-11e9-a95b-6c92bf85beda), fate-saver-pre-sf-5cee9-0_default(39915cd3-f70f-11e9-b433-6c92bf85be08), shortserver-sf-ab776-4_default(d1909587-6362-11ea-b6ae-6c92bf85be08), rainbow-h5-sf-c72e4-3_default(cba38b41-1cbe-11ea-b433-6c92bf85be08), agency-call-service-sf-674c2-29_default(e74635f4-5b46-11ea-b6ae-6c92bf85be08), hnc-ddrccp-sf-db379-1_default(f16fbe5f-9ba4-11e9-a777-6c92bf85be08), log-hnc-sf-6a5be-152_default(a87148b5-c002-11e9-92ca-6c92bf85be08), actinia-service-py-sf-61fb7-0_default(75ef3250-3998-11e9-a95b-6c92bf85beda), its-timing-test-sf-8dcfc-0_default(3ad3b185-d175-11e8-a3c8-6c92bf85beda), de-st-forecastor-sf-5a382-3_default(5c6c02b8-ea6c-11e9-8a9f-6c92bf85be08), transit-compass-sf-69886-3_default(5f28569f-7152-11e9-9a4a-6c92bf85be08), gundam-centos6-002_default(3d075c28-042a-11e9-b650-6c92bf85be08), loan-credit-server-sf-c411b-4_default(6455d8e9-61fe-11ea-b6ae-6c92bf85be08), marketingmodel-service-sf-b1260-16_default(9d570b88-5f31-11e9-9b14-6c92bf85beda), csi-hnc-sf-a8523-16_default(b6078000-5f60-11ea-b6ae-6c92bf85be08), ms-shutdown-datadriver-sf-30b86-4_default(805e9837-f0d7-11e9-a648-6c92bf85be08), abnormal-shutdown-service-sf-342c4-18_default(ccb413e6-d832-11e9-92ca-6c92bf85be08), fate-lancer-sf-49896-2_default(241ad7ce-5c53-11ea-b6ae-6c92bf85be08), jetfire-sf-7dbf2-2_default(13f50c1d-5de8-11ea-b6ae-6c92bf85be08), hnc-pre-v-sf-745f9-0_default(70a98be0-3fbe-11e9-a95b-6c92bf85beda), orderpre-sf-b433d-0_default(03f84b30-1760-11ea-b433-6c92bf85be08), gatewayserver-sf-fc2e7-9_default(310030c4-62b3-11ea-b6ae-6c92bf85be08), base-message-service-sf-1b6f0-2_default(f63261ce-d0aa-11e9-92ca-6c92bf85be08), athena-api-pre-sf-d61bf-0_default(9f7e4183-0089-11ea-b433-6c92bf85be08), soda-f-api-sf-c88f2-2_default(fc9520db-8211-11e9-913a-6c92bf85beda), soda-d-api-py-sf-c6659-5_default(af4a67f9-3216-11ea-b433-6c92bf85be08), rollsroyce-pre-sf-6ed43-0_default(78a053cf-6375-11ea-b6ae-6c92bf85be08), member-sf-747bb-74_default(7323a872-634e-11ea-b6ae-6c92bf85be08), settle-consumer-abtest-hnc-pre-v-sf-b18cf-0_default(d199f994-6363-11ea-b6ae-6c92bf85be08), dpub-vote-pre-sf-bc747-0_default(7388d4f7-1a82-11ea-b433-6c92bf85be08), delta-hub-web-sf-bc67e-2_default(35699c85-620c-11ea-b6ae-6c92bf85be08), drunkeness-model-service-sf-a38a4-43_default(c3cd5daa-5b47-11ea-b6ae-6c92bf85be08), lifeapi-hnc-sf-4a88c-2_default(46","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/:0:2","tags":["kubernetes"],"title":"Kubelet重启导致容器重启","uri":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/"},{"categories":["问题排查"],"content":"结论 赋值和获取值的操作在两个goroutine，且严重依赖于第一次的赋值操作，所以应该在保证第一次赋值后再进行取值操作才能确保容器不重启，虽然分支3判断有没有默认探针中在设置和获取的时候都加了锁，但还是无法保证代码执行顺序，所以即使走分支3，也有可能会出现设置ready为true的情况（本该设置为false），但是因为这正是我们想要的效果，所以我们是不会觉察到这种情况的问题的，也就是说开源的代码中可能也是存在类似误判的风险的，至少1.12.4版本中是存在的。 ","date":"2020-10-26","objectID":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/:0:3","tags":["kubernetes"],"title":"Kubelet重启导致容器重启","uri":"https://www.likakuli.com/posts/kubernetes-kubelet-restart/"},{"categories":["问题排查"],"content":"knative健康检查","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 在支持了前面的定制功能后，集群中部署ksvc服务时会报IngressNotConfigured错误 ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:1","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"原因分析 首先根据错误提示及日志信息，可以发现是在做健康检查的时候出的问题，期望得到200，但是得到了404 func (m *Prober) probeVerifier(item *workItem) prober.Verifier { return func(r *http.Response, _ []byte) (bool, error) { // In the happy path, the probe request is forwarded to Activator or Queue-Proxy and the response (HTTP 200) // contains the \"K-Network-Hash\" header that can be compared with the expected hash. If the hashes match, // probing is successful, if they don't match, a new probe will be sent later. // An HTTP 404/503 is expected in the case of the creation of a new Knative service because the rules will // not be present in the Envoy config until the new VirtualService is applied. // No information can be extracted from any other scenario (e.g. HTTP 302), therefore in that case, // probing is assumed to be successful because it is better to say that an Ingress is Ready before it // actually is Ready than never marking it as Ready. It is best effort. switch r.StatusCode { case http.StatusOK: hash := r.Header.Get(network.HashHeaderName) switch hash { case \"\": m.logger.Errorf(\"Probing of %s abandoned, IP: %s:%s: the response doesn't contain the %q header\", item.url, item.podIP, item.podPort, network.HashHeaderName) return true, nil case item.ingressState.hash: return true, nil default: m.logger.Warnf(\"unexpected hash: want %q, got %q\", item.ingressState.hash, hash) return true, nil } // 日志中报错的地方，探活希望得到200，但是得到了404 case http.StatusNotFound, http.StatusServiceUnavailable: return false, fmt.Errorf(\"unexpected status code: want %v, got %v\", http.StatusOK, http.StatusNotFound) default: m.logger.Errorf(\"Probing of %s abandoned, IP: %s:%s: the response status is %v, expected 200 or 404\", item.url, item.podIP, item.podPort, r.StatusCode) return true, nil } } } 其实这时候大致也能猜到是什么原因了，因为我们定制了通过USN进行过滤，探活的时候，Url中其实是没有USN的。下一步就是顺藤摸瓜，找到探活对应的代码验证我们的猜想，也比较简单 // processWorkItem processes a single work item from workQueue. // It returns false when there is no more items to process, true otherwise. func (m *Prober) processWorkItem() bool { ... // probePath /healthz probeURL.Path = path.Join(probeURL.Path, probePath) ok, err := prober.Do( item.context, transport, probeURL.String(), prober.WithHeader(network.UserAgentKey, network.IngressReadinessUserAgent), prober.WithHeader(network.ProbeHeaderName, network.ProbeHeaderValue), m.probeVerifier(item)) ... } 可以看到探活的时候就是拿Path拼上/heathz，验证了我们的猜想 ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:2","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"修复 修改也就比较简单了，在添加wotkitem时，预先把USN添加到path中即可 func (l *gatewayPodTargetLister) ListProbeTargets(ctx context.Context, ing *v1alpha1.Ingress) ([]status.ProbeTarget, error) { ... // Use sorted hosts list for consistent ordering. for i, host := range gatewayHosts[gatewayName].List() { newURL := *target.URLs[0] newURL.Host = host + \":\" + target.Port var usn string if ing.Annotations != nil { usn = ing.Annotations[\"serverless.kakuchuxing.com/usn\"] } newURL.Path = path.Join(newURL.Path, usn) qualifiedTarget.URLs[i] = \u0026newURL ... } ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:3","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["问题排查"],"content":"总结 通过这个问题也看到了对于一些细节和关键流程掌握的还不够，还是需要进行系统性的学习。至于健康检查的逻辑，和k8s的健康检查稍有不同，参考这篇文章 ","date":"2020-08-20","objectID":"https://www.likakuli.com/posts/knative-healthcheck/:0:4","tags":["knative"],"title":"Knative健康检查","uri":"https://www.likakuli.com/posts/knative-healthcheck/"},{"categories":["定制开发"],"content":"通过header访问指定版本","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 最近在搭建公司级的serverless平台，需要用到域名来访问内部服务，采取的是通过PATH来区分不同的服务，域名采用同一个。上一篇已经解决了通过Path访问不同服务的问题，但是在灰度过程中可能会想测试下新版本时候正常，如何将流量打到指定版本上呢？原生的knative是通过url的不同实现的，可以配置一个根据版本生成url的模板，设置后不同版本的服务url不同。但是我们的场景是所有服务url相同，于是我们约定通过在设置特殊的header的来实现此功能 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/:0:1","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"方案 原生通过url来区分不同版本，实现方式是通过在生成vs时，设置其Match的条件Authroty为对应的url prefix即可。显然无法满足我们当前统一使用一个url的场景。但是我们可以参考其实现方式，换一个维度，靠header实现即可，但是又不能影响正常访问，即不添加header的时候，流量按照设置的比例打到不同的revision上，添加了header后，需要将流量打到指定版本，所以不能简单的在Match中添加Header，需要分别设置正常访问的情况和访问指定版本的情况，且访问指定版本的配置应该顺序靠前。 // MakeIngressSpec creates a new IngressSpec func MakeIngressSpec( ctx context.Context, r *servingv1.Route, tls []v1alpha1.IngressTLS, targets map[string]traffic.RevisionTargets, visibility map[string]netv1alpha1.IngressVisibility, acmeChallenges ...v1alpha1.HTTP01Challenge, ) (v1alpha1.IngressSpec, error) { ... // add custom external domains customHostStr := r.Annotations[\"serverless.kakuchuxing.com/domains\"] // 倒序，否则不生效，因为访问指定版本时name不为空，不区分版本时name默认为空 sort.Sort(sort.Reverse(sort.StringSlice(names))) if len(customHostStr) \u003e 0 { customHosts := strings.Split(customHostStr, \";\") for _, name := range names { if name != \"default\" { visibility := netv1alpha1.IngressVisibilityExternalIP rule := *makeIngressRule(customHosts, r.Namespace, visibility, name, targets[name]) // If this is a public rule, we need to configure ACME challenge paths. rule.HTTP.Paths = append( makeACMEIngressPaths(challengeHosts, customHosts), rule.HTTP.Paths...) rules = append(rules, rule) } } } ... } func makeIngressRule(domains []string, ns string, visibility netv1alpha1.IngressVisibility, name string, targets traffic.RevisionTargets) *v1alpha1.IngressRule { ... return \u0026v1alpha1.IngressRule{ Hosts: domains, Visibility: visibility, HTTP: \u0026v1alpha1.HTTPIngressRuleValue{ Paths: []v1alpha1.HTTPIngressPath{ { Splits: splits, // TODO(lichuqiang): #2201, plumbing to config timeout and retries. // 把tag name保存下来，传递给vs，用来区分是否需要设置header AppendHeaders: map[string]string{ \"RevisionName\": name, }, }, }, }, } } func makeVirtualServiceRoute(hosts sets.String, usn string, http *v1alpha1.HTTPIngressPath, gateways map[v1alpha1.IngressVisibility]sets.String, visibility v1alpha1.IngressVisibility) *istiov1alpha3.HTTPRoute { ... // add revision tag header to custom domain // 获取传递过来的tag，设置match header if tag := http.AppendHeaders[\"RevisionName\"]; tag != \"\" { for i := 0; i \u003c len(matches); i++ { if matches[i].Headers == nil { matches[i].Headers = make(map[string]*istiov1alpha3.StringMatch) } matches[i].Headers[\"RevisionName\"] = \u0026istiov1alpha3.StringMatch{ MatchType: \u0026istiov1alpha3.StringMatch_Exact{ Exact: http.Splits[0].ServiceName, }, } } } ... } ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/:0:2","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"总结 至此，已经实现了通过统一域名访问集群内服务，且根据Path转发请求，并且可以通过在访问时添加指定的header来把流量打到指定版本上，这在灰度或者测试时是一个非常实用的功能。 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-version/:0:3","tags":["knative"],"title":"Knative通过header访问指定版本","uri":"https://www.likakuli.com/posts/knative-version/"},{"categories":["定制开发"],"content":"Knative根据Path转发请求","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["定制开发"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 最近在搭建公司级的serverless平台，需要用到域名来访问内部服务，采取的是通过PATH来区分不同的服务，域名采用同一个。这与原生knative的设计存在差异，原生的做法是每个服务一个自己的域名，通过域名把流量打到不同的服务上，我们已经在上一篇中解决了自定义域名无法访问knative集群的问题，这一篇来解决如何通过不同的Path访问到不同的服务 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/:0:1","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["定制开发"],"content":"方案 两个问题需要我们来解决： 不同服务的Path可能相同，如何区分 原生通过ksvc的方式不支持设置Path（通过自己创建各种类型的资源可以实现，但是控制比较复杂，而且上层需要修改适配） 解决方案： 每个服务一个USN，使用USN作为唯一标识 修改knative，支持通过Path访问 转发后需要rewrite url，把USN去掉，因为业务代码中的路由里不可能包含USN 其中第一点不需要代码改动，我们主要来实现第二、三点。 vs本身是支持根据Path转发的功能的，但是并没有在ksvc中暴露出来，所以我们需要在king创建vs的时候动态注入进去，同时在destination中添加url rewrite的逻辑。 func makeVirtualServiceSpec(ing *v1alpha1.Ingress, gateways map[v1alpha1.IngressVisibility]sets.String, hosts sets.String) *istiov1alpha3.VirtualService { spec := istiov1alpha3.VirtualService{ Hosts: hosts.List(), } // 自定义功能 usn := ing.Annotations[\"serverless.kakuchuxing.com/usn\"] if usn != \"\" { usn = \"/\" + strings.Trim(usn, \"/\") + \"/\" } gw := sets.String{} for _, rule := range ing.Spec.Rules { for _, p := range rule.HTTP.Paths { hosts := hosts.Intersection(sets.NewString(rule.Hosts...)) if hosts.Len() != 0 { http := makeVirtualServiceRoute(hosts, usn, \u0026p, gateways, rule.Visibility) // Add all the Gateways that exist inside the http.match section of // the VirtualService. // This ensures that we are only using the Gateways that actually appear // in VirtualService routes. for _, m := range http.Match { gw = gw.Union(sets.NewString(m.Gateways...)) } // rewrite path，重定向，消除USN if usn != \"\" { if http.Rewrite == nil { http.Rewrite = \u0026istiov1alpha3.HTTPRewrite{} } http.Rewrite.Uri = \"/\" } spec.Http = append(spec.Http, http) } } } spec.Gateways = gw.List() return \u0026spec } func makeVirtualServiceRoute(hosts sets.String, usn string, http *v1alpha1.HTTPIngressPath, gateways map[v1alpha1.IngressVisibility]sets.String, visibility v1alpha1.IngressVisibility) *istiov1alpha3.HTTPRoute { ... for _, host := range hosts.List() { g := gateways[visibility] if strings.HasSuffix(host, clusterDomainName) \u0026\u0026 len(gateways[v1alpha1.IngressVisibilityClusterLocal]) \u003e 0 { // For local hostname, always use private gateway g = gateways[v1alpha1.IngressVisibilityClusterLocal] } matches = append(matches, makeMatch(host, usn, http.Path, g)...) } ... } func makeMatch(host string, usn string, pathRegExp string, gateways sets.String) []*istiov1alpha3.HTTPMatchRequest { ... // add custom usn filter，添加USN的过滤条件 if usn != \"\" { if i == 0 { matches[i].Uri = \u0026istiov1alpha3.StringMatch{ MatchType: \u0026istiov1alpha3.StringMatch_Prefix{Prefix: usn}, } } else { matches[i].Uri = \u0026istiov1alpha3.StringMatch{ MatchType: \u0026istiov1alpha3.StringMatch_Exact{Exact: strings.TrimRight(usn, \"/\")}, } } } ... } 修改比较简单，完全就是按照之前说的两点进行的。其中有一个比较tricky的地方就是实现url rewrite的方式，因为社区中的vs（istio里的crd）其实是存在问题的，我们为了规避这个问题，特意做了一些特殊设置。参考这里，大致意思就是目前vs不支持url rewrite为空，rewrite为空之后，实际访问的时候需要在url的最后加上/，否则会返回400，但是我们很多前端网站主页就是一个域名，后面不跟任何内容，那这时候就有问题了，总不能再告诉用户在最后输入一个/。规避方案其实也比较简单，就是上面代码中最后makeMatch处的if else语句，且一定要保证顺序，即最长的要在前面，因为遇到第一个匹配的规则后，后续规则会被忽略。 http: - match: - uri: prefix: \"/echo/\" - uri: prefix: \"/echo\" rewrite: uri: \"/\" 如果顺序颠倒，那么当访问/echo/abc时，会重定向到//abc，返回404错误。 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/:0:2","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["定制开发"],"content":"总结 至此，已经支持通过统一域名访问，且通过Path把请求转发到不通的服务 ","date":"2020-08-19","objectID":"https://www.likakuli.com/posts/knative-pathfilter/:0:3","tags":["knative"],"title":"Knative根据Path转发请求","uri":"https://www.likakuli.com/posts/knative-pathfilter/"},{"categories":["使用说明"],"content":"knative build docker image","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["使用说明"],"content":"背景 knative 0.14.0 最近在搭建公司级的serverless平台，遇到某些问题，看了源码发现无法通过其扩展机制来解决，遂决定修改源码来解决 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/:0:1","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["使用说明"],"content":"过程 源码很快修改完了，本地编译通过，knative的组件是容器化运行的，这就需要我们再制作镜像，但是浏览完官方github项目，并未发现有Dockerfile文件，于是决定使用逆向方法通过image反推出来Dockerfile，于是利用之前保存的的shell脚本进行反向解析，如下 docker history --no-trunc 8b13dd01e81b | tac | tr -s ' ' | cut -d \" \" -f 5- | sed 's,^/bin/sh -c #(nop) ,,g' | sed 's,^/bin/sh -c,RUN,g' | sed 's, \u0026\u0026 ,\\n \u0026 ,g' | sed 's,\\s*[0-9]*[\\.]*[0-9]*\\s*[kMG]*B\\s*$,,g' | head -n -1 bazel build ... ko publish knative.dev/net-istio/cmd/webhook 463kB kodata contents, at $KO_DATA_PATH ko publish knative.dev/net-istio/cmd/webhook 52.9MB go build output, at /ko-app/webhook WTF? 这和我认知里的Dockerfile完全不是一回事啊，赶紧google，首先google搜索了bazel，然后区项目中查看，并没有发现有啥相关的文件，倒是有个.ko.yaml的文件，里面有一条语句，是个镜像名称，然后google搜索了ko，果然，大公司就是不一样，一个ko解决了从diamante编译，打镜像，上传镜像，部署到k8s集群中的所有步骤（心中暗自感叹google是真的牛），当然也支持只把镜像load到本地，而不进行push，也不在k8s中创建，加个–local就好了。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/:0:2","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["使用说明"],"content":"总结 其实整个过程还是花了较长时间的，主要有两个原因 欠缺某些知识：这种情况下我们往往无法直接找到正确答案，只能通过踩坑之后逐步排除掉错误答案，才能一步步的找到正确的答案 knative比较新（0.14.0），网上很难找到需要的答案 整个过程虽然花费较多时间，但是收获颇丰。之所以写这一篇内容，也是希望为后来人解决一下此类烦恼，在比较紧急时，为大家节省时间，希望可以帮助到一部分人。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-build/:0:3","tags":["knative"],"title":"Knative组件镜像制作","uri":"https://www.likakuli.com/posts/knative-build/"},{"categories":["定制开发"],"content":"通过外部域名访问knative集群内的服务","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"背景 knative 0.14.0 实际修改可能与贴出来的代码不符，贴出来的代码只是为了方便快速实现功能 最近在搭建公司级的serverless平台，需要用到域名来访问内部服务，采取的是通过PATH来区分不同的服务 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:1","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"问题 申请完域名后，分别通过域名和IP:PORT形式访问已部署的helloworld服务 curl -v -H \"Host: api-test.sls.intra.kaku.com\" http://api-test.sls.intra.kaku.com/ * Trying 10.88.128.112... * TCP_NODELAY set * Connected to api-test.sls.intra.kaku.com (10.88.128.112) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host: api-test.sls.intra.kaku.com \u003e User-Agent: curl/7.54.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 426 Upgrade Required \u003c Date: Thu, 09 Jul 2020 11:59:20 GMT \u003c Content-Length: 0 \u003c Connection: keep-alive \u003c server: istio-envoy \u003c * Connection #0 to host api-test.sls.intra.kaku.com left intact # 10.190.16.26 为 knative-ingress-gateway的容器IP curl -v -H \"Host:api-test.sls.intra.kaku.com\" http://10.190.16.26/ * Trying 10.190.16.26... * TCP_NODELAY set * Connected to 10.190.16.26 (10.190.16.26) port 80 (#0) \u003e GET / HTTP/1.1 \u003e Host:api-test.sls.intra.kaku.com \u003e User-Agent: curl/7.54.0 \u003e Accept: */* \u003e \u003c HTTP/1.1 404 Not Found \u003c date: Thu, 09 Jul 2020 12:03:05 GMT \u003c server: istio-envoy \u003c content-length: 0 \u003c * Connection #0 to host 10.190.16.26 left intact 可以看到都无法正常返回，通过域名访问的时候返回了426，通过IP:PORT访问的时候返回了404。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:2","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"排查 426 Upgrade Required 这个问题直接google一搜就出来答案了，参考 这里，其实这是envoy的能力，只要在envoy运行的容器中设置ISTIO_META_HTTP10环境变量为\"1\"问题就解决了，即**ISTIO_META_HTTP10: '\"1\"'** 404 Not Found 这个问题就涉及到VirtualService了，简称vs，在介绍vs之前我们先大致过一下knative创建集群的流程 假设我们通过kubectl操作，此时我们通过kubectl create -f helloworld.yaml的方式创建ksvc服务，如果集群各组件正常工作，且ksvc内容正确，那么稍微过一会就可以在集群中看到我们的服务了，我们需要做的仅仅是执行一条命令而已。可以看到knative封装的太好了，极大的简化了用户操作，对于对集群没有高级需求的用户非常友好，同时也有利于我们快速入门，否则，如果要执行一堆命令的话，就真的可以从入门到放弃了 但是我们毕竟是管理员，还是要对自己提高要求的，一定要搞清楚里面的原理，各组件之间的交互，否则系统对于我们来说就完全是个黑盒，不出问题还好，出问题就傻眼了。了解源码也是必须的，说到源码，只能感叹knative的源码要比k8s的源码封装的好太多了，其中一个原因也使得益于k8s提供的丰富的扩展机制：crd、operator、informer、webhook等。knative的源码真应该也值得拿出来一起分享，仔细研读。 回到正题，网路路由能力我们选择的是istio，我们大致分两种类型的资源进行介绍，和网络有关的 vs 和网络无关的 和网络无关的资源创建流程 ksvc --\u003e configuration --\u003e revision --\u003e deployment 和网络有关的资源创建流程 ksvc --\u003e route --\u003e king--\u003e virtualservice 我们的问题是和网络有关的，所以重点关注下面这个流程，最终对接istio的是vs，于是我们直接去看vs的配置，发现和域名相关的有两个地方，spec.hosts 和 spec.http.match.authority，于是想到的最简单的修改方式就是把我们的域名加入到spec.hosts中，去掉spec.http.match.authority，通过看代码发现这两处并没有可以修改的机制，于是想到利用MutatingWebhook来实现修改 控制这两个属性的地方都在net-istio的controller中，webhook对应的是net-istio的webhook，按照上面的分析，我们需要在webhook中添加对应的代码，主要改动两个地方，如下 // 注册virtualservice类型，表示要对其进行mutate的操作，我们只需要在此注册即可，controller会自动修改对应的MutatingWebhookConfiguration，添加对应的资源和操作 var types = map[schema.GroupVersionKind]resourcesemantics.GenericCRD{ appsv1.SchemeGroupVersion.WithKind(\"Deployment\"): \u0026defaults.IstioDeployment{}, v1alpha3.SchemeGroupVersion.WithKind(\"VirtualService\"): \u0026defaults.IstioVirtualService{}, } // pkg/defaults/virtualservice_default.go，以去掉match.authority举例 package defaults import ( \"context\" \"istio.io/client-go/pkg/apis/networking/v1alpha3\" \"knative.dev/pkg/apis\" ) type IstioVirtualService struct { v1alpha3.VirtualService `json:\",inline\"` } func (vs *IstioVirtualService) Validate(context.Context) *apis.FieldError { return nil } func (vs *IstioVirtualService) SetDefaults(ctx context.Context) { for _, http := range vs.Spec.Http { for _, match := range http.Match { match.Authority = nil } } } 可以看到整体修改很少，修改完之后重新编译，制作镜像，修改线上Pod的Image，触发原地重启，然后删除掉原有的vs，新的vs自动生成，查看新的vs，wtf？ 居然和之前一样，没有实现我们的效果，查kube-apiserver日志没有看到在创建vs时调用webhook，查看webhook的日志，也没有发现调用，但是在创建deployment时却会调用，然后查看webhook的配置，发现资源里也已经加上了，查了好久还是没有找到原因，不知道是哪个姿势不对了，由于时间关系暂时换另一种方式实现。 因为vs是由king创建的，所以在创建king的地方修改，这样在king创建vs的时候会自动带上我们自定义的domains，如下 // 通过annotation的方式，把需要添加到hosts中的域名放到annotation中 // MakeIngressSpec creates a new IngressSpec func MakeIngressSpec( ctx context.Context, r *servingv1.Route, tls []v1alpha1.IngressTLS, targets map[string]traffic.RevisionTargets, visibility map[string]netv1alpha1.IngressVisibility, acmeChallenges ...v1alpha1.HTTP01Challenge, ) (v1alpha1.IngressSpec, error) { ... // add custom external domains customHostStr := r.Annotations[\"serverless.kakuchuxing.com/domains\"] sort.Sort(sort.Reverse(sort.StringSlice(names))) if len(customHostStr) \u003e 0 { customHosts := strings.Split(customHostStr, \";\") for _, name := range names { if name != \"default\" { visibility := netv1alpha1.IngressVisibilityExternalIP rule := *makeIngressRule(customHosts, r.Namespace, visibility, name, targets[name]) // If this is a public rule, we need to configure ACME challenge paths. rule.HTTP.Paths = append( makeACMEIngressPaths(challengeHosts, customHosts), rule.HTTP.Paths...) rules = append(rules, rule) } } } ... } 首先修改ksvc，添加对应的annotaiton，然后继续之前的操作进行编译，打镜像，原地升级，删除vs，新的vs自送生成，此时可以看到已经使我们期望的效果了，然后用域名访问，HelloWorld终于可以正常访问了。 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:3","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["定制开发"],"content":"总结 问题是解决了，但是为什么通过webhook的方式不生效，现象看起来是没调用webhook，还需要再去看下k8s有关webhook调用的部分的代码，很可能又是一个知识盲区。 knative中很多类型的属性并没有在上层暴露，导致无法直接使用ksvc进行管理，要么改源码，要么自己负责管理原本由ksvc统一管理的组件，虽然更加灵活，但是使用成本也更高，违背ksvc设计的初衷 通过此次问题排查，学习到了knative整个流程、原理，理清了各组件的交互，对后续问题排查有很大的帮助 ","date":"2020-07-09","objectID":"https://www.likakuli.com/posts/knative-ingress-gateway/:0:4","tags":["knative"],"title":"Knative通过外部域名访问集群内服务","uri":"https://www.likakuli.com/posts/knative-ingress-gateway/"},{"categories":["问题排查"],"content":"endpoint变化","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"背景 k8s 1.12.4 包含自定义功能 线上集群在批量原地升级时出现流量异常问题，大体流程如下： 批量摘流，并等待7秒 批量删除容器 watch到Endpoint ready 变化，汇总2s内的变化，摘流或者接流（通用的处理方式，幂等） 原地升级是靠修改image实现的，利用的就是k8s原生的能力。第三步中为了降低对第三方API的访问次数，等待2s，汇总2s内所有变化统一调用一次API来进行摘流或者接流。问题表现为上述过程中容器先摘流，再接流（异常），再摘流，最后再接流，期望的场景是容器摘流，完后等待容器重启，正常之后再接流。 ","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/:0:1","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"分析 近期上线了原地重建的功能，出问题的集群都是使用此功能进行发布更新，所以猜测可能和这个功能有关系。在删除集群或者批量漂移容器时，也涉及对应流程，但是一直没有问题，总的排查方向如下： endpoint 变化机制 为什么批量删除时没有出现问题 原地升级和删除有什么差异 Endpoint变化机制 众所周知，k8s针对不同的资源类型会有相应的controller与之对应，控制其及其关联资源的生命周期的变化，Endpoint也不例外，在kube-controller-manager中有endpoint controller，查看其逻辑，主要相关的部分如下所示 func (e *EndpointController) syncService(key string) error { ... subsets := []v1.EndpointSubset{} var totalReadyEps int = 0 var totalNotReadyEps int = 0 for _, pod := range pods { if len(pod.Status.PodIP) == 0 { glog.V(5).Infof(\"Failed to find an IP for pod %s/%s\", pod.Namespace, pod.Name) continue } if !tolerateUnreadyEndpoints \u0026\u0026 pod.DeletionTimestamp != nil { glog.V(5).Infof(\"Pod is being deleted %s/%s\", pod.Namespace, pod.Name) continue } epa := *podToEndpointAddress(pod) hostname := pod.Spec.Hostname if len(hostname) \u003e 0 \u0026\u0026 pod.Spec.Subdomain == service.Name \u0026\u0026 service.Namespace == pod.Namespace { epa.Hostname = hostname } // Allow headless service not to have ports. if len(service.Spec.Ports) == 0 { if service.Spec.ClusterIP == api.ClusterIPNone { subsets, totalReadyEps, totalNotReadyEps = addEndpointSubset(subsets, pod, epa, nil, tolerateUnreadyEndpoints) // No need to repack subsets for headless service without ports. } } else { for i := range service.Spec.Ports { servicePort := \u0026service.Spec.Ports[i] portName := servicePort.Name portProto := servicePort.Protocol portNum, err := podutil.FindPort(pod, servicePort) if err != nil { glog.V(4).Infof(\"Failed to find port for service %s/%s: %v\", service.Namespace, service.Name, err) continue } var readyEps, notReadyEps int epp := \u0026v1.EndpointPort{Name: portName, Port: int32(portNum), Protocol: portProto} subsets, readyEps, notReadyEps = addEndpointSubset(subsets, pod, epa, epp, tolerateUnreadyEndpoints) totalReadyEps = totalReadyEps + readyEps totalNotReadyEps = totalNotReadyEps + notReadyEps } } } ... glog.V(4).Infof(\"Update endpoints for %v/%v, ready: %d not ready: %d\", service.Namespace, service.Name, totalReadyEps, totalNotReadyEps) ... } 主要的处理函数为syncService，去掉了一些逻辑，主要的处理逻辑在32行，遍历Pod，查看其PodReady Condition是否为true，true的会会把其IP放入subnet的Addresses结构中，否则放入NotReadyAddresses中。Condition主要是kubelet设置的，在generateAPIPodStatus的时候会进行设置，如下 // generateAPIPodStatus creates the final API pod status for a pod, given the // internal pod status. func (kl *Kubelet) generateAPIPodStatus(pod *v1.Pod, podStatus *kubecontainer.PodStatus) v1.PodStatus { glog.V(3).Infof(\"Generating status for %q\", format.Pod(pod)) // check if an internal module has requested the pod is evicted. for _, podSyncHandler := range kl.PodSyncHandlers { if result := podSyncHandler.ShouldEvict(pod); result.Evict { return v1.PodStatus{ Phase: v1.PodFailed, Reason: result.Reason, Message: result.Message, } } } s := kl.convertStatusToAPIStatus(pod, podStatus) // Assume info is ready to process spec := \u0026pod.Spec allStatus := append(append([]v1.ContainerStatus{}, s.ContainerStatuses...), s.InitContainerStatuses...) s.Phase = getPhase(spec, allStatus) // Check for illegal phase transition if pod.Status.Phase == v1.PodFailed || pod.Status.Phase == v1.PodSucceeded { // API server shows terminal phase; transitions are not allowed if s.Phase != pod.Status.Phase { glog.Errorf(\"Pod attempted illegal phase transition from %s to %s: %v\", pod.Status.Phase, s.Phase, s) // Force back to phase from the API server s.Phase = pod.Status.Phase } } kl.probeManager.UpdatePodStatus(pod.UID, s) s.Conditions = append(s.Conditions, status.GeneratePodInitializedCondition(spec, s.InitContainerStatuses, s.Phase)) s.Conditions = append(s.Conditions, status.GeneratePodReadyCondition(spec, s.Conditions, s.ContainerStatuses, s.Phase)) s.Conditions = append(s.Conditions, status.GenerateContainersReadyCondition(spec, s.ContainerStatuses, s.Phase)) // Status manager will take care of the LastTransitionTimestamp, either preserve // the timestamp from apiserver, or set a new one. When kubelet sees the pod, // `PodScheduled` condition must be true. s.Conditions = append(s.Conditions, v1.PodCon","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/:0:2","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"修改方案 通过mutatingwebhook实现一个通用的能力，针对endpoint的create和update事件，从配置中心（内部组件）中获取对应的配置，并通过规则引擎（开源版本可参考 https://github.com/antonmedv/expr ），对subnet中的Addresses和NotReadyAddresses做一些修改，这样可以实现无侵入式的修改，也比较灵活，可以对配置进行实时修改等，后续像sidecar这种根据用户需求来设置pod ready condition的情况，也无需修改代码，只需要添加配置即可，而且也可以通过condition看到真实的Container、Pod状态 ","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-ep-event/:0:3","tags":["kubernetes"],"title":"Endpoint异常变化","uri":"https://www.likakuli.com/posts/kubernetes-ep-event/"},{"categories":["问题排查"],"content":"优雅退出","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/","tags":["kubernetes"],"title":"Sidecar优雅退出","uri":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/"},{"categories":["问题排查"],"content":"背景 codis集群在接入弹性云测试时发现容器漂移失败，通过集群日志看，提示 调度超时，去界面查看，已经调度成功了（调度成功的标志就是已经有宿主机IP了），状态显示的pending并不一定就是调度失败。但这反应不出来问题出在哪里，接下来就需要到master机器上执行命名，查看日志来分析问题出在哪里 首先，我们要确定是不是调度超时了，可以直接通过 kubectl describe po kirovpre-krds-sf-f3dec-0 来看Pod的创建时间为 20:40:11，调度成功的时间为20:40:12，可以看到调度还是很快的。再看下相关日志，显示20:39:38删除成功，然后等待容器调度，等待30s后发现容器未调度成功，则判定为调度超时。简单梳理下时间线 20:39:38 删除成功 （kube-odin日志） 20:40:11 容器创建成功 （k8s） 20:40:12 容器调度成功 (k8s) 从时间线来看，确实从删除到调度成功耗时超过了30s，但是从容器创建出来到调度成功才1s，大部分耗时是在删除到新创建的阶段。熟悉k8s的同学应该知道，删除Pod的api支持设置删除方式：backgroud、foregroud，区别就是后台删除是异步的，调用api后立马返回，前台删除的话是同步的，会一直等容器真正删除后才返回，默认为backgroud。也就是说20:39:38提示的删除成功只是api调用成功，容器并未真正的删除，容器的删除操作一直在后台执行，直到20:40:11才删除成功，删除后立马创建新的容器。可以通过kubelet的日志证明这一点，下面的日志是经过筛选后的 I0603 20:39:37.908557 3033 kubelet.go:1913] SyncLoop (DELETE, \"api\"): \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" I0603 20:39:37.908655 3033 kubelet_pods.go:1433] Generating status for \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" I0603 20:39:37.908799 3033 kuberuntime_container.go:468] Running preStop hook for container \"docker://5fe57cf36af267adae571272f234762ad8741922e24074182ff25301e953ec72\" I0603 20:39:37.916683 3033 status_manager.go:499] Patch status for pod \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" with \"{}\" I0603 20:39:37.916699 3033 status_manager.go:506] Status for pod \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" updated successfully: (6, {Phase:Running Conditions:[{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:09 +0800 CST Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:26 +0800 CST Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:26 +0800 CST Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2020-05-29 15:07:08 +0800 CST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.89.231.24 PodIP:10.169.92.60 StartTime:2020-05-29 15:07:09 +0800 CST InitContainerStatuses:[] ContainerStatuses:[{Name:agent-kirovpre-krds-ys02 State:{Waiting:nil Running:\u0026ContainerStateRunning{StartedAt:2020-05-29 15:07:18 +0800 CST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.kaku.com/kakuonline/kvstore-sidecar-python2-centos7-agent:ea6d410 ImageID:docker-pullable://registry.kaku.com/kakuonline/kvstore-sidecar-python2-centos7-agent@sha256:6d61df206fef0fbfd940e1139d7dff6b0dfaacd847d8d64b2b480cf5afd8a513 ContainerID:docker://5fe57cf36af267adae571272f234762ad8741922e24074182ff25301e953ec72} {Name:kirovpre-krds-ys02 State:{Waiting:nil Running:\u0026ContainerStateRunning{StartedAt:2020-05-29 15:07:22 +0800 CST,} Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:true RestartCount:0 Image:registry.kaku.com/kakuonline/kvstore-sidecar-krds:stable ImageID:docker-pullable://registry.kaku.com/kakuonline/kvstore-sidecar-krds@sha256:3945a5304ee92b89605831d991874d0b9fdffb89b19fc92d5bbc4969b8a3cf1f ContainerID:docker://2e2354889588dc7483d2bb9be27a5253f292374c8e179b12367e0deea8b2d825}] QOSClass:Burstable}) I0603 20:39:37.916795 3033 kubelet_pods.go:993] Pod \"kirovpre-krds-sf-f3dec-0_default(01473fb7-a17b-11ea-8d10-c88d83d31d55)\" is terminated, but some containers are still running I0603 20:39:40.975805 3033 kuberuntime_container.go:485] preStop hook for container {\"docker\" \"5fe57cf36af267adae571272f234762ad8741922e24074182ff25301e953ec72\"} completed I0603 20:39:40.975845 3033 kuberuntime_container.go:563] Killing container \"docker://2e2354889588dc7483d2bb9be27a5253f292374c8e179b12367e0deea8b2d825\" with 5 second grace period I0603 20:39:40.975856 3033 kuberuntime_container.go:468] Running preStop hook for container \"docker://2e2354889588dc7483d2bb9be27a5253f292374c8e179b123","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/:0:1","tags":["kubernetes"],"title":"Sidecar优雅退出","uri":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/"},{"categories":["问题排查"],"content":"总结 明白了问题出在哪里，修改其实就很简单了，这里不再多说如何修改。 此次问题排查使我深刻的认识到了一点：一些看起来很容易理解的东西可能正是我们思维定式的误区，越简单的东西我们往往越容易先入为主，不再进行深入思考。而很多问题往往就是这些不起眼的细节日积月累导致的，我们要尽可能的对我们所用到的东西有深刻全面的理解，降低出问题的概率，毕竟线上无小事。有精力的话还是要看看源码，比如 max（graceperiod - cost, 2）,意思就是会为容器至少设置2s的时间，超过后才能会进行强删；再比如外面不传graceperiod时graceperiod用pod的terminationGracePeriod，传的话就是传进来的参数，这就可能导致出现负数的情况，导致不会强删，terminationGracePeriod失效等。 ","date":"2020-06-23","objectID":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/:0:2","tags":["kubernetes"],"title":"Sidecar优雅退出","uri":"https://www.likakuli.com/posts/kubernetes-graceful-shutdown/"},{"categories":["bug修复","问题排查","性能优化"],"content":"orphan controller revision cannot be adopt","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"背景 版本1.12.4 线上遇到kube-controller-manager重启慢的问题，具体表现为进程重启虽然速度快，但是重启完所有数据都同步完一遍耗时很长，集群中大约5000个statefulset，在还没同步完一遍数据之前如果有statefulset的创建、删除、修改等操作，可能（和具体statefulset的操作有关，新建的情况肯定是在最后，更新和删除的情况需要看同名的statefulset是否已经被处理过了，如果是的话也会在最后处理，如果没有的话，则不会排在最后）就需要等到所有数据都同步完之后才能继续处理。 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:1","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"问题原因 并发goroutine数 当前版本的statefulset controller只使用了一个goroutine来串行的处理所有的statefulset，在最新版的代码中已经支持了多个goroutine并行处理，且可配置 Informer List 经过添加trace信息打印每个阶段耗时，发现在根据statefulset获取其对应的controllerrevision时比较慢，如下代码段 // adoptOrphanRevisions adopts any orphaned ControllerRevisions matched by set's Selector. func (ssc *StatefulSetController) adoptOrphanRevisions(set *apps.StatefulSet) error { // 比较慢 revisions, err := ssc.control.ListRevisions(set) ... } 而且同样的逻辑会在这里在执行一次 // syncStatefulSet syncs a tuple of (statefulset, []*v1.Pod). func (ssc *StatefulSetController) syncStatefulSet(set *apps.StatefulSet, pods []*v1.Pod) error { ... // TODO: investigate where we mutate the set during the update as it is not obvious. // UpdateStatefulSet里面会再次调用ListRevisions函数 if err := ssc.control.UpdateStatefulSet(set.DeepCopy(), pods); err != nil { return err } ... } 最终就导致同步一个statefulset就需要100+ms，总的5000+ statefulset同步完一遍就需要将近20m。 ControllerRevision 每个statefulset都对应一些controllerrevision资源，从字面意思就可以看出来其作用就是记录此statefulset的历史信息，这也是为什么我们可以直接对statefulset做回滚之类的操作的原因，默认情况下会为每个statefulset保留10条历史记录，在每个statefulset上有属性可配置。 其实上面说到的耗时的逻辑就是针对每个statefulset去获取孤儿controllerrevision，如果有则会领养孤儿。所以一个优化项就是直接从孤儿中找，而不是从全量中找，且把所有的controllerrevision缓存到本地，不再使用ListInformer提供的那些方法，因为这些方法始终会在全量中寻找满足条件的，而且还会用到反射，虽然数据也都在本地，但性能还是比较差的。大体思路就是在创建statesetfulset controller时同时注册controllerrevision相关的事件，把所有的revision和孤儿revision缓存到自定义的数据结构中，后续直接从里面获取即可。 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:2","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"最终效果 优化完之后最终重启一次controller-manager知道全量数据同步完一遍的耗时由20m左右缩减到1m左右，可以看到效果还是很明显的，而且还是有优化空间的，比如继续以空间换时间，在备controller-manager启动时就先把所有的数据同步完，所有更新缓存的逻辑照样执行，只是不触发其他操作，这样在主备切换时就能省掉网络传输数据的耗时，当然得衡量数据量大小，随着集群规模越来越大，master上各组件占用的内存势必越来越多，将来可能就又会面临内存不够用的情况。 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:3","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["bug修复","问题排查","性能优化"],"content":"意外收获 测试的时候发现了一个bug，孤儿controllerrevision无法被领养，且会导致statefulset同步失败，这是statefulset controller的代码bug，pr已合入master，随着v1.18版本发布。具体可参考这里 ","date":"2020-04-01","objectID":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/:0:4","tags":["kubernetes"],"title":"Kube-controller-manager同步数据慢","uri":"https://www.likakuli.com/posts/kubernetes-controllerrevisionhistory-bug/"},{"categories":["问题排查"],"content":"flannel启动提示key not found","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"问题描述 etcd 3.3.1 flannel 0.11.0 flannel启动时报错，启动参数如下 ./flannel -etcd-keyfile=/etc/kubernetes/ssl/etcd-client-key.pem -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-endpoints=https://ip:port -etcd-certfile=/etc/kubernetes/ssl/etcd-client.pem -etcd-prefix=/coreos.com/network 错误信息如下： E0908 20:27:17.671602 2331 main.go:382] Couldn't fetch network config: 100: Key not found (/coreos.com) [22] timed out E0908 20:27:18.680096 2331 main.go:382] Couldn't fetch network config: 100: Key not found (/coreos.com) [22] timed out E0908 20:27:19.688339 2331 main.go:382] Couldn't fetch network config: 100: Key not found (/coreos.com) [22] 其中coreos.com是启动flannel时-etcd-prefix参数的默认值（/coreos.com/network） ","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/:0:1","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"解决办法 报错提示很明显，没有对应的key，于是执行etcdctl的命令插入对应的key并设置其值 ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd-client.pem --key=/etc/kubernetes/ssl/etcd-client-key.pem --endpoints=https://ip:port put /coreos.com/network/config '{\"Network\":\"192.168.0.0/16\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"vxlan\"}}' OK 重新启动flannel，依旧报错，执行etcdctl get获取key的信息也可以正常拿到之前的设置，一脸懵逼。网上搜了下说是etcd api版本的问题，不是很明白，然后去看代码，发现flannel在使用etcd时只支持etcd v2版本的api，因为上线添加key-value时使用的是v3版本的api，所以导致虽然添加成功了，但是用v2获取的时候还是失败，解决办法就是用v2版本的api添加一遍即可 etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd-client.pem --key=/etc/kubernetes/ssl/etcd-client-key.pem --endpoints=https://ip:port set /coreos.com/network/config '{\"Network\":\"192.168.0.0/16\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"vxlan\"}}' 区别就是去掉设置v3的环境变量，put改为set，需要注意一下，在master最新代码中，不设置ETCDCTL_API就默认用v3版本的api，后续使用时还需要具体版本具体对待。 ","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/:0:2","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"收获 etcd不同版本的api对应的url path的prefix不同，v2前缀为/v2/keys，v3前缀为/v3[alpha|beta]/kv，用法也不同，具体可以参考官网API说明。平时直接使用client包时这些信息都会忽略掉，封装的太好了会使使用者变傻，还是有必要看看源码是怎么实现的。 ","date":"2019-12-16","objectID":"https://www.likakuli.com/posts/kubernetes-flannel/:0:3","tags":["kubernetes"],"title":"Flannel key not found","uri":"https://www.likakuli.com/posts/kubernetes-flannel/"},{"categories":["问题排查"],"content":"背景 线上master的apiserver组件内存报警，内存使用量持续增长，监控如下 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:1","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["问题排查"],"content":"排查过程 从监控上看和另外一个程序（管理员平台）的内存使用情况吻合，使用率降下来是因为重启了apiserver和管理员平台，且问题只出现在最近两天的晚上，管理员平台中有一段逻辑是定时全量拉取集群数据（设计不合理，后续需要改），管理员平台的日志里显示拉取数据超时，基本猜测和管理员平台调用k8s api不合理有关，且k8s apiserver应该也有bug，导致内存泄露或者goroutine泄露。但是最近代码都没动过，为啥之前没事呢，后负责管理员平台的同事说近两天美东专线有问题，延迟是之前的3倍，而且出现问题的时间正好匹配，那接下来就查一下具体原因。 查看apiserver日志 apiserver错误日志里有大量的上述日志，可以看到是apiserver因为响应超时触发的，里面也有详细的函数调用堆栈信息，也有ip的信息，正好对应了master和管理员平台的地址，通过pprof也可以看到此时的goroutine使用量一直在增加，已45000+，确认是产生了goroutine泄露。下图为pprof tree看到的部分内容，里面显示了占用量最多的地方 同时在浏览器中访问http://ip:port/debug/pprof/goroutine 可以看到具体goroutine数量和执行函数的行号，此处忘记截图了，不过和上面的信息吻合，且更信息因为携带了行号的信息，可以看到是如下代码出的问题（代码版本1.12.4） // k8s.io/kubernetes/vendor/k8s.io/apiserver/pkg/server/filters/timeout.go func (t *timeoutHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) { r, after, postTimeoutFn, err := t.timeout(r) if after == nil { t.handler.ServeHTTP(w, r) return } errCh := make(chan interface{}) tw := newTimeoutWriter(w) go func() { defer func() { err := recover() if err != nil { const size = 64 \u003c\u003c 10 buf := make([]byte, size) buf = buf[:runtime.Stack(buf, false)] err = fmt.Sprintf(\"%v\\n%s\", err, buf) } errCh \u003c- err }() t.handler.ServeHTTP(tw, r) }() select { case err := \u003c-errCh: if err != nil { panic(err) } return case \u003c-after: postTimeoutFn() tw.timeout(err) } } 泄露的goroutine就是第11行处的，简单解释一下上面的逻辑：生成一个timeout的handler，起一个新的goroutine进行后续handler的处理，当前goroutine中使用select进行等待，分为两种case，分别对应新goroutine中panic的情况和整个函数超时的情况，分别看两个case的内容 第25行：从errCh读取数据，其中errCh中的数据是在新的goroutine中产生的，对应到实际情况就是22行的代码出发生了panic，在13行捕获到了，最后在20行把err写入到errCh中，但是这里需要注意一下这个errCh是个无缓存的； 第30行：after是调用time.After后产生的一个chan，在超时后可以从这个chan中获取到数据，然后在32行处会调用tw.timeout函数，里面会触发panic； 那为什么11行处的goroutine泄露了呢？ 问题就出现在了刚才提到的无缓冲的errCh上，因为触发了timeout，代码逻辑没有执行到25行，直接去了30行，然后整个函数panic，导致20行执行的时候卡住了，从而阻止了11行出的新的goroutine的退出，每有一个timeout的请求，这里就会泄露一个goroutine，从而导致内存随之泄露，cpu的话其实不受什么影响，因为泄露的goroutine已经执行过gopark，不是runnable状态的。 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:2","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["问题排查"],"content":"解决方案 印象中记得之前看k8s版本升级的release-note时有提到过修复apiserver leak字样的信息，然后就去官方项目中查，结果没找到，然后直接去看了对应文件的最新版本代码，看history，终于找到了相关的修复的commit，合入1.17。 case \u003c-after: defer func() { // resultCh needs to have a reader, since the function doing // the work needs to send to it. This is defer'd to ensure it runs // ever if the post timeout work itself panics. go func() { res := \u003c-resultCh if res != nil { switch t := res.(type) { case error: utilruntime.HandleError(t) default: utilruntime.HandleError(fmt.Errorf(\"%v\", res)) } } }() }() postTimeoutFn() tw.timeout(err) } 可以看到其思想就是在外层panic后，新加一个defer func用来从之前的errCh（新版改名为resultCh）接收数据，从而避免之前的问题。 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:3","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["问题排查"],"content":"总结 通过上面的修改，确实可以解决goroutine泄露的问题，但是也存在一个隐患：第6行新加的goroutine会从resultCh读数据，因为在上一段代码处有个处理，无论是否panic，都会往errCh（resultCh）写入err，从而可以避免同时泄露两个goroutine的情况，但是如果短时间内大量请求到来且处理时间都比较慢直至超时，虽然goroutine不会泄露，但是会产生两倍于之前的goroutine，可能会在短时间内造成内存暴涨，也算是一个稳定性风险，需要合理设置限流来降低风险。 ","date":"2019-12-06","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/:0:4","tags":["kubernetes"],"title":"Kube-apiserver goroutine leak","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-goroutine-leak/"},{"categories":["算法"],"content":"背包问题","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"最近的工作都跟集群调度有关，一直在为了满足用户需求添加各种调度策略，现在也暂时告一段落了，抽时间总结思考了之前的工作，调度本质上就是背包问题，但是相当复杂，涉及到多维多重背包、组合背包、依赖背包等。又重新开始学习背包问题，这里先从简单的01背包开始讲，在网上也找到了很多相关的文章，但是很遗憾，我找到的很多关于用golang实现背包的文章中给出的代码都是有问题的，后决定自己写出来，也希望大家一起思考，不要被误导了。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:0","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"题目 有有N件物品和一个容量为V的背包。第i件物品的费用是c[i]，价值是w[i]。求解将哪些物品装入背包可使价值总和最大。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:1","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"基本思想 这是最基础的背包问题，特点是：每种物品仅有一件，可以选择放或不放。 用子问题定义状态：即f[i][v]表示前i件物品恰放入一个容量为v的背包可以获得的最大价值。则其状态转移方程便是： f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]+w[i]} 这个方程非常重要，基本上所有跟背包相关的问题的方程都是由它衍生出来的。所以有必要将它详细解释一下：“将前i件物品放入容量为v的背包中”这个子问题，若只考虑第i件物品的策略（放或不放），那么就可以转化为一个只牵扯前i-1件物品的问题。如果不放第i件物品，那么问题就转化为“前i-1件物品放入容量为v的背包中”，价值为f[i-1][v]；如果放第i件物品，那么问题就转化为“前i-1件物品放入剩下的容量为v-c[i]的背包中”，此时能获得的最大价值就是f[i-1][v-c[i]]再加上通过放入第i件物品获得的价值w[i]。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:2","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["算法"],"content":"代码实现 测试用例 func main() { v := []int{7, 5, 8} //物品大小 w := []int{2, 3, 4} //物品价值 goods := [][]int{ []int{7, 2}, []int{5, 3}, []int{8, 4}, } fmt.Println(zeroonepack1(v, w, 5)) fmt.Println(zeroonepack(goods, 5)) fmt.Println(zeroonepack2(v, w, 5)) } // 结果应该都是3才对 二维数组实现 // 0 消耗 1 价值 // i 物品数 j 容量 // dp[i][j] = max(dp[i-1][j], goods[i][1] + dp[i-1][j-goods[i][0]]) func zeroonepack(goods [][]int, capacity int) int { num := len(goods) dp := make([][]int, num) for i := 0; i \u003c num; i++ { dp[i] = make([]int, capacity+1) } for i := goods[0][0]; i \u003c capacity+1; i++ { dp[0][i] = goods[0][1] } for i := 1; i \u003c num; i++ { // j不能直接从good[i][0]开始 for j := 0; j \u003c= capacity; j++ { // 此if else判断必须要有 if j \u003e= goods[i][0] { dp[i][j] = max(dp[i-1][j], dp[i-1][j-goods[i][0]]+goods[i][1]) } else { dp[i][j] = dp[i-1][j] } } } return dp[num-1][capacity] } func zeroonepack1(weight []int, value []int, capacity int) int { num := len(weight) dp := make([][]int, num) for i := 0; i \u003c num; i++ { dp[i] = make([]int, capacity+1) } for i := weight[0]; i \u003c= capacity; i++ { dp[0][i] = value[0] } for i := 1; i \u003c num; i++ { for j := 0; j \u003c= capacity; j++ { if j \u003e= weight[i] { dp[i][j] = max(dp[i-1][j], dp[i-1][j-weight[i]]+value[i]) } else { dp[i][j] = dp[i-1][j] } } } return dp[num-1][capacity] } 上面都是二维数组的实现，空间复杂度是O(VN)，区别就是传入的参数不同，用二维数组表示物品，或者用两个一维数组分别表示物品的消耗（重量或体积等）和价值，特别注意上面的注释，很多golang版本的代码就是那里出的问题，错误版本的代码参考这里 一维数组实现 以上方法的时间和空间复杂度均为O(VN)，其中时间复杂度应该已经不能再优化了，但空间复杂度却可以优化到O。 先考虑上面讲的基本思路如何实现，肯定是有一个主循环i=1..N，每次算出来二维数组f[i][0..V]的所有值。那么，如果只用一个数组f[0..V]，能不能保证第i次循环结束后f[v]中表示的就是我们定义的状态f[i][v]呢？f[i][v]是由f[i-1][v]和f[i-1][v-c[i]]两个子问题递推而来，能否保证在推f[i][v]时（也即在第i次主循环中推f[v]时）能够得到f[i-1][v]和f[i-1][v-c[i]]的值呢？事实上，这要求在每次主循环中我们以v=V..0的顺序推f[v]，这样才能保证推f[v]时f[v-c[i]]保存的是状态f[i-1][v-c[i]]的值。代码如下： func zeroonepack2(weight []int, value []int, capacity int) int { num := len(weight) dp := make([]int, capacity+1) for i := weight[0]; i \u003c capacity; i++ { dp[i] = value[0] } for i := 1; i \u003c num; i++ { for j := capacity; j \u003e= 0; j-- { if j \u003e= weight[i] { dp[j] = max(dp[j], dp[j-weight[i]]+value[i]) } } } return dp[capacity] } 其中的f[v]=max{f[v],f[v-c[i]]}一句恰就相当于我们的转移方程f[i][v]=max{f[i-1][v],f[i-1][v-c[i]]}，因为现在的f[v-c[i]]就相当于原来的f[i-1][v-c[i]]。如果将v的循环顺序从上面的逆序改成顺序的话，那么则成了f[i][v]由f[i][v-c[i]]推知，与本题意不符，但它却是另一个重要的背包问题最简捷的解决方案，故学习只用一维数组解01背包问题是十分必要的。 ","date":"2019-11-07","objectID":"https://www.likakuli.com/posts/backpack/:0:3","tags":["动态规划"],"title":"背包问题golang","uri":"https://www.likakuli.com/posts/backpack/"},{"categories":["源码分析"],"content":"kube-apiserver watch实现","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["源码分析"],"content":"List-Watch是kubernetes的核心机制。组件kubelet、kube-controller-manager、kube-scheduler需要监控各种资源(pod、service等)的变化，当这些对象发生变化时(add、delete、update)，kube-apiserver会主动通知这些组件。这个过程类似一个发布-订阅系统。本文章将从代码角度探究一下list-watch的实现方式。 转载自https://zhuanlan.zhihu.com/p/33335726，有修改 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/:0:0","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["源码分析"],"content":"**第一部分：**kube-apiserver对etcd的List-watch机制 流程示意图 构建PodStorage kube-apiserver针对每一类资源(pod、service、endpoint、replication controller、depolyments)都会构建Storage对象，如:PodStorage； PodStorage.Pod.Store封装了对etcd的操作； store.CompleteWithOptions会调用etcdOptions.GetRESTOptions，此方法将 调用generic.UndecoratedStorage创建无cache的Store； 或者调用genericregistry.StorageWithCacher创建带Cache的Store； StorageWithCacher 调用NewCacherFromConfig，将创建Cacher对象； 创建Cacher 首先，创建watchCache对象和cacheListerWatcher对象，cacheListWatcher对象是ListerWatcher接口实现，实现了List()和Watch()方法； 构建Cacher对象，主要的数据成员：watchCache、reflector、watchers及incoming channel； watchCache是一个cache，用来存储apiserver从etcd那里watch到的对象； watchers是一个map，map的值类型为cacheWatcher，当kubelet、kube-scheduler需要watch某类资源时，他们会向kube-apiserver发起watch请求，kube-apiserver就会生成一个cacheWatcher，cacheWatcher负责将watch的资源通过http从apiserver传递到kubelet、kube-scheduler； Reflector对象，主要数据成员：ListerWatcher，ListerWatcher是接口对象，包括方法List()和Watch()；listerWatcher包装了Storage，主要是将watch到的对象存到watchCache中； incoming channel接收watchCacheEvent； 协程调用cacher.dispatchEvents，watchCache将incoming channel接收watchCacheEvent添加到watchers的inputChan中； 协程调用cacher.startCaching; StartCaching 执行cacheListerWatcher的List方法和Watch方法； 调用reflector的watchHandler方法； cacheListWatcher.List／cacheListWatcher.Watch List方法将调用storage.List方法，这里是etcdHelper.List方法； Watch方法将调用storage.watch方法，这里是etcdHelper.WatchList方法； etcdHelper.List／etcdHelper.Watch etcdHelper对象是Storage接口对象的实现; etcdHelper的List方法： 获取etcd的对象（包括resourceVersion信息）； etcdHelper的WatchList方法： 创建etcdWatcher； etcdWatcher对象，实现了Watch接口； etcdWatcher对象，主要的数据成员是etcdIncoming channel和outgoing channel； 协程执行etcdWatcher.translate； 最后，协程运行etcdWatcher.etcdWatch； etcdWatcher.etcdWatch 如果resourceVersion==0, 运行etcdGetInitialWatchState(),获取所有的pods，并将结果输入到etcdIncoming channel; 之后，不停的调用watcher.Next()，并将结果输入到etcdIncoming channel; etcdWatcher.translate 读取etcdIncoming channel信息； 调用etcdWatcher.sendResult进行转化; 发送到outgoing channel； reflector.watchHandler 读取outgoing channel信息，操作watchCache； 操作watchCache 处理事件watchCache.processEvent 创建watchCacheEvent 调用watchCache.updateCache，更新watchCache; 到此分析完kube-apiserver对etcd的watch机制，除此之外，kube-apiserver会向其他组件提供watch接口，下面将分析kube-apiserver的watch API。 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/:1:0","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["源码分析"],"content":"第二部分：kube-apiserver的watch restful API kube-apiserver提供watch restful API给其他组件(kubelet、kube-controller-manager、kube-scheduler、kube-proxy)。watch restful API的处理流程和PUT、DELETE、GET等REST API处理流程类似。 流程示意图 registerResourceHandlers ListResource 1.调用rw.watch方法，这里将会调用Store.watch； 2.调用serveWatch方法； Store.watch 调用Storage.Watch方法和Storage.WatchList方法，这里将调用Cacher.watch方法和Cacher.WatchList方法 Cacher.watch watch方法中将调用newCacheWatcher； newCacheWatcher方法： 生成一个watcher，并将watcher插入到cacher.watchers中； 协程调用cacheWatcher.process方法，此方法将会操作input channel的消息； 操作input channel 读取input channel的信息，并调用sendWatchCacheEvent方法； sendWatchCacheEvent kube-apiserver的watch会带过滤功能； 对watchCacheEvent进行Filter，发送到cacher.result channel中； ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/:2:0","tags":["kubernetes"],"title":"Kube-apiserver watch实现","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-watch/"},{"categories":["问题排查","bug修复"],"content":"现象 k8s master进行线上升级，notifier利用client-go提供的informer机制注册了EndPoint的Update Handler，当kube-apiserver重启时触发了大量的update事件，触发依赖的第三方服务限流。 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/:0:1","tags":["kubernetes"],"title":"Kube-apiserver重启导致产生全量的update event","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/"},{"categories":["问题排查","bug修复"],"content":"原因排查 在测试环境进行了测试，并且在注册update事件处理函数中调用 reflect.DeepEqual(old, new) 进行了比较，发现返回true，即old与new完全相同却产生了update事件。 接下来就是到事件产生的地方去寻找原因，主要有两个地方，一个是reflect的ListAndWatch，相当于元数据的生产者，另一个是sharedIndexedInformer的HandleDeltas，消费元数据并生成对应类型的事件分发下去，接下来分别看 HandleDeltas （事件来源） func (s *sharedIndexInformer) HandleDeltas(obj interface{}) error { s.blockDeltas.Lock() defer s.blockDeltas.Unlock() // from oldest to newest for _, d := range obj.(Deltas) { switch d.Type { case Sync, Added, Updated: isSync := d.Type == Sync s.cacheMutationDetector.AddObject(d.Object) // 重点关注 if old, exists, err := s.indexer.Get(d.Object); err == nil \u0026\u0026 exists { if err := s.indexer.Update(d.Object); err != nil { return err } s.processor.distribute(updateNotification{oldObj: old, newObj: d.Object}, isSync) } else { if err := s.indexer.Add(d.Object); err != nil { return err } s.processor.distribute(addNotification{newObj: d.Object}, isSync) } case Deleted: if err := s.indexer.Delete(d.Object); err != nil { return err } s.processor.distribute(deleteNotification{oldObj: d.Object}, false) } } return nil } 很容易看出来，当delta类型为非Delete时，informer会从自己的indexer（带索引的缓存）中获取指定的object是否存在（注意这里其实是从object计算出key，然后用key寻找到的），如果存在则更新缓存且分发一个update事件。可以继续看后续对分发的这个notification的处理，都是直接处理，没有任何去重逻辑。到这里就可以理解为啥会收到全量的update事件了，正式因为此时缓存里已经有了对应数据，而在分发事件时并没有比较缓存中的object是否和新来的object一致就直接当成update处理了，导致客户端收到全量的更新事件。那问题又来了，为什么重启apiserver时会往deltafifo里全量扔一遍数据，正常不应该是从最后的resourceVersion开始重新watch吗？继续看下面的处理 ListAndWatch （全量数据的来源） // 代码位置 k8s.io/client-go/tools/cache/reflector.go // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { klog.V(3).Infof(\"Listing and watching %v from %s\", r.expectedType, r.name) var resourceVersion string // Explicitly set \"0\" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1.ListOptions{ResourceVersion: \"0\"} if err := func() error { initTrace := trace.New(\"Reflector ListAndWatch\", trace.Field{\"name\", r.name}) defer initTrace.LogIfLong(10 * time.Second) var list runtime.Object var err error listCh := make(chan struct{}, 1) panicCh := make(chan interface{}, 1) go func() { defer func() { if r := recover(); r != nil { panicCh \u003c- r } }() // Attempt to gather list in chunks, if supported by listerWatcher, if not, the first // list request will return the full response. pager := pager.New(pager.SimplePageFunc(func(opts metav1.ListOptions) (runtime.Object, error) { return r.listerWatcher.List(opts) })) if r.WatchListPageSize != 0 { pager.PageSize = r.WatchListPageSize } // Pager falls back to full list if paginated list calls fail due to an \"Expired\" error. list, err = pager.List(context.Background(), options) close(listCh) }() select { case \u003c-stopCh: return nil case r := \u003c-panicCh: panic(r) case \u003c-listCh: } if err != nil { return fmt.Errorf(\"%s: Failed to list %v: %v\", r.name, r.expectedType, err) } initTrace.Step(\"Objects listed\") listMetaInterface, err := meta.ListAccessor(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v: %v\", r.name, list, err) } resourceVersion = listMetaInterface.GetResourceVersion() initTrace.Step(\"Resource version extracted\") items, err := meta.ExtractList(list) if err != nil { return fmt.Errorf(\"%s: Unable to understand list result %#v (%v)\", r.name, list, err) } initTrace.Step(\"Objects extracted\") if err := r.syncWith(items, resourceVersion); err != nil { return fmt.Errorf(\"%s: Unable to sync list result: %v\", r.name, err) } initTrace.Step(\"SyncWith done\") r.setLastSyncResourceVersion(resourceVersion) initTrace.Step(\"Resource version updated\") return nil }(); err != nil { return err } resyncerrc := make(chan error, 1) cancelCh := make(chan struct{}) defer close(cancelCh) g","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/:0:2","tags":["kubernetes"],"title":"Kube-apiserver重启导致产生全量的update event","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/"},{"categories":["问题排查","bug修复"],"content":"总结 至此，已经清楚了具体的原因，ListAndWatch的修改很简单，已经给官方提了pull request 修复这个问题。 ","date":"2019-08-21","objectID":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/:0:3","tags":["kubernetes"],"title":"Kube-apiserver重启导致产生全量的update event","uri":"https://www.likakuli.com/posts/kubernetes-apiserver-refused/"},{"categories":["源码分析"],"content":"kubelet pod创建主流程","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":" kubernetes 版本： v1.12 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:0:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"kubelet 工作原理 kubelet 的工作核心就是在围绕着不同的生产者生产出来的不同的有关 pod 的消息来调用相应的消费者（不同的子模块）完成不同的行为(创建和删除 pod 等)，即图中的控制循环（SyncLoop），通过不同的事件驱动这个控制循环运行。 本文仅分析新建 pod 的流程，当一个 pod 完成调度，与一个 node 绑定起来之后，这个 pod 就会触发 kubelet 在循环控制里注册的 handler，上图中的 HandlePods 部分。此时，通过检查 pod 在 kubelet 内存中的状态，kubelet 就能判断出这是一个新调度过来的 pod，从而触发 Handler 里的 ADD 事件对应的逻辑处理。然后 kubelet 会为这个 pod 生成对应的 podStatus，接着检查 pod 所声明的 volume 是不是准备好了，然后调用下层的容器运行时。如果是 update 事件的话，kubelet 就会根据 pod 对象具体的变更情况，调用下层的容器运行时进行容器的重建。 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:1:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"kubelet 创建 pod 的流程 kubelet 创建 pod 的流程 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"1、kubelet 的控制循环（syncLoop） syncLoop 中首先定义了一个 syncTicker 和 housekeepingTicker，即使没有需要更新的 pod 配置，kubelet 也会定时去做同步和清理 pod 的工作。然后在 for 循环中一直调用 syncLoopIteration，如果在每次循环过程中出现比较严重的错误，kubelet 会记录到 runtimeState 中，遇到错误就等待 5 秒中继续循环。 func (kl *Kubelet) syncLoop(updates \u003c-chan kubetypes.PodUpdate, handler SyncHandler) { glog.Info(\"Starting kubelet main sync loop.\") // syncTicker 每秒检测一次是否有需要同步的 pod workers syncTicker := time.NewTicker(time.Second) defer syncTicker.Stop() // 每两秒检测一次是否有需要清理的 pod housekeepingTicker := time.NewTicker(housekeepingPeriod) defer housekeepingTicker.Stop() // pod 的生命周期变化 plegCh := kl.pleg.Watch() const ( base = 100 * time.Millisecond max = 5 * time.Second factor = 2 ) duration := base for { if rs := kl.runtimeState.runtimeErrors(); len(rs) != 0 { time.Sleep(duration) duration = time.Duration(math.Min(float64(max), factor*float64(duration))) continue } ... kl.syncLoopMonitor.Store(kl.clock.Now()) // 第二个参数为 SyncHandler 类型，SyncHandler 是一个 interface， // 在该文件开头处定义 if !kl.syncLoopIteration(updates, handler, syncTicker.C, housekeepingTicker.C, plegCh) { break } kl.syncLoopMonitor.Store(kl.clock.Now()) } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:1","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"2、监听 pod 变化（syncLoopIteration） syncLoopIteration 这个方法就会对多个管道进行遍历，发现任何一个管道有消息就交给 handler 去处理。它会从以下管道中获取消息： configCh：该信息源由 kubeDeps 对象中的 PodConfig 子模块提供，该模块将同时 watch 3 个不同来源的 pod 信息的变化（file，http，apiserver），一旦某个来源的 pod 信息发生了更新（创建/更新/删除），这个 channel 中就会出现被更新的 pod 信息和更新的具体操作。 syncCh：定时器管道，每隔一秒去同步最新保存的 pod 状态 houseKeepingCh：housekeeping 事件的管道，做 pod 清理工作 plegCh：该信息源由 kubelet 对象中的 pleg 子模块提供，该模块主要用于周期性地向 container runtime 查询当前所有容器的状态，如果状态发生变化，则这个 channel 产生事件。 livenessManager.Updates()：健康检查发现某个 pod 不可用，kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作 func (kl *Kubelet) syncLoopIteration(configCh \u003c-chan kubetypes.PodUpdate, handler SyncHandler, syncCh \u003c-chan time.Time, housekeepingCh \u003c-chan time.Time, plegCh \u003c-chan *pleg.PodLifecycleEvent) bool { select { case u, open := \u003c-configCh: if !open { glog.Errorf(\"Update channel is closed. Exiting the sync loop.\") return false } switch u.Op { case kubetypes.ADD: ... case kubetypes.UPDATE: ... case kubetypes.REMOVE: ... case kubetypes.RECONCILE: ... case kubetypes.DELETE: ... case kubetypes.RESTORE: ... case kubetypes.SET: ... } ... case e := \u003c-plegCh: ... case \u003c-syncCh: ... case update := \u003c-kl.livenessManager.Updates(): ... case \u003c-housekeepingCh: ... } return true } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:2","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"3、处理新增 pod（HandlePodAddtions） 对于事件中的每个 pod，执行以下操作： 1、把所有的 pod 按照创建日期进行排序，保证最先创建的 pod 会最先被处理 2、把它加入到 podManager 中，podManager 子模块负责管理这台机器上的 pod 的信息，pod 和 mirrorPod 之间的对应关系等等。所有被管理的 pod 都要出现在里面，如果 podManager 中找不到某个 pod，就认为这个 pod 被删除了 3、如果是 mirror pod 调用其单独的方法 4、验证 pod 是否能在该节点运行，如果不可以直接拒绝 5、通过 dispatchWork 把创建 pod 的工作下发给 podWorkers 子模块做异步处理 6、在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测 func (kl *Kubelet) HandlePodAdditions(pods []*v1.Pod) { start := kl.clock.Now() // 对所有 pod 按照日期排序，保证最先创建的 pod 优先被处理 sort.Sort(sliceutils.PodsByCreationTime(pods)) for _, pod := range pods { if kl.dnsConfigurer != nil \u0026\u0026 kl.dnsConfigurer.ResolverConfig != \"\" { kl.dnsConfigurer.CheckLimitsForResolvConf() } existingPods := kl.podManager.GetPods() // 把 pod 加入到 podManager 中 kl.podManager.AddPod(pod) // 判断是否是 mirror pod（即 static pod） if kubepod.IsMirrorPod(pod) { kl.handleMirrorPod(pod, start) continue } if !kl.podIsTerminated(pod) { activePods := kl.filterOutTerminatedPods(existingPods) // 通过 canAdmitPod 方法校验Pod能否在该计算节点创建(如:磁盘空间) // Check if we can admit the pod; if not, reject it. if ok, reason, message := kl.canAdmitPod(activePods, pod); !ok { kl.rejectPod(pod, reason, message) continue } } mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod) // 通过 dispatchWork 分发 pod 做异步处理，dispatchWork 主要工作就是把接收到的参数封装成 UpdatePodOptions，调用 UpdatePod 方法. kl.dispatchWork(pod, kubetypes.SyncPodCreate, mirrorPod, start) // 在 probeManager 中添加 pod，如果 pod 中定义了 readiness 和 liveness 健康检查，启动 goroutine 定期进行检测 kl.probeManager.AddPod(pod) } } static pod 是由 kubelet 直接管理的，k8s apiserver 并不会感知到 static pod 的存在，当然也不会和任何一个 rs 关联上，完全是由 kubelet 进程来监管，并在它异常时负责重启。Kubelet 会通过 apiserver 为每一个 static pod 创建一个对应的 mirror pod，如此以来就可以可以通过 kubectl 命令查看对应的 pod,并且可以通过 kubectl logs 命令直接查看到static pod 的日志信息。 ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:3","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"4、下发任务（dispatchWork） dispatchWorker 的主要作用是把某个对 Pod 的操作（创建/更新/删除）下发给 podWorkers。 func (kl *Kubelet) dispatchWork(pod *v1.Pod, syncType kubetypes.SyncPodType, mirrorPod *v1.Pod, start time.Time) { if kl.podIsTerminated(pod) { if pod.DeletionTimestamp != nil { kl.statusManager.TerminatePod(pod) } return } // 落实在 podWorkers 中 kl.podWorkers.UpdatePod(\u0026UpdatePodOptions{ Pod: pod, MirrorPod: mirrorPod, UpdateType: syncType, OnCompleteFunc: func(err error) { if err != nil { metrics.PodWorkerLatency.WithLabelValues(syncType.String()).Observe(metrics.SinceInMicroseconds(start)) } }, }) if syncType == kubetypes.SyncPodCreate { metrics.ContainersPerPodCount.Observe(float64(len(pod.Spec.Containers))) } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:4","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"5、更新事件的 channel（UpdatePod） podWorkers 子模块主要的作用就是处理针对每一个的 Pod 的更新事件，比如 Pod 的创建，删除，更新。而 podWorkers 采取的基本思路是：为每一个 Pod 都单独创建一个 goroutine 和更新事件的 channel，goroutine 会阻塞式的等待 channel 中的事件，并且对获取的事件进行处理。而 podWorkers 对象自身则主要负责对更新事件进行下发。 func (p *podWorkers) UpdatePod(options *UpdatePodOptions) { pod := options.Pod uid := pod.UID var podUpdates chan UpdatePodOptions var exists bool p.podLock.Lock() defer p.podLock.Unlock() // 如果当前 pod 还没有启动过 goroutine ，则启动 goroutine，并且创建 channel if podUpdates, exists = p.podUpdates[uid]; !exists { // 创建 channel podUpdates = make(chan UpdatePodOptions, 1) p.podUpdates[uid] = podUpdates // 启动 goroutine go func() { defer runtime.HandleCrash() p.managePodLoop(podUpdates) }() } // 下发更新事件 if !p.isWorking[pod.UID] { p.isWorking[pod.UID] = true podUpdates \u003c- *options } else { update, found := p.lastUndeliveredWorkUpdate[pod.UID] if !found || update.UpdateType != kubetypes.SyncPodKill { p.lastUndeliveredWorkUpdate[pod.UID] = *options } } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:5","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"6、调用 syncPodFn 方法同步 pod（managePodLoop） managePodLoop 调用 syncPodFn 方法去同步 pod，syncPodFn 实际上就是kubelet.SyncPod。在完成这次 sync 动作之后，会调用 wrapUp 函数，这个函数将会做几件事情: 将这个 pod 信息插入 kubelet 的 workQueue 队列中，等待下一次周期性的对这个 pod 的状态进行 sync 将在这次 sync 期间堆积的没有能够来得及处理的最近一次 update 操作加入 goroutine 的事件 channel 中，立即处理。 func (p *podWorkers) managePodLoop(podUpdates \u003c-chan UpdatePodOptions) { var lastSyncTime time.Time for update := range podUpdates { err := func() error { podUID := update.Pod.UID status, err := p.podCache.GetNewerThan(podUID, lastSyncTime) if err != nil { ... } err = p.syncPodFn(syncPodOptions{ mirrorPod: update.MirrorPod, pod: update.Pod, podStatus: status, killPodOptions: update.KillPodOptions, updateType: update.UpdateType, }) lastSyncTime = time.Now() return err }() if update.OnCompleteFunc != nil { update.OnCompleteFunc(err) } if err != nil { ... } p.wrapUp(update.Pod.UID, err) } } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:6","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"7、完成创建容器前的准备工作（SyncPod） 在这个方法中，主要完成以下几件事情： 如果是删除 pod，立即执行并返回 同步 podStatus 到 kubelet.statusManager 检查 pod 是否能运行在本节点，主要是权限检查（是否能使用主机网络模式，是否可以以 privileged 权限运行等）。如果没有权限，就删除本地旧的 pod 并返回错误信息 创建 containerManagar 对象，并且创建 pod level cgroup，更新 Qos level cgroup 如果是 static Pod，就创建或者更新对应的 mirrorPod 创建 pod 的数据目录，存放 volume 和 plugin 信息,如果定义了 pv，等待所有的 volume mount 完成（volumeManager 会在后台做这些事情）,如果有 image secrets，去 apiserver 获取对应的 secrets 数据 然后调用 kubelet.volumeManager 组件，等待它将 pod 所需要的所有外挂的 volume 都准备好。 调用 container runtime 的 SyncPod 方法，去实现真正的容器创建逻辑 这里所有的事情都和具体的容器没有关系，可以看到该方法是创建 pod 实体（即容器）之前需要完成的准备工作。 func (kl *Kubelet) syncPod(o syncPodOptions) error { // pull out the required options pod := o.pod mirrorPod := o.mirrorPod podStatus := o.podStatus updateType := o.updateType // 是否为 删除 pod if updateType == kubetypes.SyncPodKill { ... } ... // 检查 pod 是否能运行在本节点 runnable := kl.canRunPod(pod) if !runnable.Admit { ... } // 更新 pod 状态 kl.statusManager.SetPodStatus(pod, apiPodStatus) // 如果 pod 非 running 状态则直接 kill 掉 if !runnable.Admit || pod.DeletionTimestamp != nil || apiPodStatus.Phase == v1.PodFailed { ... } // 加载网络插件 if rs := kl.runtimeState.networkErrors(); len(rs) != 0 \u0026\u0026 !kubecontainer.IsHostNetworkPod(pod) { ... } pcm := kl.containerManager.NewPodContainerManager() if !kl.podIsTerminated(pod) { ... // 创建并更新 pod 的 cgroups if !(podKilled \u0026\u0026 pod.Spec.RestartPolicy == v1.RestartPolicyNever) { if !pcm.Exists(pod) { ... } } } // 为 static pod 创建对应的 mirror pod if kubepod.IsStaticPod(pod) { ... } // 创建数据目录 if err := kl.makePodDataDirs(pod); err != nil { ... } // 挂载 volume if !kl.podIsTerminated(pod) { if err := kl.volumeManager.WaitForAttachAndMount(pod); err != nil { ... } } // 获取 secret 信息 pullSecrets := kl.getPullSecretsForPod(pod) // 调用 containerRuntime 的 SyncPod 方法开始创建容器 result := kl.containerRuntime.SyncPod(pod, apiPodStatus, podStatus, pullSecrets, kl.backOff) kl.reasonCache.Update(pod.UID, result) if err := result.Error(); err != nil { ... } return nil } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:7","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"8、创建容器 containerRuntime（pkg/kubelet/kuberuntime）子模块的 SyncPod 函数才是真正完成 pod 内容器实体的创建。 syncPod 主要执行以下几个操作： 1、计算 sandbox 和 container 是否发生变化 2、创建 sandbox 容器 3、启动 init 容器 4、启动业务容器 initContainers 可以有多个，多个 container 严格按照顺序启动，只有当前一个 container 退出了以后，才开始启动下一个 container。 func (m *kubeGenericRuntimeManager) SyncPod(pod *v1.Pod, _ v1.PodStatus, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, backOff *flowcontrol.Backoff) (result kubecontainer.PodSyncResult) { // 1、计算 sandbox 和 container 是否发生变化 podContainerChanges := m.computePodActions(pod, podStatus) if podContainerChanges.CreateSandbox { ref, err := ref.GetReference(legacyscheme.Scheme, pod) if err != nil { glog.Errorf(\"Couldn't make a ref to pod %q: '%v'\", format.Pod(pod), err) } ... } // 2、kill 掉 sandbox 已经改变的 pod if podContainerChanges.KillPod { ... } else { // 3、kill 掉非 running 状态的 containers ... for containerID, containerInfo := range podContainerChanges.ContainersToKill { ... if err := m.killContainer(pod, containerID, containerInfo.name, containerInfo.message, nil); err != nil { ... } } } m.pruneInitContainersBeforeStart(pod, podStatus) podIP := \"\" if podStatus != nil { podIP = podStatus.IP } // 4、创建 sandbox podSandboxID := podContainerChanges.SandboxID if podContainerChanges.CreateSandbox { podSandboxID, msg, err = m.createPodSandbox(pod, podContainerChanges.Attempt) if err != nil { ... } ... podSandboxStatus, err := m.runtimeService.PodSandboxStatus(podSandboxID) if err != nil { ... } // 如果 pod 网络是 host 模式，容器也相同；其他情况下，容器会使用 None 网络模式，让 kubelet 的网络插件自己进行网络配置 if !kubecontainer.IsHostNetworkPod(pod) { podIP = m.determinePodSandboxIP(pod.Namespace, pod.Name, podSandboxStatus) glog.V(4).Infof(\"Determined the ip %q for pod %q after sandbox changed\", podIP, format.Pod(pod)) } } configPodSandboxResult := kubecontainer.NewSyncResult(kubecontainer.ConfigPodSandbox, podSandboxID) result.AddSyncResult(configPodSandboxResult) // 获取 PodSandbox 的配置(如:metadata,clusterDNS,容器的端口映射等) podSandboxConfig, err := m.generatePodSandboxConfig(pod, podContainerChanges.Attempt) ... // 5、启动 init container if container := podContainerChanges.NextInitContainerToStart; container != nil { ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeInit); err != nil { ... } } // 6、启动业务容器 for _, idx := range podContainerChanges.ContainersToStart { ... if msg, err := m.startContainer(podSandboxID, podSandboxConfig, container, pod, podStatus, pullSecrets, podIP, kubecontainer.ContainerTypeRegular); err != nil { ... } } return } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:8","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"9、启动容器 最终由 startContainer 完成容器的启动，其主要有以下几个步骤： 1、拉取镜像 2、生成业务容器的配置信息 3、调用 docker api 创建容器 4、启动容器 5、执行 post start hook func (m *kubeGenericRuntimeManager) startContainer(podSandboxID string, podSandboxConfig *runtimeapi.PodSandboxConfig, container *v1.Container, pod *v1.Pod, podStatus *kubecontainer.PodStatus, pullSecrets []v1.Secret, podIP string, containerType kubecontainer.ContainerType) (string, error) { // 1、检查业务镜像是否存在，不存在则到 Docker Registry 或是 Private Registry 拉取镜像。 imageRef, msg, err := m.imagePuller.EnsureImageExists(pod, container, pullSecrets) if err != nil { ... } ref, err := kubecontainer.GenerateContainerRef(pod, container) if err != nil { ... } // 设置 RestartCount restartCount := 0 containerStatus := podStatus.FindContainerStatusByName(container.Name) if containerStatus != nil { restartCount = containerStatus.RestartCount + 1 } // 2、生成业务容器的配置信息 containerConfig, cleanupAction, err := m.generateContainerConfig(container, pod, restartCount, podIP, imageRef, containerType) if cleanupAction != nil { defer cleanupAction() } ... // 3、通过 client.CreateContainer 调用 docker api 创建业务容器 containerID, err := m.runtimeService.CreateContainer(podSandboxID, containerConfig, podSandboxConfig) if err != nil { ... } err = m.internalLifecycle.PreStartContainer(pod, container, containerID) if err != nil { ... } ... // 3、启动业务容器 err = m.runtimeService.StartContainer(containerID) if err != nil { ... } containerMeta := containerConfig.GetMetadata() sandboxMeta := podSandboxConfig.GetMetadata() legacySymlink := legacyLogSymlink(containerID, containerMeta.Name, sandboxMeta.Name, sandboxMeta.Namespace) containerLog := filepath.Join(podSandboxConfig.LogDirectory, containerConfig.LogPath) if _, err := m.osInterface.Stat(containerLog); !os.IsNotExist(err) { if err := m.osInterface.Symlink(containerLog, legacySymlink); err != nil { glog.Errorf(\"Failed to create legacy symbolic link %q to container %q log %q: %v\", legacySymlink, containerID, containerLog, err) } } // 4、执行 post start hook if container.Lifecycle != nil \u0026\u0026 container.Lifecycle.PostStart != nil { kubeContainerID := kubecontainer.ContainerID{ Type: m.runtimeName, ID: containerID, } // runner.Run 这个方法的主要作用就是在业务容器起来的时候， // 首先会执行一个 container hook(PostStart 和 PreStop),做一些预处理工作。 // 只有 container hook 执行成功才会运行具体的业务服务，否则容器异常。 msg, handlerErr := m.runner.Run(kubeContainerID, pod, container, container.Lifecycle.PostStart) if handlerErr != nil { ... } } return \"\", nil } ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:2:9","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"总结 本文主要讲述了 kubelet 从监听到有容器调度至本节点再到容器创建的一个过程，kubelet 最终调用 docker api 来创建容器的。结合上篇文章，可以看出 kubelet 从启动到创建 pod 的一个清晰过程。 参考： k8s源码分析-kubelet Kubelet源码分析(一):启动流程分析 kubelet 源码分析：pod 新建流程 kubelet创建Pod流程解析 Kubelet: Pod Lifecycle Event Generator (PLEG) Design- proposals ","date":"2019-08-06","objectID":"https://www.likakuli.com/posts/kubernetes-pod-create/:3:0","tags":["kubernetes"],"title":"Pod创建流程","uri":"https://www.likakuli.com/posts/kubernetes-pod-create/"},{"categories":["源码分析"],"content":"kubelet与cni交互流程","date":"2019-07-16","objectID":"https://www.likakuli.com/posts/kubernetes-cni/","tags":["kubernetes"],"title":"Kubelet与CNI交互源码","uri":"https://www.likakuli.com/posts/kubernetes-cni/"},{"categories":["源码分析"],"content":" 转载自：https://www.cnblogs.com/haoqingchuan/p/8668746.html，有修改 代码版本1.12.4 整体介绍 kubelet通过调用 grpc 接口调用实现了 CRI 的 dockershim 完成 rpc 通信，CNI 是由 dockershim grpc server 中调用的 kubelet -\u003e CRI shim -\u003e container runtime -\u003e container POD 创建过程中从 kubelet 到 docker server 到 cni 的 UML 结构如下 CNI 插件初始化 kubelet 在初始化的时候如果使用containerRuntime为Docker，则会起动dockershim rpc server case kubetypes.DockerContainerRuntime: // Create and start the CRI shim running as a grpc server. streamingConfig := getStreamingConfig(kubeCfg, kubeDeps, crOptions) // 主要函数 ds, err := dockershim.NewDockerService(kubeDeps.DockerClientConfig, crOptions.PodSandboxImage, streamingConfig, \u0026pluginSettings, runtimeCgroups, kubeCfg.CgroupDriver, crOptions.DockershimRootDirectory, !crOptions.RedirectContainerStreaming) if err != nil { return nil, err } if crOptions.RedirectContainerStreaming { klet.criHandler = ds } // The unix socket for kubelet \u003c-\u003e dockershim communication. glog.V(5).Infof(\"RemoteRuntimeEndpoint: %q, RemoteImageEndpoint: %q\", remoteRuntimeEndpoint, remoteImageEndpoint) glog.V(2).Infof(\"Starting the GRPC server for the docker CRI shim.\") server := dockerremote.NewDockerServer(remoteRuntimeEndpoint, ds) if err := server.Start(); err != nil { return nil, err } // Create dockerLegacyService when the logging driver is not supported. supported, err := ds.IsCRISupportedLogDriver() if err != nil { return nil, err } if !supported { klet.dockerLegacyService = ds legacyLogProvider = ds } 创建 dockerservice 对象时初始化cniplugin // rpc server端 func NewDockerService(config *ClientConfig, podSandboxImage string, streamingConfig *streaming.Config, pluginSettings *NetworkPluginSettings, cgroupsName string, kubeCgroupDriver string, dockershimRootDir string, startLocalStreamingServer bool) (DockerService, error) { ... // 解析kubelet配置的pluginbindir pluginSettings.PluginBinDirs = cni.SplitDirs(pluginSettings.PluginBinDirString) cniPlugins := cni.ProbeNetworkPlugins(pluginSettings.PluginConfDir, pluginSettings.PluginBinDirs) cniPlugins = append(cniPlugins, kubenet.NewPlugin(pluginSettings.PluginBinDirs)) netHost := \u0026dockerNetworkHost{ \u0026namespaceGetter{ds}, \u0026portMappingGetter{ds}, } plug, err := network.InitNetworkPlugin(cniPlugins, pluginSettings.PluginName, netHost, pluginSettings.HairpinMode, pluginSettings.NonMasqueradeCIDR, pluginSettings.MTU) if err != nil { return nil, fmt.Errorf(\"didn't find compatible CNI plugin with given settings %+v: %v\", pluginSettings, err) } ds.network = network.NewPluginManager(plug) glog.Infof(\"Docker cri networking managed by %v\", plug.Name()) ... } // 根据指定confdir 和 bindir获取networkplugin func ProbeNetworkPlugins(confDir string, binDirs []string) []network.NetworkPlugin { old := binDirs binDirs = make([]string, 0, len(binDirs)) for _, dir := range old { if dir != \"\" { binDirs = append(binDirs, dir) } } plugin := \u0026cniNetworkPlugin{ defaultNetwork: nil, loNetwork: getLoNetwork(binDirs), execer: utilexec.New(), confDir: confDir, binDirs: binDirs, } // sync NetworkConfig in best effort during probing. plugin.syncNetworkConfig() return []network.NetworkPlugin{plugin} } 初始化cniplugin，会根据pluginDir查找符合条件的第一个 CNI config 文件，并以此 config 文件查找到对应的 CNI bin // InitNetworkPlugin inits the plugin that matches networkPluginName. Plugins must have unique names. func InitNetworkPlugin(plugins []NetworkPlugin, networkPluginName string, host Host, hairpinMode kubeletconfig.HairpinMode, nonMasqueradeCIDR string, mtu int) (NetworkPlugin, error) { if networkPluginName == \"\" { // default to the no_op plugin plug := \u0026NoopNetworkPlugin{} plug.Sysctl = utilsysctl.New() if err := plug.Init(host, hairpinMode, nonMasqueradeCIDR, mtu); err != nil { return nil, err } return plug, nil } pluginMap := map[string]NetworkPlugin{} allErrs := []error{} for _, plugin := range plugins { name := plugin.Name() if errs := validation.IsQualifiedName(name); len(errs) != 0 { allErrs = append(allErrs, fmt.Errorf(\"network plugin has invalid name: %q: %s\", name, strings.Join(errs, \";\"))) continue } if _, found := pluginMap[n","date":"2019-07-16","objectID":"https://www.likakuli.com/posts/kubernetes-cni/:0:0","tags":["kubernetes"],"title":"Kubelet与CNI交互源码","uri":"https://www.likakuli.com/posts/kubernetes-cni/"},{"categories":["问题排查"],"content":"cgroup泄露","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"背景 线上k8s节点创建容器时提示\"no space left on device\"，为已知问题，参考 https://tencentcloudcontainerteam.github.io/2018/12/29/cgroup-leaking/ http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:1","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"解决方案 按照上述链接中的提示，首先看runc部分，docker用的从opencontainers/runc项目fork出来的docker/runc项目，目前线上用的docker版本为1.13.1，对应的docker-runc的commit为 9df8b306d01f59d3a8029be411de015b7304dd8f，查看其相关代码 func (s *MemoryGroup) Apply(d *cgroupData) (err error) { path, err := d.path(\"memory\") if err != nil \u0026\u0026 !cgroups.IsNotFound(err) { return err } if memoryAssigned(d.config) { if path != \"\" { if err := os.MkdirAll(path, 0755); err != nil { return err } } // 默认关闭 if d.config.KernelMemory != 0 { if err := EnableKernelMemoryAccounting(path); err != nil { return err } } } defer func() { if err != nil { os.RemoveAll(path) } }() // We need to join memory cgroup after set memory limits, because // kmem.limit_in_bytes can only be set when the cgroup is empty. _, err = d.join(\"memory\") if err != nil \u0026\u0026 !cgroups.IsNotFound(err) { return err } return nil } 此版本默认关闭KernelMemory功能，所以docker-runc暂时不需要改，接下来看kubelet相关代码，kubelet为1.12.4版本，pkg/kubelet/cm/cgroup_manager_linux.go下 func (s *MemoryGroup) Apply(d *cgroupData) (err error) { path, err := d.path(\"memory\") if err != nil \u0026\u0026 !cgroups.IsNotFound(err) { return err } else if path == \"\" { return nil } if memoryAssigned(d.config) { if _, err := os.Stat(path); os.IsNotExist(err) { if err := os.MkdirAll(path, 0755); err != nil { return err } // Only enable kernel memory accouting when this cgroup // is created by libcontainer, otherwise we might get // error when people use `cgroupsPath` to join an existed // cgroup whose kernel memory is not initialized. // 强制开启 if err := EnableKernelMemoryAccounting(path); err != nil { return err } } } defer func() { if err != nil { os.RemoveAll(path) } }() // We need to join memory cgroup after set memory limits, because // kmem.limit_in_bytes can only be set when the cgroup is empty. _, err = d.join(\"memory\") if err != nil \u0026\u0026 !cgroups.IsNotFound(err) { return err } return nil } 上面的代码为默认开启KernelMemory，且无法关闭，解决方案是注释掉EnableKernelMemoryAccounting调用，然后重新编译kubelet即可。由于线上docker和cgroup使用的cgroup-driver为cgroupfs而不是systemd**，所以这里并没有修改systemd****对应文件里有关KernelMemory****的代码。** ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:2","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"验证 找了一台新机器，上面没有任何容器，先看下改之前的kubelet所创建的/sys/fs/cgroup/memory/kubepods/memory.kmem.slabinfo文件，如下 说明已经开启了kmem，然后替换kubelet并重启宿主，观察上面文件，如下 说明kmem已经关闭了。这里重点强调一下，必须重启宿主才能生效，只重启kubelet无法生效，因为需要修改/sys/fs/cgroup/memory/kubepods，kubelet启动时会检测此目录是否存在，不存在则创建，存在则直接使用，只重启kubelet时此目录依然存在，因为容器业务进程还在使用着相关的cgroup。新创建的Pod会以继承此目录下的cgroup的配置，所以需要重启宿主才能关闭kmem。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:3","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"总结 本篇是一种快速暴力的解决问题手段，后经过调研测试，有不需要重启宿主的方案，在这一篇中介绍 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak/:0:4","tags":["linux","cgroup"],"title":"Cgroup泄露1","uri":"https://www.likakuli.com/posts/cgroup-leak/"},{"categories":["问题排查"],"content":"cgroup泄露","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":" 线上宿主使用的kubernetes版本是1.12 ，kubelet默认是开启了kmem accounting的功能。kernel memory 在内核4.0以下的版本是一个实验特性，存在使用后不能删除cgroup的问题，造成cgroup泄漏。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:0:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"现象描述 宿主机上创建容器时失败，kubelet log中可见 报错信息如下 mkdir /sys/fs/cgroup/memory/kubepods/burstable/pod79fe803c-072f-11e9-90ca-525400090c71/b98d4aea818bf9d1d1aa84079e1688cd9b4218e008c58a8ef6d6c3c106403e7b: no space left on devic ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:1:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"到底在泄漏什么 内核中对于每个子系统的的条目数是有限制的，限制的大小定义在kernel/cgroup.c#L139。 当正常在cgroup创建一个group的目录时，条目数就加1 .我们遇到的情况就是因为开启了kmem accounting功能，虽然cgroup的目录删除了，但是条目没有回收。这样后面就无法创建65535个cgroup了。也就是说，在当前内核版本下，开启了kmem accounting功能，会导致memory cgroup的条目泄漏无法回收。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:2:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"如何查看泄漏程度 正常情况下，可以通过以下命令来查看cgroup的数目 。 cat /proc/cgroups 但是存在泄漏的宿主机上这个数字是不可信的，因为这个计数就是随着cgroup下目录的增删进行。 经过请教内核组同学，他给出了一个可以统计cgroup条目的方法 # usage : stap -g get_memcg_count.stp %{ #include \u003clinux/rcupdate.h\u003e#include \u003clinux/cgroup.h\u003e#include \u003clinux/memcontrol.h\u003e /* The embedded c function must have a return value. * If it doesn't has an argument, we must use a specified void here. */ int get_memcg_count(void) { struct cgroup_subsys_state *tmp; int count = 1; int i; rcu_read_lock(); for (i = 1; i \u003c 65536; i++) { tmp = css_lookup(\u0026mem_cgroup_subsys, i); if (tmp) count++; } rcu_read_unlock(); return count; } %} function do_calc:long() %{ int count = 0; count = get_memcg_count(); STAP_RETVALUE = count; %} probe begin { printf(\"probe begin\\n\"); printf(\"count %ld\\n\", do_calc()); exit(); } probe end { printf(\"probe end\\n\"); } ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:3:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"如何引入 \u0026 如何解决 这个问题是kubernetes 1.9 版本引入的，kubelet创建容器代码中EnableKernelMemoryAccounting 导致的。关于这个问题的分析，网络上有很多文章进行分析，比如 https://github.com/kubernetes/kubernetes/issues/61937 https://github.com/moby/moby/issues/29638 https://tencentcloudcontainerteam.github.io/2018/12/29/cgroup-leaking/ http://www.linuxfly.org/kubernetes-19-conflict-with-centos7/?from=groupmessage 在上篇cgroup泄露问题1进行了详细的分析，并提供了1.12.4版本（线上版本）的修复方案。 社区版本1.14，提供了开关可以关闭kmem accounting。 文章给出的方案简单总结就是重启宿主机后+关闭kmem account的kubelet，可以彻底解决这个问题。对于设置了node affinity的场景，重启宿主机成本较高，因此也就有了探索是否可以不重启宿主机的方案。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:4:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"cgroup迁移 下面对于memory cgroup子系统，简称为memcg 。对于已经泄漏的memcg，新创建的容器会继承父group，所以会加剧这个问题。如果我们通过一种cgroup迁移方式，将当前的memcg 迁移到另一个group，然后重新创建关闭了kmem accounting的group，并把原来的子group迁移回来是否就可以搞定这个问题了呢。 cgroup 本身是支持cgroup迁移功能的 4.2 Task migration When a task migrates from one cgroup to another, its charge is not carried forward by default. The pages allocated from the original cgroup still remain charged to it, the charge is dropped when the page is freed or reclaimed. You can move charges of a task along with task migration. See 8. \"Move charges at task migration\" This feature is disabled by default. It can be enabledi (and disabled again) by writing to memory.move_charge_at_immigrate of the destination cgroup. If you want to enable it: # echo (some positive value) \u003e memory.move_charge_at_immigrate Note: Each bits of move_charge_at_immigrate has its own meaning about what type of charges should be moved. See 8.2 for details. Note: Charges are moved only when you move mm-\u003eowner, in other words, a leader of a thread group. Note: If we cannot find enough space for the task in the destination cgroup, we try to make space by reclaiming memory. Task migration may fail if we cannot make enough space. Note: It can take several seconds if you move charges much. 我们是全量task 迁移，因此也不存在上面注意事项中提到的只支持迁移主线程的问题。 那我们梳理下，memcg 需要迁移内容包含哪些。 迁移后需要保证容器内存quota不变，容器的内存使用量不变，容器内的进程（对于cgroup来说，都是task）迁移后不丢。这三项分别对应的是memory.limit_in_bytes/memory.usage_in_bytes/tasks 因为内存使用量是memcg来控制的，我们看到的memory.usage_in_bytes 是只读的，所以这个文件中的数据迁移是依赖于task迁移来实现。 ","date":"2019-07-10","objectID":"https://www.likakuli.com/posts/cgroup-leak2/:5:0","tags":["linux","cgroup"],"title":"Cgroup泄露2","uri":"https://www.likakuli.com/posts/cgroup-leak2/"},{"categories":["问题排查"],"content":"dockerd内存泄露","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"背景 线上部分宿主机dockerd占用内存过大，有的甚至超过100G，而整个宿主上的容器使用的内存还不如dockerd一个进程使用的多，现在的处理办法是故障自愈，检测到dockerd使用内存超过10G后会设置live-restore，然后重启dockerd，而不影响正常运行的容器，但是重启后还一直存在内存泄露的问题。可以总结为两类内存泄露情况：没有设置live-restore: true的和设置了live-restore: true且重启过dockerd的，这里是针对后者的排查，因为线上默认dockerd没有开启debug模式，要想排查前者的问题，就需要重启docker，又因为没有配置live-restore: true，就会影响到正在运行的容器。 ","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:1","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"dockerd日志 tail -f /var/log/messages | grep dockerd，结果如下图，存在内存泄露的dockerd的日志都有如下的日志记录，且看时间规律是相同sandbox的记录每秒打印一遍 从源码中搜索日志内容，对应下面的源码分析-2里的内容。 查看dockerd启动时的日志，如下 ","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:2","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"步骤 步骤比较长，尤其是源码那部分，不关注源码的可以直接跳过源码，直接看代码的解释 pprof分析 开启dockerd的debug模式，即编辑/etc/docker/daemon.json，加上debug: true的配置并重启dockerd，方便利用pprof来定位内存泄露对应的代码位置。 执行go tool pprof http://ip:port/debug/pprof/heap，输入top命令查看内存分配情况，如下图 可以看到占用内存较多的函数调用，但是并不是很直观，可以继续输入web命令，会生成svg图片并通过画图软件或浏览器打开，如下图 这样就可以清楚地看到整个调用流程及各函数占用内存大小，可以发现是外部某程序调用了docker的api，最终调用SubscribeTopic，此函数里面存在内存泄露。 源码分析 明确了发生泄露的源码位置，接下来就是去看下源码的具体逻辑，下面贴出部分docker源码(tag v1.13.1)，省略部分不影响结果的代码 docker // 位置github.com/docker/docker/pkg/pubsub/publisher.go // SubscribeTopic adds a new subscriber that filters messages sent by a topic. func (p *Publisher) SubscribeTopic(topic topicFunc) chan interface{} { ch := make(chan interface{}, p.buffer) p.m.Lock() p.subscribers[ch] = topic p.m.Unlock() return ch } 这段代码很短，每次先new一个新的chan，然后把chan加入到字典中。可以看到如果发生内存泄漏，那八成就是这句p.subscribers[ch] = topic，也就是说会一直往map里添加新元素而得不到删除。为了验证，继续向上找此函数的调用方，沿着调用堆栈（svg图片里显示了具体的调用堆栈）往上找，如下 // Subscribe adds a new subscriber to the publisher returning the channel. func (p *Publisher) Subscribe() chan interface{} { // 这里调用SubscribeTopic return p.SubscribeTopic(nil) } // collect registers the container with the collector and adds it to // the event loop for collection on the specified interval returning // a channel for the subscriber to receive on. func (s *statsCollector) collect(c *container.Container) chan interface{} { s.m.Lock() defer s.m.Unlock() publisher, exists := s.publishers[c] if !exists { publisher = pubsub.NewPublisher(100*time.Millisecond, 1024) s.publishers[c] = publisher } // 这里调用Subscribe return publisher.Subscribe() } func (daemon *Daemon) subscribeToContainerStats(c *container.Container) chan interface{} { return daemon.statsCollector.collect(c) } // ContainerStats writes information about the container to the stream // given in the config object. func (daemon *Daemon) ContainerStats(ctx context.Context, prefixOrName string, config *backend.ContainerStatsConfig) error { ... // subscribe updates := daemon.subscribeToContainerStats(container) // unsubscribe defer daemon.unsubscribeToContainerStats(container, updates) noStreamFirstFrame := true for { select { case v, ok := \u003c-updates: if !ok { return nil } ... if !config.Stream \u0026\u0026 noStreamFirstFrame { // prime the cpu stats so they aren't 0 in the final output noStreamFirstFrame = false continue } if err := enc.Encode(statsJSON); err != nil { return err } if !config.Stream { return nil } case \u003c-ctx.Done(): return nil } } } 可以看到最终在ContainerStats中调用了subscribe并在此函数退出后调用defer里的unsubscribe。直接看代码可能看不懂，先介绍下docker stats的api，此api用来获取容器资源使用详情，包括cpu，memory，network等信息，支持两种方式，流和非流的方式，流是利用http的chunked属性实现的，非流的方式是直接返回。 chan****的产生 每次调用docker stats {container} 或者 docker stats的api的时候，都会进入到ContainerStats函数，这里使用了一个chan来达到异步的效果，即并不是每次调用stats都去实时的统计相关数据，而是有个后台goroutine在定时的stats（下面会介绍），并把数据推送到chan，每次调用api时只是去chan中获取数据而已，此chan对应的就是上面for select中的updates，即daemon.subscribeToContainerStats(container)的返回结果，也就是最终SubscribeTopic里new的chan。 chan****的消亡 在ContainerStats函数结束后会调用unsubscribeToContainerStats，这里会关闭创建出来的chan并从map中删除，释放内存。 知道了chan的产生和消亡，可以排除以stream形式调用api导致的，因为这种方式不会一直调用api，也就不会导致chan一直新建。那就只剩下一种形式，即外部有程序定时的以非stream的形式调用docker stats的api。但是上面也看到了函数结束后会在defer里释放掉申请的chan，为什么还会导致内存泄露呢？稍微注意一下就可以看到释放chan的函数是在defer里调用的，而且函数里还有个for循环，所以很可能是因为函数的for循环一直没有退出，导致defer一直得不到执行，chan也就一直释放不了，而且外部还定时的调用api，会导致一直会有新的chan的创建且旧的chan加入缓存后无法被删除，最终导致占用的内存越来越大。那就看下for循环内的代码，可以看到只有一个select，两个case，分别对应从updates chan中读数据和从ctx.Done()中读数据，后者是外部程序取消此次api调用后会得到执行的，即结束此次调用，前者是从updates chan中读stats数据，所以如果想要函数不退出，那么两个case都无法满足即可，即外部程序没有主动cancel request且updates chan中始终没有数据，先不管外部程序，因为也不知道外部程序是谁，代码怎么写的，但是可以肯定的是外部程序在调用docker api时的处理有问题，没有设置超时或者超时了也没有去cancel request。重点关注下为什么updates chan中一直没有数据，那就要看下写数据相关代码，如下 func (s *statsCollector) run() { type publishersPair struct { container *container.Container publisher *pubsub.Publisher } // we cannot determine the capacity here. // it will grow enough in first iteration var pairs []publishersPair // s.interval是1s，硬编码的 for range time.Tick(s.interval) { // it does not make sense in the f","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:3","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查"],"content":"总结 docker使用下来给人的感觉就是存在太多的问题了，后面还会有多篇有关docker资源泄露、目录umount失败、读写pipe失败等各式各样的问题。 ","date":"2019-07-09","objectID":"https://www.likakuli.com/posts/dockerd-memory-leak1/:0:4","tags":["docker"],"title":"Dockerd内存泄露","uri":"https://www.likakuli.com/posts/dockerd-memory-leak1/"},{"categories":["问题排查","性能优化"],"content":"create pod slowly","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"背景 线上kubernetes集群从创建sts到创建pod需要时间很长，分钟级别，但是调度却很快。偶尔还会出现导致kube-odin任务失败（超过300s）的情况 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:1","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"排查过程 分析可能的原因： watch到sts的变化有延迟 sts从入队列到出队列耗时长 处理sts耗时长 kube-controller-manager中sts相关源码中有一些日志，需要把loglevel设置为4，即调试级别才会打印，里面就包括处理单个sts的耗时。首先把kube-controller-manager日志级别调到4，日志如下图, 最后显示的时间是从队列中拿到sts到处理完sts的整个过程的耗时，可以看到耗时并不长，在毫秒级别，那就可以排除掉处理单个sts耗时长的可能性了。 还剩下两种可能，不过细想的话，第一种可能也不大，因为watch是通用的，没道理同一个集群kube-controller-manager里的watch就慢，kube-scheuler的watch就快。那就很有可能是从watch到变化后把sts入队列到从队列中拿到sts这个阶段耗时太长了。源码中并没有这一部分的耗时统计，但是从源码中可以看到整个处理过程是同步到的，即watch的所有sts按顺序入队列，消费者在顺序的从队列中拿到，每消费完一个，再去拿另一个，串行执行，那问题就来了，虽然单个sts执行耗时在毫秒级，但是整个集群的sts数量在2000+，按平均每个sts耗时40ms计算，粗略估算一下处理完一轮的话也需要40ms*2000=80s的时间，到这里已经离真相不远了，但还有一个问题，那就是kube-controller-manager在初始化的时候是会把所有的sts加载一遍放入队列中的，处理完一遍哪怕耗时2分钟，但是处理完一遍之后只watch变化的sts，数量就会少很多了，所以处理完初始化时加载的所有sts后，按道理再有sts变化应该是可以及时处理的，因为此时队列中基本没有sts了。带着问题再去看源码，发现了一个神奇的地方，如下 setInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: ssc.enqueueStatefulSet, UpdateFunc: func(old, cur interface{}) { oldPS := old.(*apps.StatefulSet) curPS := cur.(*apps.StatefulSet) if oldPS.Status.Replicas != curPS.Status.Replicas { glog.V(4).Infof(\"Observed updated replica count for StatefulSet: %v, %d-\u003e%d\", curPS.Name, oldPS.Status.Replicas, curPS.Status.Replicas) } ssc.enqueueStatefulSet(cur) }, DeleteFunc: ssc.enqueueStatefulSet, }, statefulSetResyncPeriod, // 30s ) 这就对了，上面这段代码意思是只要watch到sts的变化，就会把对应的sts放入队列，且每隔30s会把全部sts重新入一遍队列，再加上刚才的估算，80s才能处理完所有sts，在未处理完之前（处理了30s时）就又会把所有的sts重新加入到队列中（并不是简单粗暴的把所有sts入队列，中间还会做一些处理，过滤掉一些不需要重复入队列的sts），这就会导致sts的待处理队列中始终有2000+个元素，新watch到的变化会加到队尾，从而导致sts创建后过了很久Pod才创建，因为sts controller一直在消费之前未处理完的其他sts了。 下面写了一个Demo来演示这个问题，代码很简单，如下 package main import ( \"fmt\" \"k8s.io/api/apps/v1\" \"k8s.io/client-go/informers\" \"k8s.io/client-go/kubernetes\" \"k8s.io/client-go/rest\" \"k8s.io/client-go/tools/cache\" \"k8s.io/client-go/util/workqueue\" \"log\" \"math/rand\" \"time\" \"unsafe\" ) var queue workqueue.RateLimitingInterface func main() { defer queue.ShutDown() clientset, _ := kubernetes.NewForConfig(\u0026rest.Config{ Host: \"http://10.80.101.22:8080\", }) factor := rand.Float64() + 1 syncPeriod := time.Duration(float64(time.Duration(12*time.Hour).Nanoseconds()) * factor) informerFactory := informers.NewSharedInformerFactory(clientset, syncPeriod) stopChan := make(chan struct{}) informerFactory.Start(stopChan) informerFactory.WaitForCacheSync(stopChan) stsInformer := informerFactory.Apps().V1().StatefulSets().Informer() // 和sts controller一直，30s同步一遍所有sts stsInformer.AddEventHandlerWithResyncPeriod(cache.ResourceEventHandlerFuncs{ AddFunc: onAdd, UpdateFunc: onUpdate, DeleteFunc: onDelete, }, 30*time.Second) go consume() stsInformer.Run(stopChan) } func init() { queue = workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"statefulset\") } func consume() { for { func() { key, quit := queue.Get() if quit { fmt.Println(\"done\") return } defer queue.Done(key) sts := key.(*v1.StatefulSet) // 模拟处理单个sts耗时40ms time.Sleep(40 * time.Millisecond) log.Printf(\"cosumed %s, queue count: %d\", sts.Name, queue.Len()) }() } } func onAdd(obj interface{}) { sts := obj.(*v1.StatefulSet) if sts.Name == \"test-delay-sf-60f09\" { log.Printf(\"add address: %d\", unsafe.Pointer(sts)) } queue.Add(sts) } func onUpdate(old, new interface{}) { sts := old.(*v1.StatefulSet) // 测试用的sts，观察全量同步时的地址变化，用来确定是不通批次的同步，针对同一个sts来说是否地址相同 // 因为queue中用到sts作为map的key，所以此处打印地址，验证一下 if sts.Name == \"test-delay-sf-60f09\" { log.Printf(\"update address: %d\", unsafe.Pointer(sts)) } queue.Add(sts) } func onDelete(obj interface{}) { sts := obj.(*v1.StatefulSet) if sts.Name == \"test-delay-sf-60f09\" { log.Printf(\"update address: %d\", unsafe.Pointer(sts)) } queue.Add(sts) } 最终的输出结果 可以看到queue的长度从一开始的2000+一直降到1525，此时同步了一遍全量的sts，即2000+，queue中元素数量又生了上去。同时在未修改sts的情况，指定sts同步后的地址和同步前的地址相同824689074368。验证了之前的猜想，问题就出在了这里。 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:2","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"解决方案 想到两种优化方案 去掉定期（30s）全量同步的机制，目前看其他controller，如ReplicationController，ServiceController，EndpointsController等都没有设置定期全量同步 保留定期同步，添加已处理的sts的缓存，每次从queue中拿到一个新的sts时，比较已处理缓存中是否存在相同的sts(resourceversion相同)，存在则忽略此sts，否则进行处理 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:3","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查","性能优化"],"content":"社区 上述问题已反馈社区，修复方式就是方案1，直接去掉了30s的同步机制。见https://github.com/kubernetes/kubernetes/pull/75622 这里需要注意一点：30s的同步机制并不是从kube-apiserver拉取全量数据，而是把Informer本地缓存的数据（位于Indexer中）全量同步一遍，目的是为了防止出现在事件处理函数中与外部组件交互时出错的情况，参考这个issue：https://github.com/kubernetes/kubernetes/issues/75495，但是sts控制器本身没有依赖任何外部（k8s以外）组件，所以就不需要30s同步了。但是我们以Operator实现的自定义Controller就需要根据实际情况激进型设置了，后面会专门有一个系列详细讲Informer的源码，敬请期待。 ","date":"2019-03-26","objectID":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/:0:4","tags":["kubernetes"],"title":"Statefulset创建pod慢","uri":"https://www.likakuli.com/posts/kubernetes-statefulset-sync/"},{"categories":["问题排查"],"content":"etcd watch内存泄露","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"背景 通过监控看到弹性云用户平台后端程序kube-odin的内存使用量在稳定增加，每次上线完又会恢复，可以判断出kube-odin中存在内存泄漏问题 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/:0:1","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"排查过程 golang程序的问题排查，无论CPU还是Memory问题都可以用官方提供的pprof工具，最简单的办法就是在kube-odin代码加入如下包net/http/pprof，上线到了测试环境，然后通过go tool pprof httpaddress的方式查看kube-odin内存消耗，网上也有很多pprof使用方法的文章，可以自行搜索，知道怎么用了之后看下图 由于是测试环境，对接测试集群，本身数据量就不大，程序刚启动时占用内存也就百十来M，现在已经用了1G多，占用内存最多的是newWatcherGrpcStream函数，还有一些其他的函数，占用的内存也在逐步增加，先看newWatcherGrpcStream函数，可以通过list查看其具体内存使用情况，如下图 fmt.Sprintf居然都占用了86.01M内存，还有几个chan占用的也比较多，但是基本都是无缓存的chan，正常不会占用这么多的。一般内存泄露可能是流未关闭，这种情况一般文件描述符也会泄露，另外就是用到缓存时也容易造成泄露，如果缓存的内容得不到释放且一直增加内容，内存就会越来越高。去看etcd相关代码，在代码中找问题，发现了一处很可疑的代码，去掉了无关内容，且增加了fmt.Println相关函数，方便观察每次运行到此处的缓存的结果，如下 // Watch posts a watch request to run() and waits for a new watcher channel func (w *watcher) Watch(ctx context.Context, key string, opts ...OpOption) WatchChan { ... ctxKey := fmt.Sprintf(\"%v\", ctx) // find or allocate appropriate grpc watch stream w.mu.Lock() if w.streams == nil { // closed w.mu.Unlock() ch := make(chan WatchResponse) close(ch) return ch } fmt.Println(ctxKey) // 打印缓存的key fmt.Println(len(w.streams)) //打印缓存数量 wgs := w.streams[ctxKey] if wgs == nil { fmt.Println(\"new watcher stream\") //缓存里没有对应的key wgs = w.newWatcherGrpcStream(ctx) w.streams[ctxKey] = wgs }else{ fmt.Println(\"use exist watcher stream\") //缓存里有key，复用缓存 } ... } // watcher implements the Watcher interface type watcher struct { remote pb.WatchClient // mu protects the grpc streams map mu sync.RWMutex // streams holds all the active grpc streams keyed by ctx value. streams map[string]*watchGrpcStream } 这里出现了上面的fmt.Sprintf、newWatcherGrpcStream等函数，而且出现了缓存，即w.streams，每次watch时都是先调用fmt.Sprintf获取到key，再从缓存中取，如果有则复用，没有则新建，问题很有可能出现在这里，然后再找一下缓存删除数据的逻辑，如下 func (w *watcher) Close() (err error) { w.mu.Lock() fmt.Println(\"begin close watcher\") streams := w.streams w.streams = nil w.mu.Unlock() for _, wgs := range streams { if werr := wgs.Close(); werr != nil { err = werr } } return err } func (w *watcher) closeStream(wgs *watchGrpcStream) { w.mu.Lock() fmt.Println(\"delete watch stream\") //开始删除缓存 close(wgs.donec) wgs.cancel() if w.streams != nil { fmt.Println(\"before delete:\",len(w.streams)) //删除前缓存数量 fmt.Println(wgs.ctxKey) if _,ok:=w.streams[wgs.ctxKey];ok{ fmt.Println(\"delete key exist\") //删除的key在缓存里存在 } else{ fmt.Println(\"delete key NOT exist\") //删除的key在缓存里不存在 } delete(w.streams, wgs.ctxKey) fmt.Println(\"after delete:\",len(w.streams)) //删除后缓存的数量 } w.mu.Unlock() } 和删除缓存相关的函数有两个，第一个Close函数只有在etcdclient的关闭链接时才会调用，而我们在不断的lock，unlock时其实用的是同一份etcdclient，所以不会是第一个函数。还剩一个closeStream函数，这里我也加了一些打印信息，用来查看缓存相关信息，closeStream调用如下 func (w *watcher) newWatcherGrpcStream(inctx context.Context) *watchGrpcStream { ... go wgs.run() return wgs } // run is the root of the goroutines for managing a watcher client func (w *watchGrpcStream) run() { ... defer func() { ... w.owner.closeStream(w) }() ... } 整个过程从插入缓存到删除缓存看起来都没有问题，只能写个demo测试一下了，demo大致如下 func main() { client := instance.GetEtcdClient() locker := lock.New(client, lock.WithTTL(1*time.Second)) go foo(locker) http.HandleFunc(\"/gc\", func(writer http.ResponseWriter, request *http.Request) { runtime.GC() }) http.ListenAndServe(\":8080\", nil) } func foo(locker lock.Locker) { ticker := time.NewTicker(1 * time.Second) ids := []string{\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\"} for range ticker.C { for _, v := range ids { go func(i string) { unlock, _, err := locker.Trylock(context.TODO(), fmt.Sprintf(\"%s/%s\", \"/kaku/test/etcd/lock\", i)) if err != nil { if err != context.DeadlineExceeded { fmt.Println(\"lock task failed:%s\", err) } return } //fmt.Println(\"task has been locked\") defer func() { time.Sleep(time.Second) unlock() //fmt.Println(\"task has been unlocked\") }() }(v) } } } 特别简单，就是不断的去lock，unlock，结合之前增加的一些缓存打印信息，运行demo，结果如下 context.TODO.WithCancel.WithDeadline(2019-01-27 12:03:04.293267 +0800 CST m=+3.078016721 [750.274218ms]).WithCancel 0 new watcher stream context.TODO.WithCancel.WithDeadline(2019-01-27 12:03:04.293375 +0800 CST m=+3.078124824 [704.968531ms]).WithCancel 1 new watcher stream context.TODO.WithCancel.WithDeadline(2019-01-27 12:03:04.293271 +0800 CST m=+3.078020","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/:0:2","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"解决方案 升级etcd包版本至少到3.2.20 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-watch/:0:3","tags":["etcd"],"title":"Etcd watch内存泄漏","uri":"https://www.likakuli.com/posts/etcd-watch/"},{"categories":["问题排查"],"content":"etcd分布式锁加锁失败","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"现象 线上程序一直报错，错误信息：lock failed: context deadline exceeded, retry ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:1","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"排查过程 异常对应代码位置 很明显的是获取锁超时了，由于用的etcd的分布式锁，就怀疑是etcd出问题了，此时看到大量etcd日志，rejected connection from “ip:port” (error “tls: first record does not look like a TLS handshake”, ServerName “\")，怀疑是不是这个问题导致的，经过查询报错的IP，均为线上容器IP，登陆容器内看发现都是管理员平台的代码，里面也用到了py03的etcd，但是不是导致超时的原因。在排除各种可能之后，最后去etcd查看锁对应的key的情况，发现有两个key /notifier/locker/{leaseid} /notifier/locker/rwl/{leaseid} 其中第一个key是notifier自己添加的，第二个key在代码中搜不到，但是看起来像是redis whitelist的简写，先把第一个key删了，然后看notifier日志，仍然获取不到锁，所以怀疑是第二个key已经获得了锁，虽然key不一样。于是删除了第二个key，再看notifier日志，终于获得了锁，开始正常工作，于是得出猜想，etcd****的分布式锁，在子目录下加了锁之后，父目录会加锁失败。然后用etcdctl lock来验证了下，确实如此，/a/b下加了锁，/a再加锁就会失败，但是/a下加了锁，/a/b再加锁会成功。基本上可以验证上面的猜想，剩下的就是从etcd源码中找到对应处理的代码了。 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:2","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"etcd源码部分 在查询源码之前，第一反应就是这肯定是在服务端实现的，于是开始了从etcd服务端找相关源码的过程，从etcdctl命令开始追溯到所涉及的服务端，一直没有发现问题。又在网上搜了相关etcd服务端源码实现的文章，结合本地代码均没有想找的代码，于是反过来从client找起。 首先从etcdctl lock命令开始，挑主要函数展示 // 代码位置go.etcd.io/etcd/etcdctl/ctlv3/command/lock_command.go func lockUntilSignal(c *clientv3.Client, lockname string, cmdArgs []string) error { ... if err := m.Lock(ctx); err != nil { return err } ... } 接下来进入到Lock函数，这是个关键函数，etcd的分布式锁就是在这里实现的 func (m *Mutex) Lock(ctx context.Context) error { s := m.s client := m.s.Client() // 这里的pfx就是prefix，就是传进来的前缀，后面的s.Lease()会返回一个租约，是一个int64的整数，和session有关 m.myKey = fmt.Sprintf(\"%s%x\", m.pfx, s.Lease()) // 这里比较上面prefix/lease的createrevision是否为0，为0表示目前不存在该key，需要执行Put操作，下面可以看到 // 不为0表示已经有对应的key了，只需要执行Get就行 // createrevision是自增的 cmp := v3.Compare(v3.CreateRevision(m.myKey), \"=\", 0) // put self in lock waiters via myKey; oldest waiter holds lock put := v3.OpPut(m.myKey, \"\", v3.WithLease(s.Lease())) // reuse key in case this session already holds the lock get := v3.OpGet(m.myKey) // 获取所得持有者 getOwner := v3.OpGet(m.pfx, v3.WithFirstCreate()...) resp, err := client.Txn(ctx).If(cmp).Then(put, getOwner).Else(get, getOwner).Commit() if err != nil { return err } m.myRev = resp.Header.Revision if !resp.Succeeded { m.myRev = resp.Responses[0].GetResponseRange().Kvs[0].CreateRevision } // if no key on prefix / the minimum rev is key, already hold the lock ownerKey := resp.Responses[1].GetResponseRange().Kvs // 比较如果当前没有人获得锁或者锁的owner的createrevision等于当前的kv的revision，则表示已获得锁，就可以退出了 if len(ownerKey) == 0 || ownerKey[0].CreateRevision == m.myRev { m.hdr = resp.Header return nil } // 为了验证自己加的打印信息 //fmt.Printf(\"ownerKey: %s\\n\", ownerKey) // 走到这里代表没有获得锁，需要等待之前的锁被释放，即revision小于当前revision的kv被删除 hdr, werr := waitDeletes(ctx, client, m.pfx, m.myRev-1) // release lock key if wait failed if werr != nil { m.Unlock(client.Ctx()) } else { m.hdr = hdr } return werr } // waitDeletes 等待所有当前比当前key的revision小的key被删除后，锁释放后才返回 func waitDeletes(ctx context.Context, client *v3.Client, pfx string, maxCreateRev int64) (*pb.ResponseHeader, error) { getOpts := append(v3.WithLastCreate(), v3.WithMaxCreateRev(maxCreateRev)) for { resp, err := client.Get(ctx, pfx, getOpts...) if err != nil { return nil, err } if len(resp.Kvs) == 0 { return resp.Header, nil } lastKey := string(resp.Kvs[0].Key) // 为了调试自己加的这句 fmt.Printf(\"wait for %s to delete\\n\", lastKey) if err = waitDelete(ctx, client, lastKey, resp.Header.Revision); err != nil { return nil, err } } } func waitDelete(ctx context.Context, client *v3.Client, key string, rev int64) error { cctx, cancel := context.WithCancel(ctx) defer cancel() var wr v3.WatchResponse // wch是个channel，key被删除后会往这个chan发数据 wch := client.Watch(cctx, key, v3.WithRev(rev)) for wr = range wch { for _, ev := range wr.Events { if ev.Type == mvccpb.DELETE { return nil } } } if err := wr.Err(); err != nil { return err } if err := ctx.Err(); err != nil { return err } return fmt.Errorf(\"lost watcher waiting for delete\") } 看完上面的代码基本知道了etcd分布式锁的实现机制了，但是还没看到哪里和前缀Prefix相关了。其实答案就藏在getOwner里，看上述代码，不管是执行Put还是Get，最终都有个getOwner的过程，看一下这个getOwner，options模式里有个v3.WithFirstCreate函数调用，看下这个函数 // WithFirstCreate gets the key with the oldest creation revision in the request range. func WithFirstCreate() []OpOption { return withTop(SortByCreateRevision, SortAscend) } // withTop gets the first key over the get's prefix given a sort order func withTop(target SortTarget, order SortOrder) []OpOption { return []OpOption{WithPrefix(), WithSort(target, order), WithLimit(1)} } // WithPrefix enables 'Get', 'Delete', or 'Watch' requests to operate // on the keys with matching prefix. For example, 'Get(foo, WithPrefix())' // can return 'foo1', 'foo2', and so on. func WithPrefix() OpOption { return func(op *Op) { if len(op.key) == 0 { op.key, op.end = []byte{0}, []byte{0} return } op.end = getPrefix(op.key) } } 看到上面的是三个函数后，大致就找到了对应的源码的感觉，因为看到了WithPrefix函数，和上面的猜测正好匹配。所以getOwner的具体执行效果是会把所有以lockkey开头的kv都拿到，且按照createrevision升序排列，取第一个值，这个意思就很明白了，就是要拿到当前以lockkey为prefix的且createrevision最小的那个key，就是","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:3","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["问题排查"],"content":"总结 通过分析问题，看源码，可以了解到etcd锁的实现原理，以及可能存在的小坑。etcd居然把锁的实现放在了client端，也是出乎我的意料，这样的话，可以直接修改client端代码来修改其锁的实现，就可能出现虽然共用一个服务端，但是etcd行为却不一致的问题，不知道为何要这么设计，个人感觉还是要放到服务端更好些。 ","date":"2019-01-31","objectID":"https://www.likakuli.com/posts/etcd-lock/:0:4","tags":["etcd"],"title":"Etcd分布式锁加锁失败","uri":"https://www.likakuli.com/posts/etcd-lock/"},{"categories":["使用说明"],"content":"docker image p2p","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"测试环境 10.0.13.19 部署harbor，单点，docker-compose的方式部署 4核8G 10.0.13.22 dragonfly的supernode节点 16核64G docker方式部署 10.0.13.31 dragonfly的supernode节点 16核64G docker方式部署 kubernetes 集群 20个节点 ，docker storage-driver overlay 部署了dragonfly的daemon和dfget等程序 以上均为虚机，在同一个网段内，centos7.4系统 涉及到的ansible脚本在这里 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:1:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"dragonfly ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"特性 基于P2P文件分发 支持各种容器化技术 主机级别限速策略 利用CDN机制避免远程重复下载 强一致性 磁盘保护,高效的IO处理 高性能 异常自动隔离 降低文件来源服务器压力 支持标准的Http Header 使用简单 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:1","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"结构介绍 分发普通文件 注: 其中cluster manager即超级节点(supernode) 超级节点充当CDN，同时调度每个对等者(peer)在他们之间传输文件块。dfget是P2P客户端，也称为对等者(peer)，主要用于下载和共享文件块。 分发容器镜像 图中镜像仓库(registry)类似于文件服务器。dfget proxy也称为dfdaemon，它拦截来自docker pull和docker push的HTTP请求，然后将那些跟镜像分层相关的请求使用dfget来处理。 文件分块是怎么下载的 注: 其中cluster manager即超级节点(supernode) 每个文件会被分成多个块在对等者(peer)间进行传输。一个peer就是一个P2P客户端。 超级节点会判断文件是否存在本地，如果不存在，则会将其从文件服务器下载到本地。 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:2","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"流程解析 1.当执行docker pull操作时,dfget-proxy会拦截docker pull请求。将请求转发给CM(cluster manager)。 cm的地址已经在client主机的/etc/dragonfly.conf文件中配置好了。另外上文中提到的dfget-proxy其实就是df-daemon。Dragonfly中有三个项目,client端:getter(python)、daemon(golang),docker pull时,df-daemon拦截到请求并通过dfget进行文件拉取,server端:supernode(java)。 2.df-daemon启动的时候带了registry参数,并且通过dfget传给服务端supernode。supernode解析参数到对应的镜像仓库获取镜像并以block的形式返回给客户端。如果再次拉取镜像时,supernode就会检测哪一个client存在和镜像文件对应的block,如果存在直接从该client下载,如果不存在就通过server端到镜像仓库拉取镜像。 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:2:3","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"安装部署 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"安装服务端 Dragonfly官方支持基于Docker和Physical Machine两种方案部署server，这里为了方便直接使用docker方式部署。 docker run -d -p 8001:8001 -p 8002:8002 --restart=always registry.cn-hangzhou.aliyuncs.com/alidragonfly/supernode:0.2.0 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:1","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"安装客户端 #下载 此链接为0.1.0版本 wget https://github.com/alibaba/Dragonfly/raw/master/package/df-client.linux-amd64.tar.gz #解压 tar -zxvf df-client.linux-amd64.tar.gz #设置Env vim ~/.bashrc #将下面的设置添加到~/.bashrc文件末 PATH=$PATH:/root/df-client #退出vim并执行以下命令 source ~/.bashrc ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:2","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"Harbor搭建 基于在线方式的安装, 本文采用http的方式配置Harbor。Harbor版本为1.2.2 1.下载 wget https://storage.googleapis.com/harbor-releases/harbor-online-installer-v1.5.2.tgz tar -zxvf harbor-online-installer-v1.5.2.tgz 2.修改配置 cd harbor vim harbor.cfg hostname=10.0.13.19 //设置为当前主机ip 3.安装并启动harbor服务 服务启动以后,如果需要管理Harbor服务的生命周期,可以直接通过docker-compose来管理 sh install.sh ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:3","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"使用指南 1.在client主机上通过配置文件指定CM(cluster manager)节点 vi /etc/dragonfly.conf 内容: [node] address=10.0.13.22,10.0.13.31 2.**由于当前Dragonfly暂不支持harbor认证。如果按照官网配置\"configure daemon mirror\"来拉取镜像会提示授权失败。**为了绕过这个问题可以采用docker proxy的方式来解决。具体步骤如下: （1）vi /etc/systemd/system/docker.service.d/http-proxy.conf,没有该文件就直接创建该文件。通过添加proxy，在拉取镜像时将会通过下面的配置地址转发到目标机。 [Service] Environment=\"HTTP_PROXY=http://127.0.0.1:65001\" （2）更新变更 systemctl daemon-reload 3.在client机上添加harbor的insecure地址，在/etc/docker/daemon.json的insecure-registries中添加10.0.13.19 {\"disable-legacy-registry\":false,\"graph\":\"/data/docker\",\"insecure-registries\":[\"10.0.13.19\"]} 4.启动client服务 df-daemon --registry http://10.0.13.19 5.重启docker systemctl restart docker 6.docker登录 docker login --username=admin 10.0.13.19 提示登录成功说明上述配置正确 7.验证 #有数据经过65001端口则配置正确 tcpdump -i lo port 65001 #另开一个shell执行 docker pull 10.0.13.19/kaku/bigimage:v1.0 #拉取镜像 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:3:4","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"测试结果 2个supernode，20个节点并行的拉镜像 镜像大小（压缩） native cost dragonfly cost native harbor流量 dragonfly harbor流量 1.28G 2m多 稳定在1m30s左右 20*1.28 2*1.28 3.48G 10+m 稳定在5m30s左右 20*3.48 2*3.48 使用df之后，对harbor的压力明显减小，在20个节点时测试结果为 约33%的流量是通过p2p的方式获得的，随着节点数的增多，此值还会继续增大； 各节点镜像拉取时间稳定，比不使用代理时好很多，但是在单节点拉镜像时，使用代理时的耗时是要比原生docker pull耗时长的 目前部署的df supernode为0.2.0版本，client为0.0.1版本，尝试用0.1.0、0.1.1版本的client均失败，见这里 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:4:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"参考 dragonfly与harbor组建支持P2P的镜像服务 https://github.com/alibaba/Dragonfly/issues/17 https://github.com/alibaba/Dragonfly/issues/20 https://github.com/alibaba/Dragonfly/issues/50#issuecomment-382286474 ","date":"2018-09-13","objectID":"https://www.likakuli.com/posts/dragonfly/:5:0","tags":["docker","dragonfly"],"title":"Dragonfly + Harbor实现的p2p镜像分发","uri":"https://www.likakuli.com/posts/dragonfly/"},{"categories":["使用说明"],"content":"kubernetes启用GPU","date":"2018-09-06","objectID":"https://www.likakuli.com/posts/kubernetes-gpu/","tags":["kubernetes"],"title":"Kubernetes 启用GPU","uri":"https://www.likakuli.com/posts/kubernetes-gpu/"},{"categories":["使用说明"],"content":"kubernetes设置 k8s 1.10之前需要在kube-apiserver、kube-controller-manager、kube-scheduler、kubelet中开启如下feature，如果不是首次部署的话，重启以上所有组件： –feature-gates=“DevicePlugins=true” 安装 NVIDIA Driver~=361.93 安装nvidia-docker2，由于目前使用的docker版本提示信息里包含自定义字样，只能使用如下方式安装： 所有安装方式参考这里 # If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers docker volume ls -q -f driver=nvidia-docker | xargs -r -I{} -n1 docker ps -q -a -f volume={} | xargs -r docker rm -f sudo yum remove nvidia-docker # Add the package repositories distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.repo | \\ sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo # Install the nvidia runtime hook sudo yum install -y nvidia-container-runtime-hook sudo mkdir -p /usr/libexec/oci/hooks.d echo -e '#!/bin/sh\\nPATH=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" exec nvidia-container-runtime-hook \"$@\"' | \\ sudo tee /usr/libexec/oci/hooks.d/nvidia sudo chmod +x /usr/libexec/oci/hooks.d/nvidia # Test nvidia-smi with the latest official CUDA image # You can't use `--runtime=nvidia` with this setup. docker run --rm nvidia/cuda nvidia-smi 安装nvidia-container-runtime，在上一步中已经安装了对应的yum repo，这里直接执行如下命令即可： 因为使用了上一步的安装方式，所以需要进行这一步的安装，如果是通过yum直接安装的nvidia-docker2，则不需要进行此步。 # install runtime yum install nvidia-container-runtime update docker daemon，在docker daemon中添加如下配置 daemon.json中添加如下配置，可选配置为\"default-runtime\": “nvidia”，如果不设置默认runtime，则默认使用runc，启动容器是需要指定–runtime=nvidia \"default-runtime\": \"nvidia\"， \"runtimes\": { \"nvidia\": { \"path\": \"/usr/bin/nvidia-container-runtime\", \"runtimeArgs\": [] } } 安装NVIDIA device plugin，插件以daemonset方式部署，如果集群中既有CPU也有GPU节点，可以通过label筛选出GPU节点，无GPU的节点无需部署此程序 # For Kubernetes v1.8 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.8/nvidia-device-plugin.yml # For Kubernetes v1.9 kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v1.9/nvidia-device-plugin.yml ","date":"2018-09-06","objectID":"https://www.likakuli.com/posts/kubernetes-gpu/:1:0","tags":["kubernetes"],"title":"Kubernetes 启用GPU","uri":"https://www.likakuli.com/posts/kubernetes-gpu/"},{"categories":["使用说明"],"content":"测试 apiVersion: v1 kind: Pod metadata: name: cuda-vector-add spec: restartPolicy: OnFailure containers: - name: cuda-vector-add # https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile image: \"k8s.gcr.io/cuda-vector-add:v0.1\" resources: limits: nvidia.com/gpu: 1 nodeSelector: GPU: \"true\" //测试时自己给对应GPU节点加了GPU=true的label kubectl create -f test.yaml 执行结果： [root@vm10-0-13-17 ~]# kubectl get pods -a NAME READY STATUS RESTARTS AGE cuda-vector-add 0/1 Completed 0 3h nvidia-device-plugin-daemonset-pv6z8 1/1 Running 0 4h [root@vm10-0-13-17 ~]# kubectl logs cuda-vector-add [Vector addition of 50000 elements] Copy input data from the host memory to the CUDA device CUDA kernel launch with 196 blocks of 256 threads Copy output data from the CUDA device to the host memory Test PASSED Done ","date":"2018-09-06","objectID":"https://www.likakuli.com/posts/kubernetes-gpu/:2:0","tags":["kubernetes"],"title":"Kubernetes 启用GPU","uri":"https://www.likakuli.com/posts/kubernetes-gpu/"},{"categories":["使用说明"],"content":"golang监控","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"看了一篇文章，里面涉及到了一些golang程序监控的问题，回过头总结了一下实现方式，简单介绍一下 ","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:0:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"expvar go自带的runtime包拥有各种功能，包括goroutine数量，设置逻辑线程数量，当前go版本，当前系统类型等等。前两天发现了go标准库还有一个更好用的可以监控服务运行各项指标和状态的包—-expvar。 expvar包为监控变量提供了一个标准化的接口，它以 JSON 格式通过 /debug/vars 接口以 HTTP 的方式公开这些监控变量以及我自定义的变量。通过它，再加上metricBeat，ES和Kibana，可以很轻松的对服务进行监控。我这里是用gin把接口暴露出来，其实用别的web框架也都可以。下面我们来看一下如何使用它（示例代码使用GIN HTTP web framework）： package main import ( \"expvar\" \"github.com/gin-gonic/gin\" \"net/http\" \"net/http/pprof\" \"time\" ) func main() { router := gin.Default() router.GET(\"/debug/vars\", monitor.GetCurrentRunningStats) s := \u0026http.Server{ Addr: \":9090\", Handler: router, ReadTimeout: 5 * time.Second, WriteTimeout: 5 * time.Second, MaxHeaderBytes: 1 \u003c\u003c 20, } s.ListenAndServe() } 对应的handler package monitor import ( \"encoding/json\" \"expvar\" \"fmt\" \"github.com/gin-gonic/gin\" \"net/http\" \"runtime\" \"time\" ) // 开始时间 var start = time.Now() // calculateUptime 计算运行时间 func calculateUptime() interface{} { return time.Since(start).String() } // currentGoVersion 当前 Golang 版本 func currentGoVersion() interface{} { return runtime.Version() } // getNumCPUs 获取 CPU 核心数量 func getNumCPUs() interface{} { return runtime.NumCPU() } // getGoOS 当前系统类型 func getGoOS() interface{} { return runtime.GOOS } // getNumGoroutins 当前 goroutine 数量 func getNumGoroutins() interface{} { return runtime.NumGoroutine() } // getNumCgoCall CGo 调用次数 func getNumCgoCall() interface{} { return runtime.NumCgoCall() } var lastPause uint32 // getLastGCPauseTime 获取上次 GC 的暂停时间 func getLastGCPauseTime() interface{} { var gcPause uint64 ms := new(runtime.MemStats) statString := expvar.Get(\"memstats\").String() if statString != \"\" { json.Unmarshal([]byte(statString), ms) if lastPause == 0 || lastPause != ms.NumGC { gcPause = ms.PauseNs[(ms.NumGC+255)%256] lastPause = ms.NumGC } } return gcPause } // GetCurrentRunningStats 返回当前运行信息 func GetCurrentRunningStats(c *gin.Context) { c.Writer.Header().Set(\"Content-Type\", \"application/json; charset=utf-8\") first := true report := func(key string, value interface{}) { if !first { fmt.Fprintf(c.Writer, \",\\n\") } first = false if str, ok := value.(string); ok { fmt.Fprintf(c.Writer, \"%q: %q\", key, str) } else { fmt.Fprintf(c.Writer, \"%q: %v\", key, value) } } fmt.Fprintf(c.Writer, \"{\\n\") expvar.Do(func(kv expvar.KeyValue) { report(kv.Key, kv.Value) }) fmt.Fprintf(c.Writer, \"\\n}\\n\") c.String(http.StatusOK, \"\") } func init() { //这些都是我自定义的变量，发布到expvar中，每次请求接口，expvar会自动去获取这些变量，并返回给我 expvar.Publish(\"运行时间\", expvar.Func(calculateUptime)) expvar.Publish(\"version\", expvar.Func(currentGoVersion)) expvar.Publish(\"cores\", expvar.Func(getNumCPUs)) expvar.Publish(\"os\", expvar.Func(getGoOS)) expvar.Publish(\"cgo\", expvar.Func(getNumCgoCall)) expvar.Publish(\"goroutine\", expvar.Func(getNumGoroutins)) expvar.Publish(\"gcpause\", expvar.Func(getLastGCPauseTime)) } 运行程序，访问http://localhost:9090/debug/vars，如下 可以看到，expvar返回给了我我之前自定义的数据，以及它本身要默认返回的数据，比如memstats。这个memstats是干嘛的呢，其实看到这些字段名就可以知道，是各种内存堆栈以及GC的一些信息，具体可以看源码注释： type MemStats struct { // General statistics. // Alloc is bytes of allocated heap objects. // // This is the same as HeapAlloc (see below). Alloc uint64 // TotalAlloc is cumulative bytes allocated for heap objects. // // TotalAlloc increases as heap objects are allocated, but // unlike Alloc and HeapAlloc, it does not decrease when // objects are freed. TotalAlloc uint64 // Sys is the total bytes of memory obtained from the OS. // // Sys is the sum of the XSys fields below. Sys measures the // virtual address space reserved by the Go runtime for the // heap, stacks, and other internal data structures. It's // likely that not all of the virtual address space is backed // by physical memory at any given moment, though in general // it all was at some point. Sys uint64 // Lookups is the number of pointer lookups performed by the // runtime. // // This is primarily useful for debugging runtime internals. Lookups uint64 // Mallocs is the cumulative count of heap objects allocated. // The number of live objects is Mallocs - Frees. Mallocs uint64 // Frees is the cumulative count of heap objects f","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:1:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"pprof 有关pprof的基本介绍和使用，可以参考这里，最简单的使用方式如下 package main import ( \"net/http\" _ \"net/http/pprof\" ) func main() { http.ListenAndServe(\":9090\", nil) } 运行程序，访问http://localhost:9090/debug/pprof可以看到如下结果 配合go tool pprof一起使用，例如查看cpu详情，可以通过如下命令实现 root@kaku-Inspiron-7537:~/blog# go tool pprof localhost:9090/debug/pprof/profile Fetching profile over HTTP from http://localhost:9090/debug/pprof/profile Saved profile in /root/pprof/pprof.___go_build_main_go__1_.samples.cpu.002.pb.gz File: ___go_build_main_go__1_ Type: cpu Time: Jul 2, 2018 at 12:44am (CST) Duration: 30s, Total samples = 0 Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) web 执行完之后，如果本地安装了graphviz和chrome，则自动打开浏览器，可以在其中看到如下图片 因为示例程序没有任何其他代码，之暴露了http服务，所以看到的结果比较单调 ","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:2:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"Prometheus Prometheus client内置了golang metrics暴露的handler，只需要简单调用即可实现，如下 package main import ( \"github.com/prometheus/client_golang/prometheus/promhttp\" \"net/http\" ) func main() { http.Handle(\"/metrics\", promhttp.Handler()) panic(http.ListenAndServe(\":9090\", nil)) } 运行程序，访问http://localhost:9090/metrics 即可。 同时可以通过Prometheus来采集此Endpoint暴露出来的数据，也可以进行自定义数据的采集，参考这里 ","date":"2018-07-01","objectID":"https://www.likakuli.com/posts/golang-monitor/:3:0","tags":["golang"],"title":"Golang监控","uri":"https://www.likakuli.com/posts/golang-monitor/"},{"categories":["使用说明"],"content":"Prometheus","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"公司采用Prometheus来采集Kubernetes集群的性能指标数据，之前对性能数据采集这方面没有关注过，但是实际生产环境下有很多此类需求，因此重点学习了一下Prometheus采集数据的原理以及如何部署，接下来分别介绍。 Prometheus版本 2.3.0 AlertManager版本 1.4.0 因为两者的配置对旧版本的兼容不是很好，在按照网上搜索的资料进行部署时遇到了不少坑，所以当你看到这篇文章，根据文章进行部署时，可能我现在使用的版本已经很旧，您可能需要按需修改配置 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:0:0","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"原理 这张图大家应该不陌生，没错，就是从官网偷来的图片，下面特征和组件也是偷来的，哈哈。 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:1:0","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"特征 Prometheus的主要特点是： 一个具有由metric名称和键/值对标识的多维时间序列的数据模型 一种灵活的查询语言 来利用这种维度 不依赖分布式存储; 单个服务器节点是自治的（这点真不能认同，没有HA让人很难受） 时间序列收集通过HTTP上的拉式模型进行 推送时间序列通过中间网关支持 通过服务发现或静态配置来发现目标 多种模式的图形和仪表盘支持 AlertManager的主要特点： Grouping 分组 Inhibition 抑制 Silences HA (配置方式有限制，只能罗列出所有的实例，不能通过负载均衡方式配置) ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:1:1","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"组件 普罗米修斯生态系统由多个组件组成，其中许多组件是可选的： Prometheus服务器用来收集和存储时间序列数据 用于检测应用程序代码的客户端库 支持Short-lived job的Pushgateway 专门用于HAProxy，StatsD，Graphite等服务的exporter 一个警报处理器alertmanager 各种支持工具 大多数Prometheus组件都是用Go编写的，因此它们很容易构建和部署为静态二进制文件。 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:1:2","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"部署 这里主要通过Kubernetes部署Prometheus和AlertManager，实例数都只开1，原因也很好理解，Prometheus不支持HA，实例开多了也没有用，而且每个实例存的数据因为采集时间的原因会不一致，AlertManager只开一个实例的原因是虽然支持HA，但是只能在配置Prometheus时罗列出所有的实例，显然通过Kubernetes部署时无法获取到所有AlertManager的Pod实例，只能获得对应的Service，但是service就属于负载均衡了，如果AlertManager选在在集群外通过Docker或者二进制文件直接部署的话，可以开启多个实例，这里暂时在集群内部署，下面将列出所用到的Yaml文件。 首先，给出Prometheus相关的文件 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:0","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"prometheus.config.yaml apiVersion:v2kind:ConfigMapmetadata:name:prometheus-confignamespace:kube-systemdata:prometheus.yml:|global: scrape_interval: 15s evaluation_interval: 15s rule_files: - /etc/prometheus-rules/*.rules alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 scrape_configs: - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https - job_name: 'kubernetes-nodes' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics - job_name: 'kubernetes-cadvisor' kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+)target_label:__metrics_path__replacement:/api/v1/nodes/${1}/proxy/metrics/cadvisor- job_name:'kubernetes-service-endpoints'kubernetes_sd_configs:- role:endpointsrelabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scrape]action:keepregex:true- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_scheme]action:replacetarget_label:__scheme__regex:(https?)- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_path]action:replacetarget_label:__metrics_path__regex:(.+)- source_labels:[__address__, __meta_kubernetes_service_annotation_prometheus_io_port]action:replacetarget_label:__address__regex:([^:]+)(?::\\d+)?;(\\d+)replacement:$1:$2- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]action:replacetarget_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]action:replacetarget_label:kubernetes_name- job_name:'kubernetes-services'kubernetes_sd_configs:- role:servicemetrics_path:/probeparams:module:[http_2xx]relabel_configs:- source_labels:[__meta_kubernetes_service_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__address__]target_label:__param_target- target_label:__address__replacement:blackbox-exporter.example.com:9115- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_service_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_service_name]target_label:kubernetes_name- job_name:'kubernetes-ingresses'kubernetes_sd_configs:- role:ingressrelabel_configs:- source_labels:[__meta_kubernetes_ingress_annotation_prometheus_io_probe]action:keepregex:true- source_labels:[__meta_kubernetes_ingress_scheme,__address__,__meta_kubernetes_ingress_path]regex:(.+);(.+);(.+)replacement:${1}://${2}${3}target_label:__param_target- target_label:__address__replacement:blackbox-exporter.example.com:9115- source_labels:[__param_target]target_label:instance- action:labelmapregex:__meta_kubernetes_ingress_label_(.+)- source_labels:[__meta_kubernetes_namespace]target_label:kubernetes_namespace- source_labels:[__meta_kubernetes_ingress_name]target_label:kubernetes_name- job_name:'kubernetes-pods'kubernetes_sd_configs:- role:podrelabel_configs:- source_labels:[__meta","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:1","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"Prometheus.deploy.yaml ---apiVersion:apps/v1beta2 kind:Deployment metadata:labels:name:prometheus-deployment name:prometheus namespace:kube-system spec:replicas:1selector:matchLabels:app:prometheus template:metadata:labels:app:prometheus spec:containers:- image:quay.io/prometheus/prometheus name:prometheus command:- \"/bin/prometheus\"args:- \"--config.file=/etc/prometheus/prometheus.yml\"- \"--storage.tsdb.path=/prometheus\"- \"--storage.tsdb.retention=24h\"ports:- containerPort:9090protocol:TCP volumeMounts:- mountPath:\"/prometheus\"name:data - mountPath:\"/etc/prometheus\"name:config-volume - name:rules-volume mountPath:/etc/prometheus-rules resources:requests:cpu:100m memory:100Mi limits:cpu:500m memory:2500Mi serviceAccountName:prometheus volumes:- name:data emptyDir:{}- name:config-volume configMap:name:prometheus-config - name:rules-volume configMap:name:prometheus-rules--kind:ServiceapiVersion:v1metadata:labels:app:prometheusname:prometheusnamespace:kube-systemspec:type:NodePortports:- port:9090targetPort:9090nodePort:30003selector:app:prometheus 成功部署后，通过http://nodeIP:30003就可以访问Prometheus的页面了，如下图 scrape_configs中配置的kubernetes-pods、kubernetes-services等Job，可能在target界面看不到，因为需要添加对应的Annotation，例如Pod添加如下的Annotation后，就会在上面的界面中出现，注意这里是在Pod的annotation里添加，而不是在Deploy的annotation prometheus.io/scrape:\"true\" service的话按照上述配置中写的，则需要添加如下的Annotation prometheus.io/probe:\"true\" 我们可以在Graph界面进行查询，可以以Console或者Graph的形式展示查询的结果，Prometheus提供了强大的查询语法，参考这里，下图为查询每个Node的CPU使用率 此时还没有进行AlertManager的相关配置，在Alerts页面看不到任何内容，接下来我们部署AlertManager ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:2","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"alertmanager.config.yaml kind:ConfigMapapiVersion:v1metadata:name:alertmanagernamespace:kube-systemdata:config.yml:|-global: resolve_timeout: 5m smtp_smarthost: 'mailhost:port' smtp_from: 'sender@example.com' smtp_auth_username: 'sender@example.com' smtp_auth_password: 'password' slack_api_url: 'https://hooks.slack.com/services/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' templates: - '/etc/alertmanager-templates/*.tmpl' route: group_by: ['alertname', 'cluster', 'service'] group_wait: 30s group_interval: 5m repeat_interval: 15m receiver: slack-notifications receiver: email_alert routes: - match: severity: email receiver: email_alert receivers: - name: 'email_alert' email_configs: - to: 'to@example.com' - name: 'slack-notifications' slack_configs: - channel: '#alert' send_resolved: true 类似Prometheus，我们把AlertManager的配置同样保存为ConfigMap，部署时修改邮箱、slack等对应配置为自己的值，也可以添加自己的接收途径，具体参考这里，上述配置的意思是告诉AlertManager按照alertname、cluster、service对收到的通知进行分组，每隔大约5+15分钟发送一次报警信息，报警信息默认通过发送到slack，如果信息中包含severity: email的label，那么警报还会同时再发送到接收邮箱。 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:3","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"alertmanager.deploy.yaml apiVersion:extensions/v1beta1kind:Deploymentmetadata:name:alertmanagernamespace:kube-systemspec:replicas:1selector:matchLabels:app:alertmanagertemplate:metadata:name:alertmanagerlabels:app:alertmanagerspec:containers:- name:alertmanagerimage:prom/alertmanager:latestargs:- '--config.file=/etc/alertmanager/config.yml'- '--storage.path=/alertmanager'ports:- name:alertmanagercontainerPort:9093volumeMounts:- name:config-volumemountPath:/etc/alertmanager- name:templates-volumemountPath:/etc/alertmanager-templates- name:alertmanagermountPath:/alertmanagervolumes:- name:config-volumeconfigMap:name:alertmanager- name:templates-volumeconfigMap:name:alertmanager-templates- name:alertmanageremptyDir:{}---apiVersion:v1kind:Servicemetadata:annotations:prometheus.io/scrape:'true'labels:name:alertmanagername:alertmanagernamespace:kube-systemspec:selector:app:alertmanagerports:- name:alertmanagerprotocol:TCPport:9093targetPort:9093 ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:4","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"prometheus-rules node-cpu-usage.rules groups:- name:node-cpu-usagerules:- alert:HighRateErrorexpr:(100 - (avg by (instance) (irate(node_cpu{name=\"node-exporter\",mode=\"idle\"}[5m])) * 100)) \u003e 75for:2mlabels:severity:pageannotations:summary:\"{{$labels.instance}}: High CPU usage detected\"description:\"{{$labels.instance}}: CPU usage is above 75% (current value is: {{ $value }})\" prometheus-rules为文件夹，所有的通知规则都放在此文件夹下面，有关rule文件配置，可以参考这里 通过如下命令创建rule对应的ConfigMap kubectl create cm prometheus-rules --from-file=prometheus-rules -o yaml --dry-run | kubectl create -n kube-system -f - ","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:5","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["使用说明"],"content":"alertmanager-templates default.tmpl {{ define \"__alertmanager\" }}AlertManager{{ end }} {{ define \"__alertmanagerURL\" }}{{ .ExternalURL }}/#/alerts?receiver={{ .Receiver }}{{ end }} {{ define \"__subject\" }}[{{ .Status | toUpper }}{{ if eq .Status \"firing\" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.SortedPairs.Values | join \" \" }} {{ if gt (len .CommonLabels) (len .GroupLabels) }}({{ with .CommonLabels.Remove .GroupLabels.Names }}{{ .Values | join \" \" }}{{ end }}){{ end }}{{ end }} {{ define \"__description\" }}{{ end }} {{ define \"__text_alert_list\" }}{{ range . }}Labels: {{ range .Labels.SortedPairs }} - {{ .Name }} = {{ .Value }} {{ end }}Annotations: {{ range .Annotations.SortedPairs }} - {{ .Name }} = {{ .Value }} {{ end }}Source: {{ .GeneratorURL }} {{ end }}{{ end }} {{ define \"slack.default.title\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"slack.default.username\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"slack.default.fallback\" }}{{ template \"slack.default.title\" . }} | {{ template \"slack.default.titlelink\" . }}{{ end }} {{ define \"slack.default.pretext\" }}{{ end }} {{ define \"slack.default.titlelink\" }}{{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"slack.default.iconemoji\" }}{{ end }} {{ define \"slack.default.iconurl\" }}{{ end }} {{ define \"slack.default.text\" }}{{ end }} {{ define \"hipchat.default.from\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"hipchat.default.message\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"pagerduty.default.description\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"pagerduty.default.client\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"pagerduty.default.clientURL\" }}{{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"pagerduty.default.instances\" }}{{ template \"__text_alert_list\" . }}{{ end }} {{ define \"opsgenie.default.message\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"opsgenie.default.description\" }}{{ .CommonAnnotations.SortedPairs.Values | join \" \" }} {{ if gt (len .Alerts.Firing) 0 -}} Alerts Firing: {{ template \"__text_alert_list\" .Alerts.Firing }} {{- end }} {{ if gt (len .Alerts.Resolved) 0 -}} Alerts Resolved: {{ template \"__text_alert_list\" .Alerts.Resolved }} {{- end }} {{- end }} {{ define \"opsgenie.default.source\" }}{{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"victorops.default.message\" }}{{ template \"__subject\" . }} | {{ template \"__alertmanagerURL\" . }}{{ end }} {{ define \"victorops.default.from\" }}{{ template \"__alertmanager\" . }}{{ end }} {{ define \"email.default.subject\" }}{{ template \"__subject\" . }}{{ end }} {{ define \"email.default.html\" }} \u003c!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"\u003e \u003c!-- Style and HTML derived from https://github.com/mailgun/transactional-email-templates The MIT License (MIT) Copyright (c) 2014 Mailgun Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. --\u003e \u003chtml xmlns=\"http://www.w3.org/1999/xhtml\" xmlns=\"http://www.w3.org/1999","date":"2018-06-21","objectID":"https://www.likakuli.com/posts/prometheus/:2:6","tags":["prometheus"],"title":"Prometheus","uri":"https://www.likakuli.com/posts/prometheus/"},{"categories":["源码分析"],"content":"golang netpoll 源码分析","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"简介 go针对不同的操作系统，其网络io模型不同，可以从go源码目录结构和对应内容清楚的看到各平台的io模型，如针对linux系统实现的epoll，针对windows操作系统实现的iocp等，这里主要看针对linux系统的实现，涉及到的文件大体如下： runtime/netpoll.go runtime/netpoll_epoll.go runtime/proc.go net/fd_unix.go internal/poll/fd_poll_runtime.go internal/poll/fd_unix.go 在开始正式看源码之前需要具备一些基础知识，如同步、异步、阻塞、非阻塞、io多路复用、go调度模型等，不了解的话可以参考下面的链接： 同步异步阻塞非阻塞 io多路复用select poll epoll goroutine实现原理 也可以网上自行搜索，文章很多 ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:1:0","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"源码分析（v1.10.2） golang通过对epoll的封装来取得使用同步编程达异步执行的效果。总结来说，所有的网络操作都以网络描述符netFD为中心实现，netFD通过将Sysfd与pollDesc结构绑定，当在一个netFD上读写遇到EAGAIN错误时，就将当前goroutine存储到这个netFD对应的pollDesc中，同时将goroutine给park住，直到这个netFD上再次发生读写事件，才将此goroutine设置为ready放入待运行队列等待重新运行，在底层通知goroutine再次发生读写等事件的方式就是靠的epoll事件驱动机制。 ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:0","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"netFD 服务端通过Listen方法返回的Listener接口的实现和通过listener的Accept方法返回的Conn接口的实现都包含一个网络文件描述符netFD，netFD中包含一个poll.FD数据结构，而poll.FD中包含两个重要的数据结构Sysfd和pollDesc，前者是真正的系统文件描述符，后者对是底层事件驱动的封装，所有的读写超时等操作都是通过调用后者的对应方法实现的。 服务端的netFD在listen时会创建epoll的实例，并将listenFD加入epoll的事件队列 netFD在accept时将返回的connFD也加入epoll的事件队列 netFD在读写时出现syscall.EAGAIN错误，通过pollDesc将当前的goroutine park住，直到ready，从pollDesc的waitRead中返回 涉及到的一些结构： type TCPListener struct { fd *netFD } type conn struct { fd *netFD } // Network file descriptor. type netFD struct { pfd poll.FD // immutable until Close family int sotype int isConnected bool net string laddr Addr raddr Addr } // FD is a file descriptor. The net and os packages use this type as a // field of a larger type representing a network connection or OS file. type FD struct { // Lock sysfd and serialize access to Read and Write methods. fdmu fdMutex // System file descriptor. Immutable until Close. // 系统文件描述符 Sysfd int // I/O poller. pd pollDesc // Writev cache. iovecs *[]syscall.Iovec // Semaphore signaled when file is closed. csema uint32 // Whether this is a streaming descriptor, as opposed to a // packet-based descriptor like a UDP socket. Immutable. IsStream bool // Whether a zero byte read indicates EOF. This is false for a // message based socket connection. ZeroReadIsEOF bool // Whether this is a file rather than a network socket. isFile bool // Whether this file has been set to blocking mode. isBlocking bool } ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:1","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"pollDesc 上面提到的net.conn的读写等操作实际上就是调用的poll.FD对应的方法，poll.FD中包含一个重要结构poll.pollDesc，其定义如下： type pollDesc struct { runtimeCtx uintptr } 可以看到其中只包含一个指针，这个指针具体代表的其实是另一个同名不同包的结构runtime.pollDesc，定义如下： // Network poller descriptor. // // No heap pointers. // //go:notinheap type pollDesc struct { link *pollDesc // in pollcache, protected by pollcache.lock // The lock protects pollOpen, pollSetDeadline, pollUnblock and deadlineimpl operations. // This fully covers seq, rt and wt variables. fd is constant throughout the PollDesc lifetime. // pollReset, pollWait, pollWaitCanceled and runtime·netpollready (IO readiness notification) // proceed w/o taking the lock. So closing, rg, rd, wg and wd are manipulated // in a lock-free way by all operations. // NOTE(dvyukov): the following code uses uintptr to store *g (rg/wg), // that will blow up when GC starts moving objects. lock mutex // protects the following fields fd uintptr closing bool seq uintptr // protects from stale timers and ready notifications rg uintptr // pdReady, pdWait, G waiting for read or nil rt timer // read deadline timer (set if rt.f != nil) rd int64 // read deadline wg uintptr // pdReady, pdWait, G waiting for write or nil wt timer // write deadline timer wd int64 // write deadline user uint32 // user settable cookie } runtime.pollDesc包含自身类型的一个指针，用来保存下一个runtime.pollDesc的地址，go中有很多类似的实现，用来实现链表，可以减少数据结构的大小，所有的runtime.pollDesc保存在runtime.pollCache结构中，定义如下： type pollCache struct { lock mutex first *pollDesc // PollDesc objects must be type-stable, // because we can get ready notification from epoll/kqueue // after the descriptor is closed/reused. // Stale notifications are detected using seq variable, // seq is incremented when deadlines are changed or descriptor is reused. } 以tcp连接为例，分析一下Listen和Accept调用过程： ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:2","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"Listen func Listen(network, address string) (Listener, error) { addrs, err := DefaultResolver.resolveAddrList(context.Background(), \"listen\", network, address, nil) if err != nil { return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: nil, Err: err} } var l Listener switch la := addrs.first(isIPv4).(type) { case *TCPAddr: l, err = ListenTCP(network, la) case *UnixAddr: l, err = ListenUnix(network, la) default: return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: la, Err: \u0026AddrError{Err: \"unexpected address type\", Addr: address}} } if err != nil { return nil, err // l is non-nil interface containing nil pointer } return l, nil } // ListenTCP acts like Listen for TCP networks. // // The network must be a TCP network name; see func Dial for details. // // If the IP field of laddr is nil or an unspecified IP address, // ListenTCP listens on all available unicast and anycast IP addresses // of the local system. // If the Port field of laddr is 0, a port number is automatically // chosen. func ListenTCP(network string, laddr *TCPAddr) (*TCPListener, error) { switch network { case \"tcp\", \"tcp4\", \"tcp6\": default: return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: laddr.opAddr(), Err: UnknownNetworkError(network)} } if laddr == nil { laddr = \u0026TCPAddr{} } ln, err := listenTCP(context.Background(), network, laddr) if err != nil { return nil, \u0026OpError{Op: \"listen\", Net: network, Source: nil, Addr: laddr.opAddr(), Err: err} } return ln, nil } 当我们调用net.Listen(“tcp”,addr)时，根据address类型会命中ListenTCP函数去执行，ListenTCP函数很简单，基本处理逻辑都在listenTCP函数中，往下看 func listenTCP(ctx context.Context, network string, laddr *TCPAddr) (*TCPListener, error) { fd, err := internetSocket(ctx, network, laddr, nil, syscall.SOCK_STREAM, 0, \"listen\") if err != nil { return nil, err } return \u0026TCPListener{fd}, nil } func internetSocket(ctx context.Context, net string, laddr, raddr sockaddr, sotype, proto int, mode string) (fd *netFD, err error) { if (runtime.GOOS == \"windows\" || runtime.GOOS == \"openbsd\" || runtime.GOOS == \"nacl\") \u0026\u0026 mode == \"dial\" \u0026\u0026 raddr.isWildcard() { raddr = raddr.toLocal(net) } family, ipv6only := favoriteAddrFamily(net, laddr, raddr, mode) return socket(ctx, net, family, sotype, proto, ipv6only, laddr, raddr) } listenTCP函数，在此函数中终于见到了期望看到的fd字眼，跳到internetSocket函数，可以看到最终的fd是由socket函数产生的，继续 // socket returns a network file descriptor that is ready for // asynchronous I/O using the network poller. func socket(ctx context.Context, net string, family, sotype, proto int, ipv6only bool, laddr, raddr sockaddr) (fd *netFD, err error) { s, err := sysSocket(family, sotype, proto) if err != nil { return nil, err } if err = setDefaultSockopts(s, family, sotype, ipv6only); err != nil { poll.CloseFunc(s) return nil, err } if fd, err = newFD(s, family, sotype, net); err != nil { poll.CloseFunc(s) return nil, err } // This function makes a network file descriptor for the // following applications: // // - An endpoint holder that opens a passive stream // connection, known as a stream listener // // - An endpoint holder that opens a destination-unspecific // datagram connection, known as a datagram listener // // - An endpoint holder that opens an active stream or a // destination-specific datagram connection, known as a // dialer // // - An endpoint holder that opens the other connection, such // as talking to the protocol stack inside the kernel // // For stream and datagram listeners, they will only require // named sockets, so we can assume that it's just a request // from stream or datagram listeners when laddr is not nil but // raddr is nil. Otherwise we assume it's just for dialers or // the other connection holders. if laddr != nil \u0026\u0026 raddr == nil { switch sotype { case syscall.SOCK_STREAM, syscall.SOCK_SEQPACKET: if err := fd.listenStream(laddr, listenerBacklog); err != nil { fd.Close() return nil, err } return fd, nil case syscall.SOCK_DGRAM: if err := fd.listenDatagram(laddr); err != nil { fd.Close() return nil, err } ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:3","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"Accept TCPListner有两个暴露出来的Accept相关的函数，分别为Accept和AcceptTCP，这里主要从AcceptTCP分析，因为后者被tcpKeepAliveListener的Accept函数调用，而tcpKeepAliveListener的Accept方法就是常用的建立web项目时，http.ListenAndServe中会用到的Accept方法，如下： func (l *TCPListener) AcceptTCP() (*TCPConn, error) { if !l.ok() { return nil, syscall.EINVAL } c, err := l.accept() if err != nil { return nil, \u0026OpError{Op: \"accept\", Net: l.fd.net, Source: nil, Addr: l.fd.laddr, Err: err} } return c, nil } 可以看到实现逻辑基本都在accept方法内 func (ln *TCPListener) accept() (*TCPConn, error) { fd, err := ln.fd.accept() if err != nil { return nil, err } return newTCPConn(fd), nil } func (fd *netFD) accept() (netfd *netFD, err error) { d, rsa, errcall, err := fd.pfd.Accept() if err != nil { if errcall != \"\" { err = wrapSyscallError(errcall, err) } return nil, err } if netfd, err = newFD(d, fd.family, fd.sotype, fd.net); err != nil { poll.CloseFunc(d) return nil, err } if err = netfd.init(); err != nil { fd.Close() return nil, err } lsa, _ := syscall.Getsockname(netfd.pfd.Sysfd) netfd.setAddr(netfd.addrFunc()(lsa), netfd.addrFunc()(rsa)) return netfd, nil } // Accept wraps the accept network call. func (fd *FD) Accept() (int, syscall.Sockaddr, string, error) { if err := fd.readLock(); err != nil { return -1, nil, \"\", err } defer fd.readUnlock() if err := fd.pd.prepareRead(fd.isFile); err != nil { return -1, nil, \"\", err } for { s, rsa, errcall, err := accept(fd.Sysfd) if err == nil { return s, rsa, \"\", err } switch err { case syscall.EAGAIN: if fd.pd.pollable() { if err = fd.pd.waitRead(fd.isFile); err == nil { continue } } case syscall.ECONNABORTED: // This means that a socket on the listen // queue was closed before we Accept()ed it; // it's a silly error, so try again. continue } return -1, nil, errcall, err } } 这里有两个函数比较重要，一个是accept，一个是fd.pd.waitRead，首先看accept的实现，最终还是会通过汇编进行系统调用。 // Wrapper around the accept system call that marks the returned file // descriptor as nonblocking and close-on-exec. func accept(s int) (int, syscall.Sockaddr, string, error) { ns, sa, err := Accept4Func(s, syscall.SOCK_NONBLOCK|syscall.SOCK_CLOEXEC) // On Linux the accept4 system call was introduced in 2.6.28 // kernel and on FreeBSD it was introduced in 10 kernel. If we // get an ENOSYS error on both Linux and FreeBSD, or EINVAL // error on Linux, fall back to using accept. switch err { case nil: return ns, sa, \"\", nil default: // errors other than the ones listed return -1, sa, \"accept4\", err case syscall.ENOSYS: // syscall missing case syscall.EINVAL: // some Linux use this instead of ENOSYS case syscall.EACCES: // some Linux use this instead of ENOSYS case syscall.EFAULT: // some Linux use this instead of ENOSYS } // See ../syscall/exec_unix.go for description of ForkLock. // It is probably okay to hold the lock across syscall.Accept // because we have put fd.sysfd into non-blocking mode. // However, a call to the File method will put it back into // blocking mode. We can't take that risk, so no use of ForkLock here. ns, sa, err = AcceptFunc(s) if err == nil { syscall.CloseOnExec(ns) } if err != nil { return -1, nil, \"accept\", err } if err = syscall.SetNonblock(ns, true); err != nil { CloseFunc(ns) return -1, nil, \"setnonblock\", err } return ns, sa, \"\", nil } // Accept4Func is used to hook the accept4 call. var Accept4Func func(int, int) (int, syscall.Sockaddr, error) = syscall.Accept4 func Accept4(fd int, flags int) (nfd int, sa Sockaddr, err error) { var rsa RawSockaddrAny var len _Socklen = SizeofSockaddrAny nfd, err = accept4(fd, \u0026rsa, \u0026len, flags) if err != nil { return } if len \u003e SizeofSockaddrAny { panic(\"RawSockaddrAny too small\") } sa, err = anyToSockaddr(\u0026rsa) if err != nil { Close(nfd) nfd = 0 } return } // THIS FILE IS GENERATED BY THE COMMAND AT THE TOP; DO NOT EDIT func accept4(s int, rsa *RawSockaddrAny, addrlen *_Socklen, flags int) (fd int, err error) { r0, _, e1 := Syscall6(SYS_ACCEPT4, uintptr(s), uintptr(unsafe.Pointer(rsa)), uintptr(unsafe.Pointer(addrlen)), uintptr(flags), 0, 0) fd = int(r0) if e1 != 0 { err = err","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:2:4","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["源码分析"],"content":"总结 通过上面的简单分析，我们大致可以理解整个流程，里面还涉及到很多具体细节以及go调度器相关的内容无法一一介绍，有兴趣的话可以直接查看源码。剩下的read，write大致相同，这里不再分析，最终都是通过netpoll的相关函数实现的，可以说整个核心实现都在netpoll.go这个文件中，外面只是进行了一些封装和状态的处理，至于G状态的变化的相关代码，可以自行搜索go调度器，已经有相当多博客进行过讲解了。 其实我们可以看到，知识就是个圈，缺少哪一块都串不起来，让我们一起努力，填补我们所缺失的部分吧，加油！ ","date":"2018-06-10","objectID":"https://www.likakuli.com/posts/golang-netpoll/:3:0","tags":["golang","netpoll"],"title":"Golang netpoll源码分析","uri":"https://www.likakuli.com/posts/golang-netpoll/"},{"categories":["问题排查"],"content":"kubernetes's bug because of the resourceversion","date":"2018-05-26","objectID":"https://www.likakuli.com/posts/kubernetes-ep-bug/","tags":["kubernetes"],"title":"Kubernetes惊天地泣鬼神之大bug","uri":"https://www.likakuli.com/posts/kubernetes-ep-bug/"},{"categories":["问题排查"],"content":"最近docker one的交流群里发出了一篇文章，Kubernetes 惊天地泣鬼神之大Bug ，估计很多人看完文章的反应和我一样，心中万马奔腾，自己的集群会不会也有这个问题 ？？？ 文中对于bug的影响和产生的原因已经描述的很清楚了，但看完之后我有一点疑问，文中所说的复现条件( 陆续创建、删除、创建 Kubernetes service 对象，然后\"kubectl delete svc xxx\"删掉创建时间靠前的 service，也就是往 service event list 末尾插入了一条 resourceVersion 比较小的记录，这将使得 controller-manager 去从已经爬过的 service event list 位置重新爬取重放，然后就重放了 service 的 ADDED、DELETED event，于是 controller-manager 内存里缓存的 service 对象被删除，导致 EndpointController 删除了“不存在的”service 对应的 endpoints。)在日常操作中其实会经常出现，那岂不是很多集群都会存在这个问题，而且这个bug影响这么严重，为什么现在才被报出来？基于上述疑问，本文主要是到源码中一探究竟，源码版本1.9.2，按照文中指引可以快速定位到问题代码所在位置，话不多说，直接上代码，如下： // watchHandler watches w and keeps *resourceVersion up to date. func (r *Reflector) watchHandler(w watch.Interface, resourceVersion *string, errc chan error, stopCh \u003c-chan struct{}) error { start := r.clock.Now() eventCount := 0 // Stopping the watcher should be idempotent and if we return from this function there's no way // we're coming back in with the same watch interface. defer w.Stop() // update metrics defer func() { r.metrics.numberOfItemsInWatch.Observe(float64(eventCount)) r.metrics.watchDuration.Observe(time.Since(start).Seconds()) }() loop: for { select { case \u003c-stopCh: return errorStopRequested case err := \u003c-errc: return err case event, ok := \u003c-w.ResultChan(): if !ok { break loop } if event.Type == watch.Error { return apierrs.FromObject(event.Object) } if e, a := r.expectedType, reflect.TypeOf(event.Object); e != nil \u0026\u0026 e != a { utilruntime.HandleError(fmt.Errorf(\"%s: expected type %v, but watch event object had type %v\", r.name, e, a)) continue } meta, err := meta.Accessor(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) continue } newResourceVersion := meta.GetResourceVersion() switch event.Type { case watch.Added: err := r.store.Add(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to add watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Modified: err := r.store.Update(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to update watch event object (%#v) to store: %v\", r.name, event.Object, err)) } case watch.Deleted: // TODO: Will any consumers need access to the \"last known // state\", which is passed in event.Object? If so, may need // to change this. err := r.store.Delete(event.Object) if err != nil { utilruntime.HandleError(fmt.Errorf(\"%s: unable to delete watch event object (%#v) from store: %v\", r.name, event.Object, err)) } default: utilruntime.HandleError(fmt.Errorf(\"%s: unable to understand watch event %#v\", r.name, event)) } *resourceVersion = newResourceVersion r.setLastSyncResourceVersion(newResourceVersion) eventCount++ } } watchDuration := r.clock.Now().Sub(start) if watchDuration \u003c 1*time.Second \u0026\u0026 eventCount == 0 { r.metrics.numberOfShortWatches.Inc() return fmt.Errorf(\"very short watch: %s: Unexpected watch close - watch lasted less than a second and no items received\", r.name) } glog.V(4).Infof(\"%s: Watch close - %v total %v items received\", r.name, r.expectedType, eventCount) return nil } 这是文中指出的具体BUG所在，用错误的newResourceVersion去给resourceVersion和lastSyncResourceVersion赋了值，所以接下来就是要找这两个属性在什么地方被用到了，经过逐步查找代码引用，最后确定lastSyncResourceVersion和这个bug无关，那问题就出在了resourceVersion上，可以看到上面的方法中resourceVersion是通过指针传进来的，也就是说会影响到外面使用到这个属性的地方，继续查看watchHandler被调用的代码，如下： // ListAndWatch first lists all items and get the resource version at the moment of call, // and then use the resource version to watch. // It returns error if ListAndWatch didn't even try to initialize watch. func (r *Reflector) ListAndWatch(stopCh \u003c-chan struct{}) error { glog.V(3).Infof(\"Listing and watching %v from %s\", r.expectedType, r.name) var resourceVersion string // Explicitly set \"0\" as resource version - it's fine for the List() // to be served from cache and potentially be delayed relative to // etcd contents. Reflector framework will catch up via Watch() eventually. options := metav1.ListOptions{Resou","date":"2018-05-26","objectID":"https://www.likakuli.com/posts/kubernetes-ep-bug/:0:0","tags":["kubernetes"],"title":"Kubernetes惊天地泣鬼神之大bug","uri":"https://www.likakuli.com/posts/kubernetes-ep-bug/"},{"categories":["使用说明"],"content":"kubernetes deploy with CA","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"简介 kubernetes 系统的各组件需要使用 TLS 证书对通信进行加密，本文档使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 和其它证书，操作系统CentOS7 amd64； 集群节点 10.202.43.132 master \u0026 etcd 10.202.43 133 master \u0026 node \u0026 etcd 10.202.43.134 node \u0026 etcd 认证请求 ca-csr.json etcd-csr.json admin-csr.json kubernetes-csr.json kube-proxy-csr.json 生成的 CA 证书和秘钥文件如下： ca-key.pem ca.pem kubernetes-key.pem kubernetes.pem kube-proxy.pem kube-proxy-key.pem admin.pem admin-key.pem etcd.pem etcd-key.pem 证书使用情况如下： etcd：使用 ca.pem、etcd-key.pem、etcd.pem； kube-apiserver：使用 ca.pem、kubernetes-key.pem、kubernetes.pem； kubelet：使用 ca.pem； kube-proxy：使用 ca.pem、kube-proxy-key.pem、kube-proxy.pem； kubectl：使用 ca.pem、admin-key.pem、admin.pem； kube-controller、kube-scheduler 当前需要和 kube-apiserver 部署在同一台机器上且使用非安全端口通信，故不需要证书。 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:1:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"安装 CFSSL 这里采用二进制源码包安装，当然也可以使用go命令安装 cd /usr/local/bin wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:2:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"创建 CA (Certificate Authority) 创建 CA 配置文件 mkdir /opt/ssl cd /opt/ssl # ca-config.json 文件 vim ca-config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } 字段说明 ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书，生成的 ca.pem 证书中 CA=TRUE； server auth：表示client可以用该 CA 对server提供的证书进行验证； client auth：表示server可以用该 CA 对client提供的证书进行验证； 创建 CA 证书签名请求 # ca-csr.json 文件 vim ca-csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 字段说明 “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)，浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成 CA 证书和私钥 cfssl gencert -initca ca-csr.json | cfssljson -bare ca ls ca.csr ca-key.pem ca.pem ca-csr.json ca-config.json 分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 10.202.43.133:/etc/kubernetes/ssl/ scp *.csr 10.202.43.133:/etc/kubernetes/ssl/ scp *.pem 10.202.43.134:/etc/kubernetes/ssl/ scp *.csr 10.202.43.134:/etc/kubernetes/ssl/ ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:3:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"创建 etcd 证书 etcd 证书这里，我做测试时用的etcd地址如下 10.202.43.132 10.202.43.133 10.202.43.134 实际配置的 IP 还请根据实际情况修改，也可以多预留几个IP，以备后续添加能通过认证，不需要重新签发 vim etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"10.202.43.132\", \"10.202.43.133\", \"10.202.43.134\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } 生成 etcd 证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 10.202.43.133:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 10.202.43.134:/etc/kubernetes/ssl/ 修改 etcd 配置 # etcd-1 vim /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.202.43.132:2380 \\ --listen-peer-urls=https://10.202.43.132:2380 \\ --listen-client-urls=https://10.202.43.132:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.202.43.132:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.202.43.132:2380,etcd2=https://10.202.43.133:2380,etcd3=https://10.202.43.134:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 另外两个节点上的配置和上面大致相同，只需要把对应IP换成自己的就可以了 证书相关的配置 --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem --peer-cert-file=/etc/kubernetes/ssl/etcd.pem --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem --trusted-ca-file=/etc/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem 分别启动 etcd 集群 systemctl daemon-reload systemctl start etcd # 配置开机启动 systemctl enable etcd # 查看etcd状态 systemctl status etcd 验证 etcd 集群状态 etcdctl --endpoints=https://10.202.43.132:2379,https://10.202.43.133:2379,https://10.202.43.134:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:4:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"创建 kubernetes 证书 创建 admin 证书和私钥 # 创建证书签名请求 vim admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin # 查看生成 ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 10.202.43.133:/etc/kubernetes/ssl/ 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权； kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限； OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限； ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kubectl 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes 创建 kubernetes 证书和私钥 # 创建证书签名请求 vim kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.202.43.132\", \"10.202.43.133\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成证书和私钥 cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes # 查看生成 ls kubernetes* kubernetes.csr kubernetes-key.pem kubernetes.pem kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 10.202.43.133:/etc/kubernetes/ssl/ 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 10.202.43.132 和 10.202.43.133 为 Master 的 IP ，多个Master需要写多个，也可以多写几个，方便以后扩展master节点； 10.254.0.1 为 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个 IP，如 10.254.0.1 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:1","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。有关 TLS Bootstrapping 参考 这里 # 生成 token head -c 16 /dev/urandom | od -An -t x | tr -d ' ' f6280a3754345875d392258bd340ef7e # 创建 token.csv 文件 cd /opt/ssl vim token.csv f6280a3754345875d392258bd340ef7e,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 10.202.43.133:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 10.202.43.133:/etc/kubernetes/ vim /usr/lib/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=10.202.43.132 \\ --allow-privileged=true \\ --apiserver-count=2 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://10.202.43.132:2379,https://10.202.43.133:2379,https://10.202.43.134:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=10000-60000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 证书相关配置 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction # 1.8开始需要添加 NodeRestriction --audit-policy-file=/etc/kubernetes/audit-policy.yaml --authorization-mode=Node,RBAC #1.8 开始需要添加 Node --client-ca-file=/etc/kubernetes/ssl/ca.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/etcd.pem --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem --kubelet-https=true --insecure-bind-address=127.0.0.1 #不是0.0.0.0 仅供kube-controller-manager 和 kube-scheduler 使用 --insecure-port=8080 --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --enable-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:2","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vim /usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 证书相关配置 --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:3","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vim /usr/lib/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:4","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap 配置 kubelet.kubeconfig # 配置集群，server参数为apiserver地址，可以使用haproxy或nginx来代替直接使用ip,达到k8s多主的目的 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.202.43.132:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=f6280a3754345875d392258bd340ef7e \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ 配置 kubelet.service # 创建 kubelet 目录 mkdir /var/lib/kubelet vim /usr/lib/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --pod-infra-container-image=repository.gridsum.com:8443/kaku/pause-adm64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.210.250 \\ --cluster_domain=cluster.local. \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target 证书相关配置 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --cert-dir=/etc/kubernetes/ssl 配置 TLS 认证 # 查看 csr 的名称 kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-*********************************** 1m kubelet-bootstrap Pending # 增加 认证 kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 这里注意如果 csr 被删除了，请删除如下文件，并重启 kubelet 服务 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:5","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 kube-proxy 创建 kube-proxy 证书和私钥 vim kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"System\" } ] } cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy* /etc/kubernetes/ssl/ scp kube-proxy* 10.202.43.133:/etc/kubernetes/ssl/ scp kube-proxy* 10.202.43.134:/etc/kubernetes/ssl/ CN 指定该证书的 User 为 system:kube-proxy； kube-apiserver 预定义的 RoleBindings cluster-admin 将User system:kube-proxy 与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy 相关 API 的权限； 配置 kube-proxy.kubeconfig # 配置集群, server参数为apiserver地址，可以使用haproxy或nginx来代替直接使用ip,达到k8s多主的目的 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.202.43.132:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 10.202.43.133:/etc/kubernetes/ scp kube-proxy.kubeconfig 10.202.43.134:/etc/kubernetes/ 配置 kube-proxy.service 1.9 官方 ipvs 已经 beta , 尝试开启 ipvs 测试一下, 官方 –feature-gates=SupportIPVSProxyMode=false 默认是 false 的， 需要设置 –-feature-gates=SupportIPVSProxyMode=true –-masquerade-all。执行 yum install ipvsadm -y 安装 ipvs，ipvs 和 calico 不兼容，原因之一是 calico 必须不能设置 –-masquerade-all # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vim /usr/lib/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=10.202.43.133 \\ --hostname-override=10.202.43.133 \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 配置说明 # --bind-address 和 --hostname-override 按需设置， 其中 --bind-address 为 kube-proxy 所在节点的 IP 证书相关配置 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:6","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"配置 CoreDNS 配置 coredns.yaml # 下载配置文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vim coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/16 { pods insecure upstream /etc/resolv.conf fallthrough in-addr.arpa ip6.arpa } ... image: repository.gridsum.com:8443/library/coredns ... clusterIP: 10.254.210.250 # 创建 coredns kubectl apply -f coredns.yaml ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:5:7","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"dashboard 官方 dashboard dashboard都从1.7之后只支持https访问，按照官网使用kubectl proxy启动本地代理来访问。此处必须为本地代理，即必须通过http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/访问，如果要在windows上访问的话，可以自己编译一个kubectl对应的windows客户端，通过令牌登陆的时候，能看到的内容与token的权限有关。 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:6:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"apiserver访问 客户端在集群外 # 在任一 master 节点执行瑞安命令便可在集群外通过http://masterip:8001访问api server kubectl proxy --address 0.0.0.0 --accept-hosts '.*' --port=8001 集群内访问 # kubectl get secret NAME TYPE DATA AGE default-token-z6lq7 kubernetes.io/service-account-token 3 15d # kubectl describe secret default-token-z6lq7 Name: default-token-z6lq7 Namespace: default Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name=default kubernetes.io/service-account.uid=c246a42f-3d73-11e8-aa36-00155d010a3a Type: kubernetes.io/service-account-token Data ==== ca.crt: 1310 bytes namespace: 7 bytes token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tejZscTciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyNDZhNDJmLTNkNzMtMTFlOC1hYTM2LTAwMTU1ZDAxMGEzYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Qfi9881mVeX8Z4ophf6_l5e5jGcILFobS5mKTQgdoz6NF0rJ4buRASmQiqu4N5ErolOstSIJhhK1yVzHbkBYsYReip6ffTnOwF2cWU5EJAhP7_o2zGWK5b11amlp5qLU0rWucfYe34ZfGxAcxoekmgKwJ6Hu58JlgD3ae5lu-_J6yVT_O-klC6FUXCY-r3FbtwwYz7WbrSxhuu5nCbegmEy5gPy9aEeVZcz6v5ZIyTU62mvbO_M1xYQfPyaHPWgjbkh9H540j2LUa7Y7RQw_LLEp_NbpzVfN58sFLY8cndzUfr5v_KjtFXyPVqmUX0qxoIRWL2opB7Qr2lL7qyTg5w #上述secret会挂载到所有的pod上，容器内对应路径为/var/run/secrets/kubernetes.io/serviceaccount #集群内的Pod通过在http请求的header中添加Authorization: Bearer ${KUBE_BEARER_TOKEN} 进行访问 # kubectl exec -it crawler-bgapi-new-9bjc7 sh # cd /var/run/secrets/kubernetes.io/serviceaccount # ls ca.crt namespace token # cat token eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImRlZmF1bHQtdG9rZW4tejZscTciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGVmYXVsdCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImMyNDZhNDJmLTNkNzMtMTFlOC1hYTM2LTAwMTU1ZDAxMGEzYSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0OmRlZmF1bHQifQ.Qfi9881mVeX8Z4ophf6_l5e5jGcILFobS5mKTQgdoz6NF0rJ4buRASmQiqu4N5ErolOstSIJhhK1yVzHbkBYsYReip6ffTnOwF2cWU5EJAhP7_o2zGWK5b11amlp5qLU0rWucfYe34ZfGxAcxoekmgKwJ6Hu58JlgD3ae5lu-_J6yVT_O-klC6FUXCY-r3FbtwwYz7WbrSxhuu5nCbegmEy5gPy9aEeVZcz6v5ZIyTU62mvbO_M1xYQfPyaHPWgjbkh9H540j2LUa7Y7RQw_LLEp_NbpzVfN58sFLY8cndzUfr5v_KjtFXyPVqmUX0qxoIRWL2opB7Qr2lL7qyTg5 ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:7:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"},{"categories":["使用说明"],"content":"参考 Generate self-signed certificates Setting up a Certificate Authority and Creating TLS Certificates Client Certificates V/s Server Certificates 数字证书及 CA 的扫盲介绍 kubernetes 1.9.1 kubernetes安装之证书认证 管理集群中的TLS ","date":"2018-05-24","objectID":"https://www.likakuli.com/posts/kubernetes-ca/:8:0","tags":["kubernetes"],"title":"Kubernetes 1.9.6 CA IPVS CoreDNS","uri":"https://www.likakuli.com/posts/kubernetes-ca/"}]